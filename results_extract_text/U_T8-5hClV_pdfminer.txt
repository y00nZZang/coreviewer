Published as a conference paper at ICLR 2023

A PRIMAL-DUAL FRAMEWORK FOR TRANSFORMERS
AND NEURAL NETWORKS

Tan M. Nguyen*
Department of Mathematics
University of California, Los Angeles
tanmnguyen89@ucla.edu

Tam Nguyen*
Department of ECE
Rice University
nguyenminhtam9520@gmail.com

Nhat Ho
Department of Statistics & Data Sciences
University of Texas at Austin
minhnhat@utexas.edu

Andrea L. Bertozzi
Department of Mathematics
University of California, Los Angeles
bertozzi@math.ucla.edu

Richard G. Baraniuk**
Department of ECE
Rice University
richb@rice.edu

Stanley J. Osher**
Department of Mathematics
University of California, Los Angeles
sjo@math.ucla.edu

ABSTRACT

Self-attention is key to the remarkable success of transformers in sequence model-
ing tasks including many applications in natural language processing and computer
vision. Like neural network layers, these attention mechanisms are often developed
by heuristics and experience. To provide a principled framework for constructing
attention layers in transformers, we show that the self-attention corresponds to the
support vector expansion derived from a support vector regression problem, whose
primal formulation has the form of a neural network layer. Using our framework,
we derive popular attention layers used in practice and propose two new atten-
tions: 1) the Batch Normalized Attention (Attention-BN) derived from the batch
normalization layer and 2) the Attention with Scaled Head (Attention-SH) derived
from using less training data to fit the SVR model. We empirically demonstrate the
advantages of the Attention-BN and Attention-SH in reducing head redundancy,
increasing the model’s accuracy, and improving the model’s efficiency in a variety
of practical applications including image and time-series classification.

1

INTRODUCTION

Transformer models (Vaswani et al., 2017) have achieved impressive success with state-of-the-art per-
formance in a myriad of sequence processing tasks, including those in computer vision (Dosovitskiy
et al., 2021; Liu et al., 2021; Touvron et al., 2020; Ramesh et al., 2021; Radford et al., 2021; Arnab
et al., 2021; Liu et al., 2022; Zhao et al., 2021; Guo et al., 2021), natural language processing (Devlin
et al., 2018; Al-Rfou et al., 2019; Dai et al., 2019; Child et al., 2019; Raffel et al., 2020; Baevski &
Auli, 2019; Brown et al., 2020; Dehghani et al., 2018), reinforcement learning (Chen et al., 2021;
Janner et al., 2021), and other important applications (Rives et al., 2021; Jumper et al., 2021; Zhang
et al., 2019; Gulati et al., 2020; Wang & Sun, 2022). Transformers can also effectively transfer
knowledge from pre-trained models to new tasks with limited supervision (Radford et al., 2018;
2019; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019). The driving force behind the success of
transformers is the self-attention mechanism (Cho et al., 2014; Parikh et al., 2016; Lin et al., 2017),
which computes a weighted average of feature representations of the tokens in the sequence with the
weights proportional to similarity scores between pairs of representations. The weights calculated by
the self-attention determine the relative importance between tokens and thus capture the contextual
representations of the sequence (Bahdanau et al., 2014; Vaswani et al., 2017; Kim et al., 2017). It has

∗ Co-first authors. ∗∗ Co-last authors. Please correspond to: tanmnguyen89@ucla.edu

1

(1)

(2)

Published as a conference paper at ICLR 2023

been argued that the flexibility in capturing diverse syntactic and semantic relationships is critical for
the success of transformers (Tenney et al., 2019; Vig & Belinkov, 2019; Clark et al., 2019).

1.1 BACKGROUND: SELF-ATTENTION

For a given input sequence X := [x1, · · · , xN ]⊤ ∈ RN ×Dx of N feature vectors, self-attention
transforms X into the output sequence H in the following two steps:

Step 1. The input sequence X is projected into the query matrix Q, the key matrix K, and the value
matrix V via three linear transformations
Q = XW⊤

K; V = XW⊤
V ,

Q; K = XW⊤

where WQ, WK ∈ RD×Dx , and WV ∈ RDv×Dx are the weight matrices. We denote Q :=
[q1, · · · , qN ]⊤, K := [k1, · · · , kN ]⊤, and V := [v1, · · · , vN ]⊤, where the vectors qi, ki, vi for
i = 1, · · · , N are the query, key, and value vectors, respectively.
Step 2. The output sequence H := [h1, · · · , hN ]⊤ is then computed as follows

H = softmax

QK⊤/

D

V := AV,

(cid:16)

√

(cid:17)

√

(cid:17)

(cid:16) QK⊤
√
D

where the softmax function is applied to each row of the matrix QK⊤/
D. The matrix A :=
∈ RN ×N and its component aij for i, j = 1, · · · , N are called the attention matrix
softmax
and attention scores, respectively. For each query vector qi for i = 1, · · · , N , an equivalent form of
Eqn. (1) to compute the output vector hi is given by
(cid:16)

√

(cid:17)

N
(cid:88)

hi =

softmax

q⊤
i kj/

D

vj.

j=1

The self-attention computed by Eqn. (1) and (2) is called the scaled dot-product or softmax attention.
In our paper, we call a transformer that uses this attention the softmax transformer. The structure
that the attention matrix A learns from training determines the ability of the self-attention to capture
contextual representation for each token. Additionally, a residual connection can be added to the
output of the self-attention layer, hi = xi + (cid:80)N

vj.

√

D

(cid:16)

(cid:17)

j=1 softmax

q⊤
i kj/

Multi-head Attention (MHA). In MHA, multiple heads are concatenated to compute the final output.
This MHA mechanism allows transformers to capture more diverse attention patterns and increase the
(cid:3) ∈ RDv×HDv
O, . . . , WH
capacity of the model. Let H be the number of heads and Wmulti
O
O ∈ RDv×Dv . The MHA is defined as
be the projection matrix for the output where W1
H
(cid:88)

O = (cid:2)W1

O, . . . , WH

H
(cid:88)

MultiHead({H}H

s=1) = Concat(H1, . . . , HH )Wmulti⊤

O

AsVsWs⊤
O .

HsWs⊤

O =

(3)

=

Despite their remarkable success, most attention layers are developed based on heuristic approaches,
and a coherent principled framework for synthesizing attention layers has remained elusive.

s=1

s=1

1.2 CONTRIBUTION

We derive the self-attention as the support vector expansion of a given support vector regression
(SVR) problem. The primal representation of the regression function has the form of a neural network
layer. Thus, we establish a primal-dual connection between an attention layer in transformers and
a neural network layer in deep neural networks. Our framework suggests a principled approach
to developing an attention mechanism: Starting from a neural network layer and a support vector
regression problem, we derive the dual as a support vector expansion to attain the corresponding
attention layer. We then employ this principled approach to invent two novel classes of attentions:
the Batch Normalized Attention (Attention-BN) derived from the batch normalization layer in deep
neural networks and the Attention with Scaled Heads (Attention-SH) resulting from solving the
support vector regression model with less amount of training data. Our contribution is three-fold.

1. We derive self-attention as a support vector expansion that solves a SVR problem, thus

providing a principled primal-dual framework to study and develop self-attentions.

2. We re-derive popular attentions, such as the linear attention (Katharopoulos et al., 2020),
the sparse attention (Child et al., 2019), and the multi-head attention (Vaswani et al., 2017),
from our proposed framework.

2

Published as a conference paper at ICLR 2023

3. We develop two new attention mechanism: the Batch Normalized Attention (Attention-BN)

and the Attention with Scaled Heads (Attention-SH) using our proposed framework.

We empirically demonstrate that 1) the Attention-BN significantly outperforms the baseline softmax
and linear attention and 2) the Attention-SH performs better while being more efficient than the same
baselines on a variety of practical tasks including image and time-series classification.

2 PRIMAL-DUAL INTERPRETATION OF SELF-ATTENTION

We first provide a primal-dual interpretation of self-attention as a support vector regression problem
in Section 2.1. Based on that primal-dual framework, we derive popular attention mechanisms as
the support vector expansion in Section 2.2. Finally, we introduce two new attention mechanisms in
Section 2.3, the Attention-BN and Attention-SH.

2.1 ATTENTION AS A SUPPORT VECTOR REGRESSION MODEL

In this section, we derive self-attention from a support vector regression problem. Suppose we are
given a training data {(k1, y1), . . . , (kN , yN )} ⊂ K × Y, where K = RD and Y = RDv . Here,
k1, . . . , kN are attention keys in self-attention, and y1, . . . , yN are the training targets. We consider
the function f , taking the form

y = f (x) := W

Φ(x)
h(x)

+ b,

(4)

where x ∈ K = RD, Φ(x) = [ϕ1(x), . . . , ϕDϕ(x)] ∈ RDϕ, W = [w1, . . . , wDv ]⊤ ∈ RDv×Dϕ,
b ∈ RDv , and h(x) is a vector-scalar function. We fit the function f to the training data
{(k1, y1), . . . , (kN , yN )} with an L2 regularization on W, i.e., a ridge regression, by solving
the following convex optimization problem:

minimize
W
ξj , ˜ξj ,j=1,...,N

subject to

1
2

∥W∥2

F + C

N
(cid:88)

Dv(cid:88)

j=1

d=1

(cid:16)
ξj(d) + ˜ξj(d)

(cid:17)

=

1
2

Dv(cid:88)

d=1

∥wd∥2 + C

N
(cid:88)

Dv(cid:88)

(cid:16)

j=1

d=1

ξj(d) + ˜ξj(d)

(cid:17)






d Φ(kj)/h(kj) − b(d) ≤ ϵ + ξj(d)
d Φ(kj)/h(kj) + b(d) − yj(d) ≤ ϵ + ˜ξj(d)

yj(d) − w⊤
w⊤
ξj(d), ˜ξj(d) ≥ 0

, j = 1, . . . , N, d = 1, . . . , Dv.

(5)
The Eqn. 5 implies that there exists a function f that can approximates all pairs (kj, yj) with ϵ
precision. The additional slack variables ξj, ˜ξj relax this assumption and allows some of the training
set data points to have the training error greater than ϵ just as in the soft-margin SVM (Cortes &
Vapnik, 1995; Sch¨olkopf et al., 2002). C > 0 is a constant determining the trade between the
complexity penalizer (cid:80)Dv
d=1 ∥wd∥2, i.e., the flatness of f , and the amount up to which deviations
larger than ϵ are tolerated.

In order to derive the self-attention from the support vector regression defined by the optimization
problem 5, the key idea to construct the Lagrangian from Eqn. 5 and find the representation of the
wd, d = 1, . . . , Dv, in terms of the dual variables. We define the Lagrangian function as follows:

L :=

1
2

Dv(cid:88)

d=1

∥wd∥2 + C

N
(cid:88)

Dv(cid:88)

j=1

d=1

(cid:16)

(cid:17)
ξj(d) + ˜ξj(d)

−

N
(cid:88)

Dv(cid:88)

j=1

d=1

(cid:16)

(cid:17)
ηj(d)ξj(d) + ˜ηj(d) ˜ξj(d)

−

−

N
(cid:88)

Dv(cid:88)

j=1

d=1

N
(cid:88)

Dv(cid:88)

j=1

d=1

(cid:18)

(cid:18)

αj(d)

˜αj(d)

ϵ + ξj(d) − yj(d) + w⊤
d

ϵ + ˜ξj(d) + yj(d) − w⊤
d

(cid:19)

+ b(d)

(cid:19)

− b(d)

,

Φ(kj)
h(kj)

Φ(kj)
h(kj)

(6)

where ηj, ˜ηj, αj and ˜αj are Lagrange multipliers. These dual variables have to satisfy positivity
constraints, i.e., ηj(d), ˜ηj(d), αj(d), ˜αj(d) ≥ 0, ∀j = 1, . . . , N, ∀d = 1, . . . , Dv. It follows from
the saddle point condition that the partial derivatives of the Lagrangian function L with respect to
, d = 1, . . . , Dv, have to vanish for optimality,
the primal variables

wd, b(d), {ξj(d), ˜ξj(d)}N

(cid:17)

(cid:16)

j=1

3

Published as a conference paper at ICLR 2023

namely, we have:
N
(cid:88)

∂b(d)L =

( ˜αj(d) − αj(d)) = 0 ⇒

N
(cid:88)

(αj(d) − ˜αj(d)) = 0,

j=1

∂wd L = wd −

N
(cid:88)

j=1

(αj(d) − ˜αj(d))

j=1

Φ(kj)
h(kj)

= 0 ⇒ wd =

N
(cid:88)

(αj(d) − ˜αj(d))

j=1

Φ(kj)
h(kj)

,

∂ξj (d)L = C − αj(d) − ηj(d) = 0, ∂ ˜ξj (d)L = C − ˜αj(d) − ˜ηj(d) = 0.

(7)

(8)

(9)

Let vj = [ αj (1)− ˜αj (1)
obtain the following support vector expansion of the linear basis function f :

, . . . , αj (Dv)− ˜αj (Dv)

h(kj )

h(kj )

]⊤, j = 1, . . . , N , and substitute Eqn. 8 into Eqn. 4, we





f (x) =

N
(cid:88)

j=1

αj(1) − ˜αj(1)
h(kj)

Φ(x)⊤Φ(kj)
h(x)

, . . . ,

N
(cid:88)

j=1

αj(Dv) − ˜αj(Dv)
h(kj)

Φ(x)⊤Φ(kj)
h(x)



⊤



+ b,

(10)

=

N
(cid:88)

j=1

Φ(x)⊤Φ(kj)
h(x)

vj + b.

(12)

(13)

(cid:19)

,

We then select Φ(x) =

a(t)
l =

Since

exp (xT y) =

then Eqn. 27 becomes

Remark 1 Notice that from Eqn. 9 and the conditions ηj(d), ˜ηj(d), αj(d), ˜αj(d) ≥ 0, we can
prove that αj(d), ˜αj(d) ∈ [0, C]. Furthermore, we can show that αj(d) ∗ ˜αj(d) = 0 (Smola &
Sch¨olkopf, 2004; Sch¨olkopf et al., 2002). As a result, vj(d) ∈

(cid:105)
, d = 1, . . . , Dv.

− C

(cid:104)

h(kj ) , C
h(kj )

Deriving Softmax Attention. Choosing the appropriate h(x) and Φ(x) allows us to derive the popu-
lar softmax attention given in Eqn. 1 and 2. In particular, if we choose h(x) := (cid:80)N
j Φ(x)T Φ(kj),
Eqn. 10 becomes

f (x) =

vj + b =

N
(cid:88)

(cid:80)N

j=1
(cid:16)

Φ(x)⊤Φ(kj)
j′ Φ(x)T Φ(kj′)
, . . . , a(t)
1 , . . . , a(1)
, a(1)
a(0)
l1
l0
√
√
D)n1 . . . (xD/ 4
D)nD
(x1/ 4
√
n1! . . . nD!

(cid:80)N

j=1 Φ(x)⊤Φ(kj)vj
(cid:80)N
j′ Φ(x)T Φ(kj′)
, . . .

(cid:17)

1 , . . . , a(t)
lt

where lt = (cid:0)D+t−1

(cid:1) and

t

+ b.

(11)

| n1 + · · · + nD = t, 1 ≤ l ≤ lt.

∞
(cid:88)

t=0

(xT y)t
t!

∞
(cid:88)

=

(cid:88)

t=0

n1+···+nD=t

(cid:18) xn1
√

1 . . . xnD
n1! . . . nD!

D

(cid:19) (cid:18) yn1
√

1 . . . ynD
n1! . . . nD!

D

√

f (x) =

N
(cid:88)

j=1

exp (x⊤kj/
D)
√
j′=1 exp (x⊤kj′/

(cid:80)N

vj + b =

D)

N
(cid:88)

j=1

softmax

(cid:16)

√

x⊤kj/

(cid:17)

D

vj + b.

(14)

Let x = qi, b = 0 and relax the boundness constraint of vj in Remark 1. Eqn. 30 becomes Eqn. 2 of
the softmax attention (Vaswani et al., 2017). We summarize our results in the following theorem.

Theorem 1 (Softmax Attention as a Support Vector Expansion) Given the function f defined in
Eqn. 4 with h(x) := (cid:80)N
j Φ(x)T Φ(kj) and the support vector regression problem defined in Eqn. 5,
we set b = 0, choose Φ(x) as in Eqn. 28, and relax the boundness constraint of the variables vj =
[ αj (1)− ˜αj (1)
]⊤, where αj and ˜αj are dual variables of Eqn. 5, j = 1, . . . , N .
h(kj )
Then, the support vector expansion of f derived from Eqn. 5 has the form of a softmax attention

, . . . , αj (Dv)− ˜αj (Dv)

h(kj )

f (x) =

N
(cid:88)

j=1

(cid:16)

softmax

√

x⊤kj/

(cid:17)

D

vj.

(15)

Remark 2 Since b is set to 0, the centering constraint of αj and ˜αj in Eqn. 7 can be ignored.

Remark 3 Theorem 1 and its derivation can be easily extended to capture the full form of the softmax
attention with the residual connection, the query matrix projection WQ, the key matrix projection
WK, and the value matrix projection WV . We include this result in Appendix F.

4

Published as a conference paper at ICLR 2023

Remark 4 The primal representation of the function f as in Eqn. 4 has the form of a neural network
layer where W is the weight, b is the bias term, Φ(x) is the input, and h(x) is the normalization
term. Thus, an attention layer and a neural network layer are primal-dual of each other.

A principled approach to developing an attention mechanism. The observation in Remark 4
suggests a principled way to construct an attention layer: Starting from a neural network layer and
a support vector regression problem, we derive the dual as a support vector expansion to attain
the corresponding attention layer. Using this approach, we derive popular attention mechanisms in
Section 2.2 and propose our new attention mechanisms in Section 2.3.

2.2 DERIVING POPULAR ATTENTION MECHANISMS AS THE SUPPORT VECTOR EXPANSION

In this section, we derive popular attentions such as the linear attention (Katharopoulos et al., 2020),
the sparse attention (Child et al., 2019), and the multi-head attention (Vaswani et al., 2017).

2.2.1 LINEAR ATTENTION
The Eqn. 27, which is obtained when choosing h(x) := (cid:80)N
j Φ(x)T Φ(kj), already matches the
formula of the linear attention. Here, we can let b = 0 as above and select the function Φ that results
in a positive similarity function, e.g. Φ(x) = elu(x) + 1, as in (Katharopoulos et al., 2020).

2.2.2 SPARSE ATTENTION

The sparse attention (Child et al., 2019) can be derived by fitting the function f in Eqn. 4 using a differ-
ent subset {(kmx(1), ymx(1)), . . . , (kmx(M ), ymx(M ))} of training data {(k1, y1), . . . , (kN , yN )}
for each input data x, where Mx = {mx(1), . . . , mx(M )} ⊂ {1, . . . , N }. The support vector
expansion of f is then given by

where 1Mx(j) = [j ∈ Mx] :=

f (x) =

N
(cid:88)

j=1

1Mx(j)
(cid:26)1 if j ∈ Mx
0 otherwise

Φ(x)⊤Φ(kj)
h(x)

vj + b

(16)

. Note that the subsets Mx are different for

different x. When letting x = qi where qi, i = 1, . . . , N , are the query vectors and choosing Φ, h, b
as in Section 2.1, we can obtain the sparse attention in (Child et al., 2019) where the binary matrix
M = (cid:0)1Mqi
2.2.3 MULTI-HEAD ATTENTION (MHA)

becomes the sparse masking matrix.

(j)(cid:1)N

i,j=1

The MHA can be derived by solving multiple support vector
and then linearly combining their outputs.
{(k1
RDv . We define the function f applied on the input vector x = [x1, . . . , xH ] as follows
(cid:19)

regression problems
In particular, given H training datasets
N )} ⊂ K × Y, where K = RD and Y =

N )}, . . . , {(kH

1 ), . . . , (kH

1), . . . , (k1

N , yH

1 , yH

N , y1

1, y1

y = f (x) :=

Ws

Oys =

Ws

Of s(xs) =

Ws
O

+ bs

,

(17)

H
(cid:88)

s=1

(cid:18)
Ws Φs(xs)
hs(xs)

1, ys

where each function f s(xs) = Ws Φs(xs)
is fitted to the training dataset
{(ks
s=1 as in Sec-
tion 2.1, we can rewrite f (x) in terms of the support vector expansions of the individual functions
f s(xs), which are the individual softmax attentions


N )}. Following the same derivation and choosing {Φs, hs, bs}H

hs(xs) + bs

1), . . . , (ks

N , ys







H
(cid:88)

s=1

H
(cid:88)

s=1

H
(cid:88)

f (x) =

Ws
O



N
(cid:88)

Φs(xs)⊤Φs(ks
j )
hs(xs)

s=1

j=1

j + bs
vs

 =

Ws
O



softmax

xs⊤ks
j /

H
(cid:88)

N
(cid:88)

(cid:16)

√

(cid:17)

D

vs
j

 .

s=1

j=1

(18)
Comparing Eqn. 18 and Eqn. 3, we see that Eqn. 18 computes the MHA when choosing xs = qs
i
where qs
2.3 DERIVING NEW ATTENTION MECHANISMS: BATCH NORMALIZED ATTENTION AND

i , i = 1, . . . , N , are the query vectors at the sth head.

MULTIRESOLUTION HEAD ATTENTION

In this section, we employ our primal-dual framework to develop new attention mechanisms. In par-
ticular, we derive: 1) the Batch Normalized Attention from employing the batch normalization (Ioffe
& Szegedy, 2015); and 2) the Attention with Scaled Heads from using different amounts of training
data. By 1) and 2), we demonstrate that new attentions can be invented by modifying the primal
neural network layer and the support vector regression problem in our framework, respectively.

5

Published as a conference paper at ICLR 2023

2.3.1 BATCH NORMALIZED ATTENTION

We incorporate the batch normalization into the primal form of the function f in Eqn. 4. Given a
training data {(k1, y1), . . . , (kN , yN )} ⊂ K × Y, where K = RD and Y = RDv as in Section 2.1,
the resultant f is defined as follows

f (x) := W

Φ((x − µ) ⊙ s−1)
h((x − µ) ⊙ s−1)

+ b,

(19)

where

µ =

1
N

N
(cid:88)

j=1

(cid:34)

kj, s−1 =

1
(cid:112)σ2
1 + ϵ

, . . . ,

1
(cid:112)σ2
D + ϵ

(cid:35)⊤

, σ2

d =

1
N

N
(cid:88)

j=1

(kj(d) − µ(d))2.

(20)

Here, d = 1, . . . , D, and the mean subtraction and division by the standard deviation is performed
element-wise along the feature dimension of x. Following the same derivation as in Section 2.1, we
derive the following support vector expansion of f

f (x) =

N
(cid:88)

j=1

Φ((x − µ) ⊙ s−1)⊤Φ((kj − µ) ⊙ s−1)
h((x − µ) ⊙ s−1)

vj + b.

(21)

(cid:104) αj (1)− ˜αj (1)
h((kj −µ)⊙s−1) , . . . , αj (Dv)− ˜αj (Dv)

Here, vj =
h((kj −µ)⊙s−1)
1, . . . , N . Same as in Section 2.1, in Eqn. 21, we choose Φ as in Eqn. 28, h(x) := (cid:80)N
and b = 0 to obtain the the Batch Normalized Attention, which is defined as follows.

, where αj and ˜αj are the dual variables, j =
j Φ(x)T Φ(kj),

(cid:105)⊤

Definition 1 (Batch Normalized Attention) Given a set of the key and value vectors {kj, vj}N
j=1,
for each query vector qi, i = 1, . . . , N , the Batch Normalized Attention (Attention-BN) computes the
corresponding output vector hi of the query qi by the following attention formula:

hi =

N
(cid:88)

j=1

(cid:16)

softmax

((qi − µ) ⊙ s−1)⊤((kj − µ) ⊙ s−1)/

√

(cid:17)

D

vj,

(22)

where

µ =

1
N

N
(cid:88)

j=1

(cid:34)

kj, s−1 =

1
(cid:112)σ2
1 + ϵ

, . . . ,

1
(cid:112)σ2
D + ϵ

(cid:35)⊤

, σ2

d =

1
N

N
(cid:88)

j=1

(kj(d) − µ(d))2.

(23)

The Effect of Normalization. Expanding the dot product in the Attention-BN (see Appendix E),
Eqn. 22 becomes

hi =

N
(cid:88)

j=1

softmax

(cid:32) D
(cid:88)

d=1

(cid:80)N

qi(d)kj(d) − 1
N
D(σ2

√

j′=1 kj′(d)kj(d)
d + ϵ)

(cid:33)

vj.

(24)

Eqn. 24 implies that in the Attention-BN, the similarity between the query qi and the key kj is
adjusted by the similarity between the key kj and all the keys kj′, j′ = 1, . . . , N . In particular, if the
key kj is too similar to other keys, the query qi will attend to it less and vice versa.

2.3.2 ATTENTION WITH SCALED HEADS

The Attention with Scaled Heads, named Attention-SH, is derived based on the derivation of
the MHA in Section 2.2.3. The key idea underlying the Attention-SH is to train multiple sup-
In particular, the
port vector regression problems using different amounts of training data.
Attention-SH follows Eqn. 17 in Section 2.2.3 and defines the same regression function f as the
MHA. However, the Attention-SH fits the function f s, s = 1, . . . , H, in Eqn. 17 with training
sets {(k1
)} ⊂ K × Y of different sizes
N1, . . . , NH , where K = RD and Y = RDv . The resultant support vector expansion yields the
formula of the Attention-SH as in the following definition.

1 ), . . . , (kH
NH

1), . . . , (k1
N1

)}, . . . , {(kH

1 , yH

, yH
NH

, y1
N1

1, y1

j , v1

j }N1

the key and value vectors
Definition 2 (Attention with Scaled Heads) Given H sets of
{k1
i , . . . , qH
j=1, for each set of H query vectors q1
i , i = 1, . . . , N , the
Attention with Scaled Heads (Attention-SH) computes the corresponding output vector hi of the
queries q1

j=1, . . . , {kH

j }NH

j , vH

i , . . . , qH

i by the following attention formula:
(cid:16)



Ns(cid:88)

H
(cid:88)

hi =

Ws
O



softmax

i ks
qs⊤
j /

√

(cid:17)

D



vs
j

 .

(25)

s=1

j=1

6

Published as a conference paper at ICLR 2023

Table 1: Test Accuracy (%) of the Attention-BN/SH/BN+SH vs. the baseline softmax attention on a subset
of the UEA Time Series Classification Archive benchmark (Bagnall et al., 2018). Our proposed attentions
significantly outperform the baseline. We also include the reported results from (Zerveas et al., 2021) and (Wu
et al., 2022) (in parentheses) in addition to our reproduced results.

Dataset/Model

Baseline Softmax

Attention-BN Attention-SH Attention-BN+SH

ETHANOLCONCENTRATION
FACEDETECTION
HANDWRITING
HEARTBEAT
JAPANESEVOWELS
PEMS-SF
SELFREGULATIONSCP1
SELFREGULATIONSCP2
SPOKENARABICDIGITS
UWAVEGESTURELIBRARY

32.08 ± 1.24 (33.70)
68.70 ± 0.61 (68.10)
32.08 ± 0.88 (30.50)
75.77 ± 1.01 (77.60)
99.46 ± 0.27 (99.40)
82.66 ± 0.51 (82.10)
91.46 ± 0.35 (92.50)
54.72 ± 0.74 (53.90)
99.33 ± 0.02 (99.30)
84.45 ± 0.72 (85.60)

33.33 ± 0.44
68.62 ± 0.26
33.17 ± 0.20
76.10 ± 0.98
99.55 ± 0.31
84.77 ± 0.33
91.58 ± 0.39
56.11 ± 0.71
99.23 ± 0.09
86.46 ± 0.81

33.59 ± 0.58
68.83 ± 0.16
33.29 ± 0.42
76.25 ± 1.02
99.46 ± 0.27
83.04 ± 0.86
91.70 ± 0.39
55.93 ± 0.85
99.34 ± 0.11
86.77 ± 0.78

34.35 ± 0.53
68.67 ± 0.13
33.45 ± 0.61
76.26 ± 1.07
99.55 ± 0.31
83.81 ± 0.62
92.04 ± 0.36
57.04 ± 0.82
99.42 ± 0.28
87.60 ± 0.67

AVERAGE ACCURACY

72.07 ± 0.47(72.27)

72.89 ± 0.09

72.82 ± 0.12

73.22 ± 0.33

Remark 5 For a given input sequence X := [x1, · · · , xN ]⊤ ∈ RN ×Dx of N feature vectors in
self-attention, in order to generate the sets of {ks
j=1 at the scale sth, we can downsample
the input X before projecting into the key matrix K and the value matrix V. There are multiples
approaches to downsampling X, such as using the average-pooling, max-pooling, 1-D convolution,
or K-means clustering. In this paper, we employ the average-pooling to downsample X.

j }Ns

j , vs

Linear Attention with Batch Normalization and Scaled Heads. The Attention-BN/SH can be
extended to use with the linear attention. In particular, in the Linear Attention-BN/SH, we replace the
softmax kernel in Eqn. 22 and Eqn. 25 by the linear kernel, respectively.

3 EXPERIMENTAL RESULTS

In this section, we empirically demonstrate the advantages of our Attention-BN, Attention-SH,
and their combination (Attention-BN+SH) over the baseline softmax attention on the UEA time-
series classification benchmark (Bagnall et al., 2018), the Long Range Arena benchmark (Tay et al.,
2021), and the image classification task on the Imagenet dataset (Deng et al., 2009; Russakovsky
et al., 2015). We aim to show that: (i) Attention-BN significantly outperforms the softmax baseline
across tasks; (ii) Attention-SH achieves better or comparable accuracy while saving computation
and memory compared to the baseline; (iii) Attention-BN+SH, which combines both Attention-BN
and Attention-SH, results in the best model performance in term of accuracy and efficiency; (iv)
all our proposed models help reduce the redundancy in multi-head attention and benefit learning
of the long-term dependency in long input sequences; (v) Attention-BN and Attention-SH can be
applied on other attention mechanisms beyond the softmax attention. When combined with the linear
attention (Katharopoulos et al., 2020), the resultant Linear Attention-BN and Linear Attention-SH
yield similar advantages mentioned in (i), (ii), (iii) and (iv) over the baseline linear attention.

In our experiments, we compare the proposed models with the baseline softmax and linear attentions
of the same configuration. For the Attention-BN and Attention-BN+SH, we observe that recentering
queries and keys alone is sufficient for improving the model performance. In addition, weighting µ
with a constant β, as in Eqn. 26 in the Appendix, enables the Attention-BN/BN+SH to adjust the
effect of normalization to the attention score and help increase the accuracy. Our results are averaged
over 5 runs. Details on datasets, models, and training are provided in Appendix A.

UEA Time Series Classification. Table 1 compares the accuracy of the Attention-BN and Attention-
SH with the baseline softmax attention on 10 tasks in the UEA Time Series Classification bench-
mark (Bagnall et al., 2018). Both Attention-BN and Attention-SH significantly outperform the
softmax baseline on most tasks and on average among all tasks. When combining two models, the
resulting Attention-BN+SH yields the best accuracy with more than 1% overall improvement over
the softmax baseline. Notably, the Attention-SH and Attention-BN+SH are much more efficient than
the baseline since they need much fewer keys and values in computing the attention output. The
efficiency advantage of the Attention-SH/BN+SH is discussed and analyzed in detail in Section 4.

Long Range Arena (LRA) benchmark. In this experiment, we verify the advantage of our methods
over the softmax baseline on tasks that involve very long sequences (e.g., the sequence length can
be up to 4K) in the LRA benchmark (Tay et al., 2021). Those tasks require the model to capture
long-range dependency in the input sequence. The summarized results in Table 2 indicate significant
improvements of Attention-BN/SH/BN+SH over the baseline softmax attention. Same as in the UEA

7

Published as a conference paper at ICLR 2023

Table 2: Test Accuracy (%) of the Attention-BN/SH/BN+SH vs. the baseline softmax attention on the LRA
benchmark (Tay et al., 2021). Our models significantly outperform the softmax baseline.

Dataset/Model

Baseline Softmax Attention-BN Attention-SH Attention-BN+SH

LISTOPS
TEXT
RETRIEVAL
IMAGE
PATHFINDER

36.76 ± 0.42
64.90 ± 0.07
79.68 ± 0.52
39.23 ± 1.35
72.72 ± 0.75

37.32 ± 0.06
65.07 ± 0.08
81.05 ± 0.08
39.75 ± 1.21
73.24 ± 0.67

37.08 ± 0.48
65.19 ± 0.19
80.74 ± 0.32
38.87 ± 1.24
74.00 ± 0.29

37.33 ± 0.53
65.03 ± 0.35
81.20 ± 0.11
39.18 ± 1.27
73.98 ± 0.55

AVERAGE ACCURACY

58.66 ± 0.26

59.28 ± 0.25

59.18 ± 0.22

59.35 ± 0.29

Table 3: Top-1 and top-5 accuracy (%) of the Attention-BN/SH/SH+BN Deit vs. the baseline softmax attention
Deit on the ImageNet benchmark. The Attention-BN Deit outperforms the baseline in terms of accuracy. The
Attention-SH/BN+SH Deit achieve comparable accuracy with the baseline while being more efficient.
Baseline Softmax Deit Attention-BN Deit Attention-SH Deit Attention-BN+SH Deit

Metric/Model

Top-1 Acc (%)
Top-5 Acc (%)

72.23 ± 0.23
91.13 ± 0.15

72.79 ± 0.21
91.43 ± 0.12

72.08 ± 0.23
91.05 ± 0.14

72.25 ± 0.22
91.14 ± 0.12

Figure 1: (Left) FLOPS ratios and (Right) memory usage ratios between the Attention-BN+SH and the
softmax baseline trained on retrieval task for different model dimensions and sequence lengths. The reduction in
computation and memory when using our models improves with sequence length. When scaling up the model,
our methods remain significantly more beneficial than the baseline.

Time Series experiment, on this LRA benchmark, Attention-BN and Attention-SH both outperform
the softmax attention on most five tasks. Moreover, Attention-BN+SH, which combines these two
attention mechanisms, results in the most accurate models on average across tasks. Specifically, for
the retrieval task, the most challenging task with the largest sequence length in the LRA benchmark,
Attention-BN+SH achieve a remarkable improvement of more than 1.5% over the baseline.

Image Classification on Imagenet. We corroborate the advantage of our proposed attention over
the baseline softmax attention when scaled up for the large-scale ImageNet image classification
task. We summarize the results in Table 3. The Deit model (Touvron et al., 2021) equiped with the
Attention-BN yields better performance than the softmax baseline. Meanwhile, Attention-SH/BN+SH
Deit perform on par with the baseline while being more efficient. These results, together with other
results above justify the benefits of our proposed methods across various tasks and data modalities,
proving the effectiveness of our primal-dual approach to develop new attentions.

4 EMPIRICAL ANALYSIS

Efficiency Analysis. The Attention-BN+SH not only improves the accuracy of the model remarkably
but also help reduce the computational and memory cost significantly. Fig.1 presents the efficiency
benefits of our Attention-BN+SH trained on the retrieval task when the model dimension D and
sequence lengths N grow. The efficiency advantage of our model increase as N increase. In addition,
the scaled-up models (with large D) remains significantly more efficient than the baseline. When
the model dimension is 64 and sequence length is 4096, which is the standard configuration of the
task, the model’s FLOPS, in both training and inference, reduce almost 25%, whereas the reductions
for memory usage in training and testing are 31.9% and 47.3%, respectively. Notably, this efficient
model also outperforms the baseline with more than 1.5% improvement in accuracy. These results
prove the benefits of applying the Attention-BN+SH for long-sequence tasks and large-scale models.

New Attentions Helps Reduce Head Redundancy. We compute the average L2 distances between
heads to analyze the attention diversity. Given our trained models for the retrieval task, the layer-
average mean and standard deviation of distances between heads are reported in Table 4. All our
introduced attentions attain greater L2 distances compared to the baseline, reducing the risk of
learning redundant heads. In particular, Attention-SH has the highest head difference, indicating the
model’s attention patterns are most spread out between heads.

8

Published as a conference paper at ICLR 2023

Table 4: Layer-average mean and standard deviation of L2 distances between heads of Attention-BN/SH/BN+SH
versus dot-product attention transformer trained on the retrieval task. Our attentions attain greater L2 distances
between heads than the baseline, suggesting that they capture more diverse attention patterns.
Baseline Softmax Attention-BN Attention-SH Attention-BN+SH

MetricModel

Mean
Std

2.01
0.39

2.45
0.66

3.81
0.75

3.19
1.01

Combining Attention-BN and Attention-SH with Other Attentions. Our methods can be extended
to combine with other attention mechanisms. We study the Linear Attention-BN/SH/BN+SH, that
combine the Attention-BN/SH/BN+SH with the linear attention (Katharopoulos et al., 2020) as
explained at the end of Section 2.3. We summarize our results in Table 5 in Appendix B.1.

5 RELATED WORK

Interpretation of Attention Mechanism. Recent works have focused on understanding the attention
mechanism in transformers from different perspectives. (Tsai et al., 2019) considers attention as
a weighted moving average over the inputs via a smoothing kernel. (Nguyen et al., 2022) draws
a connection between self-attention and nonparametric kernel regression. With this understand-
ing, the work explores better regression estimators, e.g.
the generalized Fourier nonparametric
regression estimator, to improve transformers. In addition, (Cao, 2021) then shows that the linear
transformer (Katharopoulos et al., 2020) corresponds to a Petrov-Galerkin projection (Reddy, 2004)
and proves that the softmax normalization in the softmax attention is sufficient but not necessary.
Other works that employ ordinary/partial differential equations to provide an interpretation for atten-
tion include (Lu et al., 2019; Sander et al., 2022). From a probabilistic perspective, (Tang & Matteson,
2021; Gabbur et al., 2021; Zhang & Feng, 2021) propose Gaussian mixture model frameworks to
study the self-attention in transformers. Using graph-structured learning and message passing in
graphical models is another attempt at understanding the attention mechanism Wang et al. (2018);
Shaw et al. (2018); Kreuzer et al. (2021). Optimization perspectives of attention mechanisms are
recently explored. (Sander et al., 2022) connects transformers with an optimization process across
iterations by specifically constructing the core energy function. (Sahiner et al., 2022) derive finite-
dimensional convex equivalence of attentions that can be solved for global optimality. Different from
these approaches, our primal-dual framework focuses on deriving attention as the dual expansion of a
primal neural network layer via solving a support vector regression problem. This framework allows
us to not only explain many different types of attention mechanisms but also create new ones.

Efficient Transformers. Recently, efficient transformers have been studied (Roy et al., 2021). Among
them are sparse transformers which incorporate sparse structures into the attention matrix (Parmar
et al., 2018; Liu et al., 2018; Qiu et al., 2019; Child et al., 2019; Beltagy et al., 2020). Another class
of efficient transformers are models that aim to have better coverage by integrating different access
patterns (Child et al., 2019; Ho et al., 2019), which can also be learned from the data (Kitaev et al.,
2020; Roy et al., 2021; Tay et al., 2020). An emerging body of work is proposed to distill and prune
the model, including (Sanh et al., 2019; Sun et al., 2019; Voita et al., 2019; Sajjad et al., 2020). In
other works, a side memory module is utilized in order to access multiple tokens simultaneously
(Lee et al., 2019; Sukhbaatar et al., 2019; Asai & Choi, 2020; Beltagy et al., 2020). Low-rank and
kernelization methods have been proposed to improve the efficiency of self-attention calculation (Tsai
et al., 2019; Wang et al., 2020; Katharopoulos et al., 2020; Choromanski et al., 2021; Shen et al.,
2021; Peng et al., 2021). Our Attention-SH/BN+SH is orthogonal to these methods.

6 CONCLUDING REMARKS

In this paper, we derive self-attention as a support vector expansion that solves a support vector
regression (SVR) problem and provide a principled primal-dual framework to analyze and synthesize
attention mechanisms. We then use our framework to invent two new attention mechanisms, the
Batch Normalized Attention (Attention-BN) and the Attention with Scaled Heads (Attention-SH),
that improve the accuracy and the efficiency of the baseline softmax attention. In our work, we
approximate and learn the dual variables αj and ˜αj using the value vector vj, j = 1, . . . , N in
self-attention. It is natural to include more inductive biases and structures of those dual variables from
solving the dual optimization problem of SVR into the value vectors vj. Furthermore, extending our
framework to explain the attention modules that compute the attention using neural network layers or
convolutional neural network layers applying on the input feature, such as the Convolutional Block
Attention Module (Woo et al., 2018), is an interesting research direction. We leave these exciting
research ideas as future work.

9

Published as a conference paper at ICLR 2023

ACKNOWLEDGEMENTS

This material is based on research sponsored by the NSF under Grant# 2030859 to the Computing
Research Association for the CIFellows Project (CIF2020-UCLA-38). SJO acknowledges support
from the ONR N00014-20-1-2093 and N00014-20-1-2787 and the NSF DMS 2208272 and 1952339.
RGB acknowledges support from the NSF grants CCF-1911094, IIS-1838177, and IIS-1730574;
ONR grants N00014-18-12571, N00014-20-1-2534, and MURI N00014-20-1-2787; AFOSR grant
FA9550-22-1-0060; and a Vannevar Bush Faculty Fellowship, ONR grant N00014-18-1-2047. ALB
acknowledges support from the NSF grants DMS-2152717 and DMS-1952339. NH acknowledges
support from the NSF IFML 2019844 and the NSF AI Institute for Foundations of Machine Learning.

REFERENCES

Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level
language modeling with deeper self-attention. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pp. 3159–3166, 2019.

Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid.
Vivit: A video vision transformer. In 2021 IEEE/CVF International Conference on Computer
Vision (ICCV), pp. 6816–6826, 2021. doi: 10.1109/ICCV48922.2021.00676.

Akari Asai and Eunsol Choi. Challenges in information seeking qa: Unanswerable questions and

paragraph retrieval. arXiv preprint arXiv:2010.11915, 2020.

Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In
International Conference on Learning Representations, 2019. URL https://openreview.net/forum?
id=ByxZX20qFQ.

Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul
Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018. arXiv
preprint arXiv:1811.00075, 2018.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly

learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.

Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.

arXiv preprint arXiv:2004.05150, 2020.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

Shuhao Cao. Choose a transformer: Fourier or galerkin. Advances in Neural Information Processing

Systems, 34, 2021.

Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel,
Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence
modeling. Advances in neural information processing systems, 34:15084–15097, 2021.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse

transformers. arXiv preprint arXiv:1904.10509, 2019.

Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder–
decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 1724–1734, Doha, Qatar, October 2014.
Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL https://www.aclweb.
org/anthology/D14-1179.

Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea
Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser,
David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with
In International Conference on Learning Representations, 2021. URL https:
performers.
//openreview.net/forum?id=Ua6zuk0WRH.

10

Published as a conference paper at ICLR 2023

Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look
at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP, pp. 276–286, Florence, Italy, August
2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https:
//www.aclweb.org/anthology/W19-4828.

Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273–297,

1995.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-
nov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint
arXiv:1901.02860, 2019.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal

transformers. arXiv preprint arXiv:1807.03819, 2018.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.
In International Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=YicbFdNTTy.

Prasad Gabbur, Manjot Bilkhu, and Javier Movellan. Probabilistic attention for interactive segmenta-

tion. Advances in Neural Information Processing Systems, 34, 2021.

Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo
Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for
speech recognition. arXiv preprint arXiv:2005.08100, 2020.

Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu.

Pct: Point cloud transformer. Computational Visual Media, 7(2):187–199, 2021.

Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-

mensional transformers. arXiv preprint arXiv:1912.12180, 2019.

Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015.

Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence
modeling problem. Advances in neural information processing systems, 34:1273–1286, 2021.

John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc¸ois Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. In International Conference on Machine
Learning, pp. 5156–5165. PMLR, 2020.

Yoon Kim, Carl Denton, Luong Hoang, and Alexander M Rush. Structured attention networks. arXiv

preprint arXiv:1702.00887, 2017.

11

Published as a conference paper at ICLR 2023

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv

preprint arXiv:2001.04451, 2020.

Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L´etourneau, and Prudencio Tossou.
Rethinking graph transformers with spectral attention. Advances in Neural Information Processing
Systems, 34, 2021.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set trans-
former: A framework for attention-based permutation-invariant neural networks. In International
Conference on Machine Learning, pp. 3744–3753. PMLR, 2019.

Zhouhan Lin, Minwei Feng, C´ıcero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and
Yoshua Bengio. A structured self-attentive sentence embedding. CoRR, abs/1703.03130, 2017.
URL http://arxiv.org/abs/1703.03130.

Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. Learn-
ing long-range spatial dependencies with horizontal gated recurrent units.
In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances
in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL
https://proceedings.neurips.cc/paper/2018/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf.

Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198,
2018.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.

Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.

Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin
transformer. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.

Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and Tie-Yan Liu.
Understanding and improving transformer from a multi-particle dynamic system point of view.
arXiv preprint arXiv:1906.02762, 2019.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, pp. 142–150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL https:
//www.aclweb.org/anthology/P11-1015.

Nikita Nangia and Samuel Bowman. ListOps: A diagnostic dataset for latent tree learning. In
Proceedings of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Student Research Workshop, pp. 92–99, New Orleans, Louisiana,
USA, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-4013. URL
https://www.aclweb.org/anthology/N18-4013.

Tam Minh Nguyen, Tan Minh Nguyen, Dung DD Le, Duy Khuong Nguyen, Viet-Anh Tran, Richard
Baraniuk, Nhat Ho, and Stanley Osher. Improving transformers with probabilistic attention keys.
In International Conference on Machine Learning, pp. 16595–16621. PMLR, 2022.

Ankur Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention
model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pp. 2249–2255, Austin, Texas, November 2016. Association for
Computational Linguistics. doi: 10.18653/v1/D16-1244. URL https://www.aclweb.org/anthology/
D16-1244.

12

Published as a conference paper at ICLR 2023

Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 4055–4064. PMLR, 10–15 Jul 2018. URL http://proceedings.mlr.press/
v80/parmar18a.html.

Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong.
Random feature attention. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=QtTKTdVrFBB.

Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise

self-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019.

Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl

anthology network corpus. Language Resources and Evaluation, 47(4):919–944, 2013.

Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-

standing by generative pre-training. OpenAI report, 2018.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language

models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International Conference on Machine Learning, pp.
8748–8763. PMLR, 2021.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL
http://jmlr.org/papers/v21/20-074.html.

Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine
Learning, pp. 8821–8831. PMLR, 2021.

JN Reddy. An introduction to the finite element method, volume 1221. McGraw-Hill New York,

2004.

Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo,
Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from
scaling unsupervised learning to 250 million protein sequences. Proceedings of the National
Academy of Sciences, 118(15), 2021.

Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse
attention with routing transformers. Transactions of the Association for Computational Linguistics,
9:53–68, 2021. URL https://www.aclweb.org/anthology/2021.tacl-1.4.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International Journal of Computer Vision, 115(3):211–252, 2015.

Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Un-
raveling attention via convex duality: Analysis and interpretations of vision transformers. In
Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato
(eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of
Proceedings of Machine Learning Research, pp. 19050–19088. PMLR, 17–23 Jul 2022. URL
https://proceedings.mlr.press/v162/sahiner22a.html.

Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. Poor man’s bert: Smaller and faster

transformer models. arXiv e-prints, pp. arXiv–2004, 2020.

13

Published as a conference paper at ICLR 2023

Michael E Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyr´e. Sinkformers: Transformers with
doubly stochastic attention. In International Conference on Artificial Intelligence and Statistics,
pp. 3515–3530. PMLR, 2022.

Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of

bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

Bernhard Sch¨olkopf, Alexander J Smola, Francis Bach, et al. Learning with kernels: support vector

machines, regularization, optimization, and beyond. MIT press, 2002.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.
In Proceedings of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 464–468,
New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/
N18-2074. URL https://aclanthology.org/N18-2074.

Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention:
In Proceedings of the IEEE/CVF Winter Conference on

Attention with linear complexities.
Applications of Computer Vision, pp. 3531–3539, 2021.

Alex J Smola and Bernhard Sch¨olkopf. A tutorial on support vector regression. Statistics and

computing, 14(3):199–222, 2004.

Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. Aug-

menting self-attention with persistent memory. arXiv preprint arXiv:1907.01470, 2019.

Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model

compression. arXiv preprint arXiv:1908.09355, 2019.

Chang Wei Tan, C. Bergmeir, Franc¸ois Petitjean, and Geoffrey I. Webb. Monash university, uea, ucr

time series regression archive. ArXiv, abs/2006.10996, 2020.

Binh Tang and David S. Matteson. Probabilistic transformer for time series analysis. In A. Beygelz-
imer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information
Processing Systems, 2021. URL https://openreview.net/forum?id=HfpNVDg3ExA.

Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse Sinkhorn attention.
In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on
Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 9438–9447.
PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/tay20a.html.

Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient
In International Conference on Learning Representations, 2021. URL https:
transformers.
//openreview.net/forum?id=qVyeW-grC2k.

Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.
4593–4601, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/
v1/P19-1452. URL https://www.aclweb.org/anthology/P19-1452.

Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv´e
J´egou. Training data-efficient image transformers and distillation through attention. arXiv preprint
arXiv:2012.12877, 2020.

Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv´e

J´egou. Training data-efficient image transformers distillation through attention, 2021.

Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhut-
dinov. Transformer dissection: An unified understanding for transformer’s attention via the lens of
kernel. arXiv preprint arXiv:1908.11775, 2019.

14

Published as a conference paper at ICLR 2023

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
In Advances in neural information

Kaiser, and Illia Polosukhin. Attention is all you need.
processing systems, pp. 5998–6008, 2017.

Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language
model. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP, pp. 63–76, Florence, Italy, August 2019. Association for Computational
Linguistics. doi: 10.18653/v1/W19-4808. URL https://www.aclweb.org/anthology/W19-4808.

Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head
self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint
arXiv:1905.09418, 2019.

Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with

linear complexity. arXiv preprint arXiv:2006.04768, 2020.

Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794–7803,
2018.

Zifeng Wang and Jimeng Sun. TransTab: Learning Transferable Tabular Transformers Across Tables.

In Advances in Neural Information Processing Systems (NeurIPS 2022), 2022.

Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block
attention module. In Proceedings of the European conference on computer vision (ECCV), pp.
3–19, 2018.

Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:
Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 22–31, 2021.

Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Flowformer: Linearizing
transformers with conservation flows. In International Conference on Machine Learning, 2022.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V
Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint
arXiv:1906.08237, 2019.

George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff.
A transformer-based framework for multivariate time series representation learning. In Proceedings
of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 2114–2124,
2021.

Shaolei Zhang and Yang Feng. Modeling concentrated cross-attention for neural machine translation
with Gaussian mixture model. In Findings of the Association for Computational Linguistics:
EMNLP 2021, pp. 1401–1411, Punta Cana, Dominican Republic, November 2021. Association for
Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.121. URL https://aclanthology.
org/2021.findings-emnlp.121.

Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep learning based recommender system: A survey

and new perspectives. ACM Computing Surveys (CSUR), 52(1):1–38, 2019.

Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16259–16268,
2021.

Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar,
and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. In
NeurIPS, 2021.

15

Published as a conference paper at ICLR 2023

Supplement to “A Primal-Dual Framework for Transformers and
Neural Networks”

A ADDITIONAL DETAILS ON THE EXPERIMENTS

This section provides datasets, models, and training details for experiments in Section 3. As mentioned
in Section 3, for Attention-BN models, recentering queries and keys alone is sufficient for accuracy
improvement, and we weight the mean µ in Eqn 22 with a constant β. Hence Eqn 22 is simplified to:

hi =

N
(cid:88)

j=1

(cid:16)

softmax

(qi − βµ)⊤(kj − βµ)/

√

(cid:17)

D

vj.

(26)

In our experiments, we consider the constant β in Attention-BN/BN+SH and the different downsam-
pling scales in Attention-SH/SH+BN as hyper-parameters to finetune. All of our experiments are
conducted on a server with 4 NVIDIA A100 GPUs.

A.1 UEA TIME SERIES CLASSIFICATION

Datasets and metrics The benchmark (Bagnall et al., 2018) consists of 30 datasets. Following (Wu
et al., 2022), we choose 10 datasets, which vary in input sequence lengths, the number of classes,
and dimensionality, to evaluate our models on temporal sequences. We report the test accuracy as
evaluation for the benchmark.

Models and baselines The experiment setups and configurations for the softmax/linear baseline
and our models are the same as in (Wu et al., 2022) 1 (for the PEMS-SF, SelfRegulationSCP2,
UWaveGestureLibrary datasets) and (Zerveas et al., 2021) 2 (for the other tasks). In all models, the
number of heads is 8, whereas the model dimension and number of transformer layers are varied. For
Attention-SH/SH+BN, we downsample keys and values by the factor of 2, after every two successive
heads.

A.2 LONG RANGE ARENA BENCHMARK

Datasets and metrics We adopt the tasks: Listops (Nangia & Bowman, 2018), byte-level IMDb
reviews text classification (Maas et al., 2011), byte-level document retrieval (Radev et al., 2013),
CIFAR-10 image classification (Krizhevsky et al., 2009) and the Pathfinder challenge (Linsley et al.,
2018) in the LRA benchmark for our experiments. They consist of long sequences of length 2K,
4K, 4K, 1K, and 1K respectively. The evaluation protocol and metric are the same as in (Tay et al.,
2021).

Models and baselines All our models and softmax/linear baselines follow the same architecture
and configuration as in (Zhu et al., 2021)3. Each model consists of two layers and 64 embedding
dimensions. While one head at each layer remains intact, the keys and values of the other heads are
halved in our Attention-SH/SH+BN experiments.

A.3

IMAGE CLASSIFICATION ON IMAGENET

Datasets and metrics The ImageNet dataset (Deng et al., 2009; Russakovsky et al., 2015) consists
of 1.28M training images and 50K validation images. The task is to classify 1000 categories. Top-1
and top-5 accuracies are reported.

Models and baselines Our baseline is DeiT-tiny model (Touvron et al., 2021) with 12
transformer layers, 4 attention heads per layer, and the model dimension of 192. For model setting
and setting and configuration, we follow (Touvron et al., 2021)4. The downsampling scales in
Attention-SH/BN+SH models are [1, 1, 2, 4] for 4 heads at each layer, respectively.

1Implementation available at https://github.com/thuml/Flowformer.
2Implementation available at https://github.com/gzerveas/mvts transformer.
3Implementation available at https://github.com/NVIDIA/transformer-ls.
4Implementation available at https://github.com/facebookresearch/deit.

16

Published as a conference paper at ICLR 2023

Table 5: Test Accuracy (%) of the Linear Attention-BN/SH/BN+SH vs. the baseline Linear Atten-
tion (Katharopoulos et al., 2020) on the UEA Time Series Classification Archive benchmark (Bagnall et al.,
2018). Our proposed attentions outperform the baseline.

Dataset/Model

Baseline Linear

Linear Attention-BN Linear Attention-SH Linear Attention-BN+SH

ETHANOLCONCENTRATION
FACEDETECTION
HANDWRITING
HEARTBEAT
JAPANESEVOWELS
PEMS-SF
SELFREGULATIONSCP1
SELFREGULATIONSCP2
SPOKENARABICDIGITS
UWAVEGESTURELIBRARY

AVERAGE ACCURACY

33.84 ± 0.66
69.17 ± 0.32
32.87 ± 0.27
75.61 ± 0.73
99.37 ± 0.16
83.43 ± 0.88
90.90 ± 0.40
55.18 ± 0.89
99.07 ± 0.10
85.63 ± 0.81

72.51 ± 0.34

34.98 ± 0.74
69.22 ± 0.17
32.86 ± 0.49
75.78 ± 0.71
99.60 ± 0.19
85.74 ± 0.67
91.81 ± 0.69
56.11 ± 0.94
99.01 ± 0.07
86.04 ± 0.86

73.12 ± 0.20

34.76 ± 0.69
69.38 ± 0.17
32.82 ± 0.12
74.96 ± 0.62
99.28 ± 0.41
86.51 ± 0.88
90.76 ± 0.59
54.44 ± 0.88
99.03 ± 0.18
84.89 ± 1.00

72.68 ± 0.40

34.35 ± 0.70
69.12 ± 0.19
32.98 ± 0.36
75.94 ± 0.68
99.33 ± 0.19
84.97 ± 0.76
91.92 ± 0.60
55.74 ± 0.92
98.91 ± 0.17
85.78 ± 0.75

72.90 ± 0.23

Table 6: Top-1 and top-5 accuracy (%) of the Attention-Conv2D Deit vs. the baseline Deit with the softmax
attention on the ImageNet image classification task. The Attention-Conv2D Deit significantly outperforms the
baseline in both top-1 and top-5 accuracy.

Metric/Model

Baseline Softmax Deit Attention-Conv2D Deit

Top-1 Acc (%)
Top-5 Acc (%)

72.23 ± 0.23
91.13 ± 0.15

73.18 ± 0.24
91.52 ± 0.13

Table 7: Test Accuracy (%) of the Attention-Conv1D vs. the baseline softmax attention on 5 tasks of the LRA
benchmark (Tay et al., 2021). Our models outperform the softmax baseline.

Dataset/Model

Baseline Softmax Attention-Conv1D

LISTOPS
TEXT
RETRIEVAL
IMAGE
PATHFINDER

36.76 ± 0.42
64.90 ± 0.07
79.68 ± 0.52
39.23 ± 1.35
72.72 ± 0.75

37.20 ± 0.05
64.92 ± 0.44
80.75 ± 0.15
39.18 ± 0.59
73.01 ± 0.24

AVERAGE ACCURACY

58.66 ± 0.26

59.01 ± 0.20

B ADDITIONAL EXPERIMENTAL RESULTS

B.1 UEA TIME SERIES CLASSIFICATION USING THE LINEAR ATTENTION-BN/SH/BN+SH

Table 5 summarizes the comparison between the Linear Attention-BN/SH/BN+SH and the baseline
Linear Attention on the UEA Time Series Classification task. The Linear Attention-BN/SH/BN+SH
achieve better accuracy than the Linear Attention baseline while being more efficient.

B.2 CONVOLUTION ATTENTION

Table 6 demonstrates the advantage of Attention-Conv2D (Def. 3, Section G) over softmax Deit on
the ImageNet image classification task. Furthemore, as shown in Table 7, the Attention-Conv1D
(Def. 4, Section G) outperforms the baseline softmax attention on 5 tasks of the LRA benchmark (Tay
et al., 2021).

B.3 ADDITIONAL EXPERIMENTS ON THE UEA TIMESERIES CLASSIFICATION BENCHMARK

AND THE UCR TIME SERIES REGRESSION ARCHIVE

In this section, we further demonstrate the advantage of our Attention-BN/SH/BN+SH on additional
15 tasks in the UEA Time Series Classification benchmark and on 6 tasks in the UCR Time Series
Regression benchmark. The results in Table 8 and 9 show that our Attention-BN and Attention-
SH+BN outperform the baseline softmax transformers significantly on both of these benchmarks,
while the attention-SH has comparable performance with the baseline but being more effiicient.

B.4 UEA TIME SERIES CLASSIFICATION USING THE SPARSE ATTENTION-BN/SH/BN+SH

Table 10 summarizes the comparison between the Sparse Attention-BN/SH/BN+SH and the Sparse
Attention baseline on a subset of the UEA Time Series Classification benchmark. Our models
when combined with Sparse Attention achieve significantly better accuracy than the Sparse Atten-
tion baseline while the Sparse Attention-SH/BN+SH are more efficient (See Fig. 3 and Fig. 4 in
Appendix C).

17

Published as a conference paper at ICLR 2023

Table 8: Root mean square error (RMSE) of the Attention-BN/SH/BN+SH vs. the baseline softmax attention
on 6 UCR Time Series Regression tasks (Tan et al., 2020). Smaller RMSE indicates better performance.

Dataset/Model

Baseline Softmax Attention-BN Attention-SH Attention-BN+SH

APPLIANCESENERGY
BENZENECONCENTRATION
BEIJINGPM10
BEIJINGPM25
LIVEFUELMOISTURE
IEEEPPG

3.44 ± 0.06
0.91 ± 0.03
92.31 ± 1.06
59.73 ± 1.21
43.08 ± 0.17
32.12 ± 1.25

3.38 ± 0.34
0.89 ± 0.17
92.00 ± 0.89
59.55 ± 0.92
43.01 ± 0.50
30.69 ± 0.64

3.39 ± 0.02
1.00 ± 0.09
92.82 ± 0.92
59.66 ± 0.88
43.65 ± 0.09
31.38 ± 1.02

3.37 ± 0.23
0.90 ± 0.08
92.40 ± 0.85
59.24 ± 1.22
43.79 ± 0.49
30.73 ± 1.20

AVERAGE RMSE

38.60 ± 0.67

38.25 ± 0.30

38.65 ± 0.27

38.40 ± 0.51

Table 9: Accuracy (%) of the Attention-BN/SH/BN+SH vs. the baseline softmax attention on other 15 UEA
Time Series classification tasks (Bagnall et al., 2018).

Dataset/Model

Baseline Softmax Attention-BN Attention-SH Attention-BN+SH

ARTICULARYWORDRECOGNITION
BASICMOTIONS
EPILEPSY
ERING
FINGERMOVEMENTS
LIBRAS
NATOPS
RACKETSPORTS
ATRIALFIBRILLATION
CRICKET
STANDWALKJUMP
HANDMOVEMENTDIRECTION
LSST
DUCKDUCKGEESE
MOTORIMAGERY

97.44 ± 0.42
98.75 ± 1.25
93.71 ± 1.23
95.18 ± 0.52
59.67 ± 0.47
85.00 ± 0.45
95.00 ± 0.45
87.28 ± 0.82
33.33 ± 2.71
94.90 ± 0.65
50.00 ± 2.33
63.96 ± 2.30
58.54 ± 0.54
64.50 ± 1.96
58.66 ± 1.25

98.22 ± 0.87
99.38 ± 1.08
92.27 ± 0.74
95.18 ± 0.37
63.00 ± 0.41
85.37 ± 0.69
95.37 ± 0.26
87.93 ± 0.31
41.67 ± 2.88
95.37 ± 0.65
55.55 ± 2.14
64.41 ± 2.76
57.05 ± 0.26
65.00 ± 1.73
60.67 ± 1.69

97.22 ± 0.95
99.37 ± 1.06
89.13 ± 1.07
94.72 ± 0.66
61.33 ± 0.70
83.88 ± 0.45
96.29 ± 0.69
88.16 ± 0.54
35.00 ± 2.89
93.98 ± 0.65
50.01 ± 2.34
61.71 ± 2.64
60.34 ± 0.73
64.50 ± 1.95
59.00 ± 1.41

98.44 ± 0.41
99.78 ± 0.51
92.02 ± 1.02
95.46 ± 0.40
63.66 ± 0.64
85.00 ± 0.78
95.74 ± 0.94
89.03 ± 0.64
41.68 ± 2.80
96.29 ± 0.65
55.00 ± 2.08
66.66 ± 2.54
59.91 ± 0.34
65.50 ± 1.66
62.00 ± 0.81

AVERAGE ACCURACY

75.73 ± 0.51

77.10 ± 0.22

75.61 ± 0.18

77.74 ± 0.24

Table 10: Test Accuracy (%) of the Sparse Attention-BN/SH/BN+SH vs. the baseline Sparse Attention (Child
et al., 2019) on a subset of the UEA Time Series Classification Archive benchmark (Bagnall et al., 2018). Our
proposed attentions outperform the baseline.
Baseline Sparse

Sparse Attention-BN Sparse Attention-SH Sparse Attention-BN+SH

Dataset/Model

ETHANOLCONCENTRATION
FACEDETECTION
HANDWRITING
HEARTBEAT
JAPANESEVOWELS
PEMS-SF
SELFREGULATIONSCP1
SELFREGULATIONSCP2
SPOKENARABICDIGITS
UWAVEGESTURELIBRARY

AVERAGE ACCURACY

33.33 ± 1.23
68.58 ± 0.95
31.08 ± 0.38
74.95 ± 0.81
99.45 ± 0.10
82.08 ± 0.63
91.24 ± 0.85
55.18 ± 0.69
99.04 ± 0.06
84.90 ± 0.39

71.98 ± 0.38

33.33 ± 0.78
68.65 ± 0.44
31.79 ± 0.44
75.98 ± 0.72
99.54 ± 0.12
83.81 ± 0.47
91.69 ± 0.42
58.52 ± 0.71
99.10 ± 0.15
85.73 ± 0.38

72.81 ± 0.15

32.50 ± 0.57
68.67 ± 0.78
32.75 ± 0.39
74.96 ± 0.80
99.18 ± 0.14
82.66 ± 0.63
91.47 ± 0.84
55.92 ± 0.94
99.06 ± 0.13
85.31 ± 0.88

72.25 ± 0.24

33.46 ± 0.71
68.44 ± 0.51
33.37 ± 0.61
76.09 ± 0.75
99.36 ± 0.34
84.01 ± 0.89
91.70 ± 0.16
56.67 ± 0.68
99.15 ± 0.09
86.56 ± 0.25

72.88 ± 0.35

Table 11: Test Accuracy (%) of the Attention-BN/BN+SH with β is learnable or set as a hyperparameter on the
retrieval task (Tay et al., 2021).

Model

Attention-BN (learn β)
Attention-BN+SH (learn β)
Attention-BN (β as a hyperparameter)
Attention-BN+SH (β as a hyperparameter)

Retrieval

80.77 ± 0.23
81.31 ± 0.25
81.05 ± 0.08
81.20 ± 0.11

B.5 ATTENTION-BN/BN+SH WITH LEARNABLE β

We experiment with our Attention-BN/BN+SH with learnable β on the retrieval task. Table 11 shows
that learning β does not improve much over setting β to be a hyperparameter.

C ADDITIONAL RESULTS ON EFFICIENCY ANALYSIS

This section provides more efficiency analysis on our models.

18

Published as a conference paper at ICLR 2023

Figure 2: (Left) FLOPS ratios and (Right) memory usage ratios between the Attention-SH and the softmax
attention baseline trained on the LRA retrieval task for different model dimensions and sequence lengths.

Figure 3: (Left) FLOPS ratios and (Right) memory usage ratios between the Sparse Attention-BN+SH and
the Sparse Attention baseline trained on the LRA retrieval task for different model dimensions and sequence
lengths. When using our models, the reduction in computation and memory improves with sequence length.
When scaling up the model with greater model dimension, our methods remain significantly more efficient than
the baseline.

Figure 4: (Left) FLOPS ratios and (Right) memory usage ratios between the Sparse Attention-SH and the
Sparse Attention baseline trained on the LRA retrieval task for different model dimensions and sequence lengths.
When using our models, the reduction in computation and memory improves with sequence length. When
scaling up the model with greater model dimension, our methods remain significantly more efficient than the
baseline.
Attention-SH. Fig.2 shows the efficiency benefits of our Attention-SH when trained on the retrieval
task. Same as in the case of Attention-SH+BN, the efficiency benefits of our Attention-SH over the
baseline Softmax attention grows when N and D increase.

Sparse Attention-SH/BN+SH. Fig.3 and Fig.4 show that the efficiency advantages of our Sparse
Attention-BN+SH and Sparse Attention-SH, respectively, increase as the model dimension D and
sequence length N grow. All models are trained on the LRA retrieval task. In addition to the efficiency
advantage, the Sparse Attention-BN+SH also significantly outperforms the Sparse Attention baseline
in terms of accuracy in this task (79.86% vs. 78.20%) while the Sparse Attention-SH achieves a
comparable result to the baseline. More accuracy advantages of the Sparse Attention-BN/SH/BN+SH
over the Sparse Attention baseline are given in Table 10.

D DERIVING SOFTMAX ATTENTION.

Choosing the appropriate h(x) and Φ(x) allows us to derive the popular softmax attention given in
Eqn. 1 and 2. In particular, if we choose h(x) := (cid:80)N

j Φ(x)T Φ(kj), Eqn. 10 becomes

f (x) =

N
(cid:88)

j=1

Φ(x)⊤Φ(kj)
j′ Φ(x)T Φ(kj′)

(cid:80)N

vj + b =

(cid:80)N

j=1 Φ(x)⊤Φ(kj)vj
(cid:80)N
j′ Φ(x)T Φ(kj′)

+ b.

(27)

19

Published as a conference paper at ICLR 2023

We then select Φ(x) =

(cid:16)

, a(1)

1 , . . . , a(1)
a(0)
, . . . , a(t)
l0
l1
√
√
D)n1 . . . (xD/ 4
(x1/ 4
√
n1! . . . nD!

D)nD

a(t)
l =

Since

1 , . . . , a(t)
lt

(cid:17)

, . . .

where lt = (cid:0)D+t−1

(cid:1) and

t

| n1 + · · · + nD = t, 1 ≤ l ≤ lt.

exp (xT y) =

∞
(cid:88)

t=0

(xT y)t
t!

∞
(cid:88)

=

(cid:88)

t=0

n1+···+nD=t

(cid:18) xn1
√

1 . . . xnD
n1! . . . nD!

D

(cid:19) (cid:18) yn1
√

1 . . . ynD
n1! . . . nD!

D

(28)

(29)

(cid:19)

,

then Eqn. 27 becomes

f (x) =

N
(cid:88)

j=1

=

N
(cid:88)

j=1

(cid:80)∞

t=0

(cid:80)

n1+···+nD=t

(cid:32) (cid:16) x1
4√

D

(cid:17)n1 ...
(cid:16) xD
4√
D
√
n1!...nD!

(cid:17)nD

(cid:33) (cid:32) (cid:16) kj1
4√

D

(cid:17)n1

...

(cid:16) kjD
4√
D
n1!...nD!

√

(cid:17)nD

(cid:33)

(cid:80)N

j′=1

(cid:80)∞

t=0

(cid:80)

n1+···+nD=t

(cid:32) (cid:16) x1
4√

D

(cid:17)nD

(cid:16) xD
(cid:17)n1 ...
4√
D
√
n1!...nD!

(cid:33) 


(cid:18) k
j′ 1
4√
D

(cid:19)n1

(cid:18) k

...

j′D
4√
D
n1!...nD!

√

(cid:19)nD



vj + b



exp

(cid:17)⊤

(cid:18)(cid:16) x
4√
(cid:18)(cid:16) x
4√

D

D

(cid:19)

kj
4√
D
(cid:17)⊤ kj′
4√
D

(cid:19) vj + b =

N
(cid:88)

j=1

√

exp (x⊤kj/
D)
√
j′=1 exp (x⊤kj′/

(cid:80)N

vj + b.

(30)

D)

(cid:80)N

j′=1 exp

Let x = qi, b = 0 and relax the boundness constraint of vj in Remark 1. Eqn. 30 becomes Eqn. 2 of
the softmax attention (Vaswani et al., 2017).

E BATCH NORMALIZED ATTENTION: DERIVATION OF EQN. 24

hi =

=

=

=

=

=

=

N
(cid:88)

j=1

N
(cid:88)

j=1

N
(cid:88)

j=1

N
(cid:88)

j=1

N
(cid:88)

j=1

N
(cid:88)

j=1

N
(cid:88)

j=1

d=1
(cid:16)(cid:80)D

exp

(cid:80)N

j′=1 exp

(cid:16)(cid:80)D

exp

(cid:80)N

j′=1 exp

(cid:16)(cid:80)D

exp

(cid:80)N

j′=1 exp

softmax

softmax

(cid:32) D
(cid:88)

d=1
(cid:32) D
(cid:88)

d=1

(cid:32) D
(cid:88)

d=1
(cid:32) D
(cid:88)

softmax

softmax

(qi(d) − µ(d))(kj(d) − µ(d))

√

D(σ2

d + ϵ)

(cid:33)

vj

qi(d)kj(d) − qi(d)µ(d) − µ(d)kj(d) + µ(d)µ(d)

√

D(σ2

d + ϵ)

(cid:33)

vj

d=1
(cid:16)(cid:80)D

√

qi(d)kj (d)−qi(d)µ(d)−µ(d)kj (d)+µ(d)µ(d)
D(σ2

d+ϵ)
qi(d)kj′ (d)−qi(d)µ(d)−µ(d)kj′ (d)+µ(d)µ(d)
D(σ2
(cid:17)

d+ϵ)

√

d=1

(cid:17) vj

(cid:17)

(cid:16)(cid:80)D

d=1
(cid:16)(cid:80)D

exp

µ(d)µ(d)−qi(d)µ(d)
D(σ2

√

d+ϵ)

(cid:17)

d=1

µ(d)µ(d)−qi(d)µ(d)
D(σ2

√

d+ϵ)

(cid:17) vj

d=1
(cid:16)(cid:80)D

qi(d)kj (d)−µ(d)kj (d)
D(σ2

√

d+ϵ)
qi(d)kj′ (d)−µ(d)kj′ (d)
D(σ2

√

exp
(cid:17)

d+ϵ)

d=1

(cid:17)

d=1
(cid:16)(cid:80)D

qi(d)kj (d)−µ(d)kj (d)
D(σ2

√

d+ϵ)
qi(d)kj′ (d)−µ(d)kj′ (d)
D(σ2

√

d+ϵ)

d=1

(cid:17) vj

qi(d)kj(d) − µ(d)kj(d)

√

D(σ2

d + ϵ)

(cid:33)

vj

(cid:80)N

qi(d)kj(d) − 1
N
D(σ2

√

j′=1 kj′(d)kj(d)
d + ϵ)

(cid:33)

vj.

(31)

F ATTENTION WITH THE RESIDUAL CONNECTION AND MATRIX

PROJECTIONS

In this supplement, we first discuss attention with the residual connection and matrix projections in
Appendix F.
Suppose we are given a training data {(x1, y1), . . . , (xN , yN )} ⊂ X × Y, where X = RDx and
Y = RDv . Here, x1, . . . , xN are the training inputsd, and y1, . . . , yN are the training targets. In

20

Published as a conference paper at ICLR 2023

order to derive the attention with the residual connection and query, key, and value matrix projections,
we define the function f as follows

y = f (x) := W

+ x + b,

(32)

Φ(Wprojx)
h(x)

where x ∈ X = RDx, Wproj = [wproj
D ]⊤ ∈ RD×Dx , Φ(·) = [ϕ1(·), . . . , ϕDϕ(·)] : RD →
RDϕ, W = [w1, . . . , wDv ]⊤ ∈ RDv×Dϕ, b ∈ RDv , and h(x) is a vector-scalar function. We fit
the function f to the training data {(x1, y1), . . . , (xN , yN )} with an L2 regularization on W and
Wproj by solving the following convex optimization problem:
(cid:16)

, . . . , wproj

1

N
(cid:88)

Dv(cid:88)

Dv(cid:88)

1
2

Dv(cid:88)

d=1

∥wd∥2 +

∥wproj

d ∥2 + C

1
2

d=1

j=1

d=1

(cid:17)
ξj(d) + ˜ξj(d)

minimize
W,Wproj
ξj , ˜ξj ,j=1,...,N

subject to






d Φ(Wprojxj)/h(xj) − xj − b(d) ≤ ϵ + ξj(d)
d Φ(Wprojxj)/h(xj) + xj + b(d) − yj(d) ≤ ϵ + ˜ξj(d)

yj(d) − w⊤
w⊤
ξj(d), ˜ξj(d) ≥ 0

The Lagrangian of the optimization problem 33 is given by

L1 :=

1
2

Dv(cid:88)

d=1

∥wd∥2 +

1
2

Dv(cid:88)

d=1

∥wproj

d ∥2 + C

N
(cid:88)

Dv(cid:88)

j=1

d=1

(cid:16)

(cid:17)
ξj(d) + ˜ξj(d)

−

−

−

N
(cid:88)

Dv(cid:88)

j=1

d=1

N
(cid:88)

Dv(cid:88)

j=1

d=1

(cid:18)

αj(d)

(cid:18)

˜αj(d)

ϵ + ξj(d) − yj(d) + w⊤
d

ϵ + ˜ξj(d) + yj(d) − w⊤
d

Φ(Wprojxj)
h(xj)

Φ(Wprojxj)
h(xj)

+ xj + b(d)

(cid:19)

− xj − b(d)

,

N
(cid:88)

Dv(cid:88)

j=1

d=1

(cid:19)

, j = 1, . . . , N, d = 1, . . . , Dv.

(33)

(cid:16)

(cid:17)
ηj(d)ξj(d) + ˜ηj(d) ˜ξj(d)

(34)
Similar to the derivation in Section 2.1, the partial derivatives of L1 with respect to the primal variable
wd, d = 1, . . . , Dv, have to vanish for optimality, which leads to

∂wd L1 = wd −

N
(cid:88)

(αj(d) − ˜αj(d))

j=1

Φ(Wprojxj)
h(xj)

= 0 ⇒ wd =

N
(cid:88)

j=1

(αj(d) − ˜αj(d))

Φ(Wprojxj)
h(xj)

.

(35)
Note that here we only find the form of the optimal solution for W = [w1, . . . , wDv ]⊤. The optimal
value of Wproj can then be found by optimization algorithm such as the (stochastic) gradient descent
when training the transformer.
Let vj = [ αj (1)− ˜αj (1)
expansion of the function f :

]⊤, j = 1, . . . , N , we obtain the following support vector

, . . . , αj (Dv)− ˜αj (Dv)

h(xj )

h(xj )

f (x) =

(αj(1) − ˜αj(1))





N
(cid:88)

j=1

Φ(Wprojxj)
h(xj)

, . . . ,

N
(cid:88)

j=1

(αj(Dv) − ˜αj(Dv))

Φ(Wprojxj)
h(xj)



⊤



Φ(Wprojx)
h(x)

+ x + b,





N
(cid:88)

j=1

αj(1) − ˜αj(1)
h(xj)

Φ(Wprojx)⊤Φ(Wprojxj)
h(x)

, . . . ,

N
(cid:88)

j=1

αj(Dv) − ˜αj(Dv)
h(xj)

Φ(Wprojx)⊤Φ(Wprojxj)
h(x)



⊤



+ x + b,

=

=

N
(cid:88)

j=1
(cid:124)

Φ(Wprojx)⊤Φ(Wprojxj)
h(x)

vj + x

+b.

(36)

(cid:123)(cid:122)
Residual connection

(cid:125)

Here, the support vector expansion of f already includes a residual connection. The softmax attention
can then be derived by selecting h(x) := (cid:80)N
j Φ(Wprojx)T Φ(Wprojxj) and choosing Φ as in
Eqn. 28 in Section 2.1. Note that in Eqn. 36, {xj}N
j=1 and x are the training samples and test sample,
respectively. In order to derive the key, query, and value matrix projections in attention, we can then
relax Eqn. 36 by letting Wprojxj = WKxj, Wprojx = WQx, vj = WV xj and choosing the test
sample x among the training samples {xj}N

j=1.

21

Published as a conference paper at ICLR 2023

Remark 6 Here, for self-attention, we choose the test sample x among the training samples {xj}N
j=1
to compute the attention score of a token to other tokens in the same sequence. For cross-attention
where a token in a sequence attends to tokens in another sequence, this constraint can be removed.

G 2D-CONVOLUTION ATTENTION

1

1

, ytrain

), . . . , (xtrain

, ytrain
1
, . . . , xtrain

In this section, we discuss attention with 2D-convolution. Suppose we are given a training
)} ⊂ X × Y, where X = RDx and Y = RDv .
data {(xtrain
Here, xtrain
are the training tar-
gets. Let Xtrain ∈ RNH ×NW ×Dx be the 3D-tensor of training inputs, where Xtrain(h, w, d) =
xtrain
NW ×(h−1)+w(d). Given a new set of inputs {x1, . . . , xNH ×NW } ⊂ X and the corresponding
3D-tensor X ∈ RNH ×NW ×Dx of these inputs, where X(h, w, d) = xNW ×(h−1)+w(d). We consider
the function f applying on the 3D-tensor X and taking the following form

NH ×NW
NH ×NW
are the training inputs, and ytrain

, . . . , ytrain

NH ×NW

NH ×NW

1

f (xi) = W

Φ(Flatten(Conv2D(X, s))(i))
h(xi)

, i = 1, . . . , NH × NW

(37)

where Conv2D is the depth-wise 2D-convolution (Howard et al., 2017), with the kernel size s × s
and identical kernel channels, applied on the input tensor X. Here, the last dimension of X,
i.e., Dx, is the depth. Also, Φ(x) = [ϕ1(x), . . . , ϕDϕ(x)] ∈ RDϕ, W = [w1, . . . , wDv ]⊤ ∈
RDv×Dϕ, b ∈ RDv , and h is a vector-scalar function. We fit the function f to the training data
)} with an L2 regularization on W, i.e., a ridge regres-
{(xtrain
1
NH ×NW
sion, by solving the following convex optimization problem:

), . . . , (xtrain

, ytrain
1

, ytrain

NH ×NW

minimize
W
ξj , ˜ξj ,j=1,...,NH ×NW

1
2

∥W∥2

F + C

NH ×NW(cid:88)

Dv(cid:88)

j=1

d=1

(cid:16)

(cid:17)
ξj(d) + ˜ξj(d)

=

1
2

Dv(cid:88)

d=1

∥wd∥2 + C

NH ×NW(cid:88)

Dv(cid:88)

(cid:16)

ξj(d) + ˜ξj(d)

(cid:17)

j=1

d=1

subject to






ytrain
j

w⊤
d

(d) − w⊤
d

Φ(Flatten(Conv2D(Xtrain, s))(j))
h(xtrain
)
j
Φ(Flatten(Conv2D(Xtrain, s))(j))
h(xtrain
j

)

j

+ b(d) − ytrain

ξj(d), ˜ξj(d) ≥ 0, j = 1, . . . , NH × NW , d = 1, . . . , Dv.

− b(d) ≤ ϵ + ξj(d)

(d) ≤ ϵ + ˜ξj(d)

The Lagrangian of the optimization problem 38 is given by

(38)

L :=

1
2

Dv(cid:88)

d=1

NH ×NW(cid:88)

Dv(cid:88)

j=1

d=1

NH ×NW(cid:88)

Dv(cid:88)

−

−

j=1

d=1

∥wd∥2 + C

NH ×NW(cid:88)

Dv(cid:88)

(cid:16)

ξj(d) + ˜ξj(d)

(cid:17)

−

NH ×NW(cid:88)

Dv(cid:88)

(cid:16)

ηj(d)ξj(d) + ˜ηj(d) ˜ξj(d)

(cid:17)

j=1

d=1

(cid:32)

αj(d)

ϵ + ξj(d) − ytrain

j

(d) + w⊤
d

(cid:32)

˜αj(d)

ϵ + ˜ξj(d) + ytrain

j

(d) − w⊤
d

j=1

d=1

Φ(Flatten(Conv2D(Xtrain, s))(j))
h(xtrain
j

)

+ b(d)

Φ(Flatten(Conv2D(Xtrain, s))(j))
h(xtrain
j

)

− b(d)

,

(cid:33)

(cid:33)

(39)
Similar to the derivation in Section 2.1 in the main text, the partial derivatives of L with respect to
the primal variable wd, d = 1, . . . , Dv, have to vanish for optimality, which leads to

∂wd L = wd −

NH ×NW(cid:88)

j=1

(αj(d) − ˜αj(d))

Φ(Flatten(Conv2D(Xtrain, s))(j))
h(xtrain
j

)

= 0

(40)

⇒ wd =

NH ×NW(cid:88)

j=1

(αj(d) − ˜αj(d))

Φ(Flatten(Conv2D(Xtrain, s))(j))
h(xtrain
j

)

.

(41)

22

Published as a conference paper at ICLR 2023



+ b,

(42)

Let vj = [ αj (1)− ˜αj (1)
)
Eqn. 38, we obtain the following support vector expansion of the linear basis function f :


, . . . , αj (Dv)− ˜αj (Dv)

]⊤, j = 1, . . . , NH × NW , and substitute Eqn. 41 into

h(xtrain
j

h(xtrain
j

⊤

)

f (xi) =


NH ×NW(cid:88)


j=1

αj(1) − ˜αj(1)
h(xtrain
j

)

Aij
h(xi)

, . . . ,

NH ×NW(cid:88)

j=1

αj(Dv) − ˜αj(Dv)
h(xtrain
j

)

Aij
h(xi)

NH ×NW(cid:88)

=

j=1

Aij
h(xi)

vj + b,

where Aij := Φ(Flatten(Conv2D(X, s))(i))⊤Φ(Flatten(Conv2D(Xtrain, s))(j)).

j=1 Aij and select Φ as in Eqn. 28. Let the training inputs {xtrain

Same as in Section 2.1, we set bs = 0. To derive the softmax normalization in attention, we choose
h(xi) := (cid:80)N
} ⊂ X
be the attention keys {k1, . . . , kNH ×NW } ⊂ K, where K = RD, in self-attention. Also, let the new
inputs {x1, . . . , xNH ×NW } ⊂ X be the attention queries {q1, . . . , qNH ×NW } ⊂ K in self-attention.
We define the 2D-Convolution Attention (Attention-Conv2D) as follows:

, . . . , xtrain

NH ×NW

1

Definition 3 (2D-Convolution Attention) Given a set of the key and value vectors {kj, vj}NH ×NW
,
and a set of the query vectors {qi}NH ×NW
. Denote the key tensor and the query tensor by K ∈
RNH ×NW ×D and Q ∈ RNH ×NW ×D, respectively, where K(h, w, d) = kNW ×(h−1)+w(d) and
Q(h, w, d) = qNW ×(h−1)+w(d). The 2D-Convolution Attention (Attention-Conv2D) computes the
corresponding output vector hi of the query qi by the following attention formula:
√

j=1

i=1

softmax

Flatten(Conv2D(Q, s))(i)⊤Flatten(Conv2D(K, s))(j)/

N
(cid:88)

hi =

(cid:17)

D

vj,

(43)

(cid:16)

j=1

where the Conv2D(·, s) is the depth-wise 2D-convolution (Howard et al., 2017) with the kernel size
s × s and identical kernel channels.

Remark 7 (Convolutional Projection for Attention in the Convolutional vision Transformer)
The convolutional projections used in the Convolutional vision Transformer (CvT) (Wu et al., 2021)
can be derived from Eqn. 42 by letting the training input tensor Xtrain to be the 2D input matrix of
size N × Dx of the self-attention layer (see Section 1.1 in the main text) reshaped into a 3D tensor of
size NH × NW × Dx where N = NH × NW . Here, to avoid confusion, we denote the input of the
self-attention layer by Xinput and its reshaped version by Reshape2D(Xinput). We then replace the
depth-wise 2D-convolution by the depth-wise separable 2D-convolution in (Wu et al., 2021) and
remove the constraint that the kernels have identical channels. In order to derive the convolutional
projections for the keys, queries, and values in CvT, for i, j = 1, . . . , N , we let
kj = Flatten(Conv2D(Xtrain, s))(j)) = Φ(Flatten(Conv2D(Reshape2D(Xinput), s, WK))(j),
qi = Flatten(Conv2D(X, s))(i)) = Φ(Flatten(Conv2D(Reshape2D(Xinput), s, WQ))(i),
vj = Flatten(Conv2D(Xtrain, s))(j)) = Φ(Flatten(Conv2D(Reshape2D(Xinput), s, WV ))(j).

(44)
Here, we specify the kernel/filter WK, WQ, and WV to emphasize that the convolutional projec-
tions in CvT uses different kernels to compute keys, queries, and values in self-attention. Eqn. 44
matches the convolutional projects in CvT. By choosing h and Φ similar to above, we can derive the
convolutional attention in CvT.

H 1D-CONVOLUTION ATTENTION

Following the derivation for the Attention-Conv2D in Appendix G above, we can derive the 1D-
Convolution Attention (Attention-Conv1D) in a similar way by letting Xtrain ∈ RN ×Dx and
X ∈ RN ×Dx be 2D-matrices of training inputs and new inputs, respectively, and by replacing
Conv2D by Conv1D, which is the depth-wise 1D-convolution, with the kernel size s × 1 and identical
kernel channels, applied on the input tensor X. Here, the last dimension of X, i.e., Dx, is the depth.
We define the 1D-Convolution Attention (Attention-Conv1D) as follows:

Definition 4 (1D-Convolution Attention) Given a set of the key and value vectors {kj, vj}N
and a set of the query vectors {qi}N

j=1,
i=1. Denote the key matrix and the query matrix by K :=

23

Published as a conference paper at ICLR 2023

[k1, . . . , kN ]⊤ ∈ RN ×D and Q := [q1, . . . , qN ]⊤ ∈ RN ×D, respectively. The 1D-Convolution
Attention (Attention-Conv1D) computes the corresponding output vector hi of the query qi by the
following attention formula:

N
(cid:88)

hi =

(cid:16)

softmax

Conv1D(Q, s)(i)⊤Conv1D(K, s)(j)/

√

(cid:17)

D

vj,

(45)

where the Conv1D(·, s) is the depth-wise 1D-convolution with the kernel size s × 1 and identical
kernel channels.

j=1

I ATTENTION WITH BATCH NORMALIZATION AND SCALED HEADS

The Attention-BN+SH combines
Attention-BN+SH fits the function f s, s = 1, . . . , H,
)}, . . . , {(kH
{(k1
N1, . . . , NH , where K = RD and Y = RDv . The function f s is defined as:

the Attention-BN and Attention-SH. The
in Eqn. 17 with training sets
)} ⊂ K × Y of different sizes

1 ), . . . , (kH
NH

1), . . . , (k1
N1

1 , yH

, yH
NH

, y1
N1

1, y1

both

f s(x) := Ws Φ((x − µs) ⊙ ss−1
)
hs((x − µs) ⊙ ss−1 )

+ bs,

(46)

where

µs =



j , ss−1
ks

=



(cid:113)

1
Ns

Ns(cid:88)

j=1

1

σs2
1 + ϵ

, . . . ,

(cid:113)

1

σs2
D + ϵ



⊤

, σs2

d =



1
Ns

Ns(cid:88)

(ks

j (d) − µs(d))2.

j=1

(47)

Following the same derivation as in Section 2.1, we derive the following support vector expansion of
f s

Φ((x − µs) ⊙ ss−1

)⊤Φ((ks

j − µs) ⊙ ss−1

)

vs
j + bs.

(48)

f s(x) =

Ns(cid:88)

j=1

(cid:20) αs
hs((ks

j (1)

j (1)− ˜αs
j −µs)⊙ss−1 )

Here, vs

j =

j are the dual variables,
j = 1, . . . , N . Same as in Section 2.1, in Eqn. 48, we choose Φ as in Eqn. 28, hs(x) :=
(cid:80)Ns
j ), and bs = 0 to obtain the Batch Normalized Attention with Scaled Heads

j Φ(x)T Φ(ks

, . . . ,

, where αs

j and ˜αs

αs
hs((ks

j (Dv)− ˜αs

j (Dv)
j −µs)⊙ss−1 )

(Attention-BN+SH), which is defined as follows:

hs((x − µs) ⊙ ss−1)

(cid:21)⊤

Definition 5 (Batch Normalized Attention with Scaled Heads) Given H sets of the key and value
vectors {k1
i , i = 1, . . . , N ,
the Batch Normalized Attention with Scaled Heads (Attention-BN+SH) computes the corresponding
output vector hi of the queries q1

j=1, for each set of H query vectors q1

i by the following attention formula:

j=1, . . . , {kH

i , . . . , qH

i , . . . , qH

j }NH

j , vH

j }N1

j , v1



(cid:16)

softmax

((qs

i − µs) ⊙ ss−1

)⊤((ks

j − µs) ⊙ ss−1

)/

√

(cid:17)

D

vs
j

 ,

(49)

hi =

H
(cid:88)

s=1

Ws
O





Ns(cid:88)

where

µs =

1
Ns

Ns(cid:88)

j=1

j=1



j , ss−1
ks

=



(cid:113)

1

σs2
1 + ϵ

, . . . ,

(cid:113)

1

σs2
D + ϵ



⊤

, σs2

d =



1
Ns

Ns(cid:88)

(ks

j (d) − µs(d))2.

j=1

(50)

Following the same Remark 5 in Section 2.3.2, given input sequence X := [x1, · · · , xN ]⊤ ∈ RN ×Dx
of N feature vectors in self-attention, in order to generate the sets of {ks
j=1 at the scale sth, we
can downsample the input X before projecting into the key matrix K and the value matrix V. In this
paper, we use the average-pooling to downsample X.

j }Ns

j , vs

As in the same case of Attention-BN, for Attention-BN+SH, recentering queries and keys
alone are sufficient for accuracy improvement, and we weight the mean µ in Eqn 49 with a constant
β. Hence Eqn. 49 is simplified to:
Ns(cid:88)

H
(cid:88)





√

(cid:16)

(cid:17)

Ws
O



softmax

(qs

i − βµs)⊤(ks

j − βµs)/

D

vs
j

 .

hi =

(51)

s=1

j=1

24

Published as a conference paper at ICLR 2023

Table 12: The values of β for Linear Attention-BN/BN+SH and Sparse Attention-BN/BN+SH trained on the
selected 10 UEA tasks.

Dataset/Model

Linear Attention-BN Linear Attention-BN+SH Sparse Attention-BN Sparse Attention-BN+SH

ETHANOLCONCENTRATION
FACEDETECTION
HANDWRITING
HEARTBEAT
JAPANESEVOWELS
PEMS-SF
SELFREGULATIONSCP1
SELFREGULATIONSCP2
SPOKENARABICDIGITS
UWAVEGESTURELIBRARY

0.15
0.6
0.25
0.6
0.6
0.35
0.35
0.75
0.6
0.65

0.95
0.6
0.3
0.15
0.6
0.65
0.25
0.15
0.6
0.55

0.8
0.6
0.3
0.4
0.6
0.5
0.1
0.5
0.6
0.9

0.2
0.6
0.3
0.5
0.6
0.6
0.9
0.3
0.6
0.3

Table 13: The values of β for Attention-BN/BN+SH trained on 25 UEA Time Series classification tasks (Bagnall
et al., 2018) and 6 UEA Time Series Regression tasks.

Dataset/Model

Attention-BN Attention-BN+SH

ETHANOLCONCENTRATION
FACEDETECTION
HANDWRITING
HEARTBEAT
JAPANESEVOWELS
PEMS-SF
SELFREGULATIONSCP1
SELFREGULATIONSCP2
SPOKENARABICDIGITS
UWAVEGESTURELIBRARY
ARTICULARYWORDRECOGNITION
BASICMOTIONS
EPILEPSY
ERING
FINGERMOVEMENTS
LIBRAS
NATOPS
RACKETSPORTS
ATRIALFIBRILLATION
CRICKET
STANDWALKJUMP
HANDMOVEMENTDIRECTION
LSST
DUCKDUCKGEESE
MOTORIMAGERY

APPLIANCESENERGY
BENZENECONCENTRATION
BEIJINGPM10
BEIJINGPM25
LIVEFUELMOISTURE
IEEEPPG

0.25
0.6
0.65
0.55
0.6
0.25
0.5
1.2
0.65
0.1
0.2
0.1
0.3
0.1
1.0
0.3
1.0
1.0
0.9
0.2
1.0
0.5
0.3
0.6
0.2

0.4
0.2
0.1
0.1
0.1
0.4

0.15
0.6
0.25
0.85
0.6
0.35
0.85
0.9
0.6
0.2
0.6
0.1
0.2
0.9
0.3
0.7
0.4
0.4
0.6
1.0
0.6
0.5
0.2
0.3
0.3

0.2
0.1
0.5
0.2
0.3
0.2

Table 14: The values of β of Attention-BN/BN+SH trained on the 5 tasks of the LRA benchmark (Tay et al.,
2021).

Dataset/Model Attention-BN Attention-BN+SH

LISTOPS
TEXT
RETRIEVAL
IMAGE
PATHFINDER

0.5
0.5
1.0
0.2
0.2

0.2
0.8
1.0
0.2
0.4

J HYPERPARAMETERS

In this section, we provide the hyper-parameters for our best models.

J.1 UEA TIME SERIES CLASSIFICATION AND REGRESSION

For these two benchmarks, use the set of downsampling factors s = [1, 1, 2, 2, 4, 4, 8, 8] for Attention-
SH/BN+SH and Linear/Sparse Attention-SH/BN+SH models trained on the UEA benchmark. Ta-
ble 13 and Table 12 provide the values of β used for our best Attention-BN/BN+SH and Linear
Attention-BN/BN+SH, Sparse Attention-BN/BN+SH models trained on subsets of the two bench-
marks.

25

Published as a conference paper at ICLR 2023

J.2 LONG RANGE ARENA BENCHMARK

For all 5 tasks of the LRA benchmark, we set the downsampling factors s of Attention-SH/BN+SH,
Linear/Sparse Attention-SH/BN+SH is [1, 2] and kernel size of Attention-Conv1D models is 5. In
addition, Table 14 provides the values β of Attention-BN/BN+SH models trained on the benchmark.

J.3

IMAGENET CLASSIFICATION

This task’s β of Attention-BN/BN+SH is 1. Attention-SH/BN+SH has the downsampling factor of
[1, 1, 2, 4], and the kernel size of Attention-Conv2D is (2, 2).

26

