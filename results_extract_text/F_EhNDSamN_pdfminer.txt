Published as a conference paper at ICLR 2023

PARAMETRIZING PRODUCT SHAPE MANIFOLDS BY
COMPOSITE NETWORKS

Josua Sassen
University of Bonn

Klaus Hildebrandt
TU Delft

Martin Rumpf
University of Bonn

Benedikt Wirth
University of M¨unster

ABSTRACT

Parametrizations of data manifolds in shape spaces can be computed using the rich
toolbox of Riemannian geometry. This, however, often comes with high computa-
tional costs, which raises the question if one can learn an efficient neural network
approximation. We show that this is indeed possible for shape spaces with a spe-
cial product structure, namely those smoothly approximable by a direct sum of
low-dimensional manifolds. Our proposed architecture leverages this structure by
separately learning approximations for the low-dimensional factors and a subse-
quent combination. After developing the approach as a general framework, we
apply it to a shape space of triangular surfaces. Here, typical examples of data
manifolds are given through datasets of articulated models and can be factorized,
for example, by a Sparse Principal Geodesic Analysis (SPGA). We demonstrate
the effectiveness of our proposed approach with experiments on synthetic data as
well as manifolds extracted from data via SPGA.

1

INTRODUCTION

Modeling collections of shapes as data on Riemannian manifolds has enabled the usage of a rich
set of mathematical tools in areas such as computer graphics and vision, medical imaging, compu-
tational biology, and computational anatomy. For example, Principal Geodesic Analysis, a general-
ization of Principal Component Analysis, can be used to parametrize submanifolds approximating
given data points while preserving structure of the data such as its invariance to rigid motion. The
evaluation of such a parametrization, however, typically comes at a high computational cost as
the Riemannian exponential, mapping infinitesimal shape variations to shapes, has to be evaluated.
This motivates trying to learn an efficient approximation for these parametrizations. Direct applica-
tion of deep neural networks (NNs), however, proves ineffective for high-dimensional spaces with
strongly nonlinear variations. Therefore, we consider more structured shape manifolds, namely, we
assume that they can be approximated by an affine sum of low-dimensional submanifolds. In com-
puter graphics, typical examples of data manifolds are given through datasets of articulated models,
e.g. human bodies, faces or hands. Then, the desired structure of an affine sum of factor manifolds
can be produced, for example, by a Sparse Principal Geodesic Analysis (SPGA). Motivated by this,
we exploit the data manifolds’ approximability with such affine sums: We separately approximate
the exponential map on the factor manifolds by fully connected NNs and the subsequent combina-
tion of factors by a convolutional NN to yield our approximate parametrization. In formulas, based
on a judiciously chosen decomposition v = v1 + . . . + vJ , our aim is to approximate the Rieman-
nian exponential expz(v) by Ψζ(ψζ
j are further
NNs approximating the Riemannian exponential expz on the low-dimensional factor manifolds.
We develop our approach focusing on the shape space of discrete shells, where shapes are given
by triangle meshes and the manifold is equipped with an elasticity-based metric. In principle, our
approach is also applicable to other shape spaces such as manifolds of images, and we will include
remarks on how we propose this could work. We evaluate our approach with experiments on data
manifolds of triangle meshes, both synthetic ones and ones extracted from data via SPGA, and
we demonstrate that the proposed composite network architecture outperforms a monolithic fully
connected network architecture as well as an approach based on the affine combination of the factors.
We see this work as a first step to use NNs to accelerate the complex computations of shape manifold
parameterizations. Therefore, we think that our approach has great potential to stimulate further
research in this direction, which could in turn advance the applications of Riemannian shape spaces.

J (vJ )), where Ψζ is a NN and the ψζ

1(v1), . . . , ψζ

1

Published as a conference paper at ICLR 2023

Contributions

In summary, the contributions of this paper are

• combining the Riemannian exponential map on shape spaces and neural network methodol-

ogy for the efficient parametrization of shape space data manifolds,

• demonstrating the applicability of such an approach for data manifolds which can be

smoothly approximated via direct sums of low-dimensional submanifolds,

• using a combination of fully connected neural networks for the factorwise Riemannian ex-

ponential maps and a convolutional network to couple them,

• verifying that such a setup works well with existing methods to construct product manifolds,

such as Sparse Principal Geodesic Analysis, and

• showing that the composite network architecture outperforms alternative approaches.

2 RELATED WORK

Shape Spaces Shape spaces are manifolds in which each point is a shape, e.g., a triangle mesh or
an image. A Riemannian metric on such a space provides means to define distances between shapes,
to interpolate between shapes by computing shortest geodesic paths, and to explore the space by
constructing the geodesic curve starting from a point into a given direction. Shape spaces have
proven useful for applications in areas such as computer graphics (Kilian et al., 2007; Heeren et al.,
2012; Wang et al., 2018) and vision (Heeren et al., 2018; Xie et al., 2014), medical imaging (Kurtek
et al., 2011b; Samir et al., 2014; Kurtek et al., 2016; Bharath et al., 2018), computational biology
(Laga et al., 2014), and computational anatomy (Miller et al., 2006; Pennec, 2009; Kurtek et al.,
2011a). For an introduction to the topic, we refer to the textbook of Younes (2010).

Shape Space of Meshes Triangle meshes are widely used to represent shapes in computer graphics
and vision. Riemannian metrics on shape spaces of triangle meshes can be defined geometrically,
using norms on function spaces on the meshes (Kilian et al., 2007), or physics-based, considering
the meshes as thin shells and measuring the dissipation required to deform the shells (Heeren et al.,
2012; 2014). The computation of geodesic curves in these spaces requires numerically solving
high-dimensional nonlinear variational value problems, which can be costly. For shape interpolation
problems, model reduction methods can be used to efficiently find approximate solutions (Brandt
et al., 2016; von Radziewsky et al., 2016).

Statistics in Shape Spaces Data in a Riemannian shape spaces can be analyzed using Principal
Geodesic Analysis (PGA) (Fletcher et al., 2004; Pennec, 2006). Analogous to principal component
analysis (PCA) for data in Euclidean spaces, PGA can construct low-dimensional latent represen-
tations that preserve much of the variability in the data. This is achieved by mapping the data with
a non-linear mapping, the Riemannian logarithmic map, from the manifold to a linear space, the
tangential space at the mean shape, and computing a PCA there. Latent variables of the PCA are
then mapped with the inverse mapping, the Riemannian exponential map, onto the manifold, so that
the latent space describes a submanifold of the shape space. A PGA in shape spaces of meshes
was introduced in (Heeren et al., 2018) and used to obtain a low-dimensional, nonlinear, rigid body
motion invariant description of shape variation from data.

Sparse PGA While PCA modes involve all variables of the data, Sparse Principal Component
Analysis (Zou et al., 2006) constructs modes that involve just few variables. This is achieved by
integrating a sparsity encouraging term to the objective that defines the modes. Based on this idea,
Neumann et al. (2013) proposed a scheme for extracting Sparse Localized Deformation Compo-
nents (SPLOCS) from a dataset of non-rigid shapes. Since SPLOCS are linear modes, they are
well-suited to describe small deformations such as face motions accurately. To increase the range of
deformations and to compensate linearization artifacts Huang et al. (2014) integrated SPLOCS with
gradient-domain techniques and Wang et al. (2017; 2021) with edge lengths and dihedral angles.
In (Sassen et al., 2020b), a Sparse Principal Geodesic Analysis (SPGA) was introduced. Similar to
PGA, the SPGA modes are nonlinear and rigid motion invariant. On top of that, however, the SPGA
modes describe localized deformations. Due to the localization, many pairs of SPGA modes have
disjoint support and are therefore independent of each other. We want to take advantage of this prop-
erty to effectively learn the reconstruction of points in the manifold from their latent representation
through an adapted network structure.

2

Published as a conference paper at ICLR 2023

Product Manifolds This work is focused on the parametrization of the product manifold structures
obtained from SPGA. Alternative approaches for extracting product structures of data manifolds
include the approach of Fumero et al. (2021), which finds a product structure of a manifold based
on a geometric notion of disentanglement, the Geometric Manifold Component Estimator presented
in (Pfau et al., 2020), which uses a Lie group of transformations to generate a symmetry-based
disentanglement of data manifolds, and the approach of Zhang et al. (2021), which decomposes the
eigenfunctions of the Laplace–Beltrami operator of a manifold in order to find a product structure
in the manifold. To the best of our knowledge, there are no works focusing on using networks to
approximate the parametrization of such product manifolds in Riemannian shape spaces.

3 PRELIMINARIES AND NOTATION

We introduce step by step the background necessary to understand the context of our work. We also
provide an overview of our notation in Appendix E.

Riemannian Shape Space A shape space is a manifold S ⊂ Rn whose elements are shapes.
These could, for example, be images, curves, or surfaces described in various ways. Endowing such
a shape manifold with the structure of a Riemannian manifold, i.e. a (smoothly) z-dependent inner
product gz on the tangent space TzS at each point z ∈ S, provides us with a rich set of geometric
tools. For example, we can use geodesics c : [0, 1] → S, i.e. arc length parametrized locally shortest
paths, as mathematical formulation of shape interpolation. The Riemannian logarithm logz ˜z ∈ TzS
is then defined as the time derivative ˙c(0) of the geodesic c interpolating between c(0) = z and
c(1) = ˜z. This allows to interpret the tangent space TzS as the linear space of infinitesimal shape
variations. Lastly, the Riemannian exponential map is the inverse of the logarithm, which means
that for a tangent vector v ∈ TzS one ‘shoots’ a geodesic in its direction, i.e. constructs a geodesic
curve c with initial velocity v to obtain expz v := c(1) ∈ S. The exponential map allows to transfer
operations from infinitesimal shape variations back to actual shapes. We provide more details on
Riemannian operators and their discretization in Appendix C.

Principal Geodesic Analysis With these tools at hand, one can use Principal Geodesic Analysis
(PGA) to compute submanifolds of the shape space approximating given data points {zi} ⊂ S:
One first computes their Riemannian center of mass ¯z, i.e. the point with minimal sum of squared
distances to all data points. Then one computes the logarithms vi = log¯z zi and thus linearizes
the approximation problem at the center of mass by passing to the tangent space T¯zS. In T¯zS, one
uses classical Principal Component Analysis (PCA) to compute the m dominant modes {uj}, whose
span U is the best m-dimensional subspace approximating the logarithms vi. Then the submanifold
M approximating the data points is parametrized by the exponential map, i.e. M := exp¯z U.

Sparse Principal Geodesic Analysis The coordinates on Riemannian shape spaces, e.g. pixel
values, often correspond to different spatial locations of the shape, e.g. pixel positions, such that it
makes sense to consider their support and sparsity: Sassen et al. (2020b) introduced Sparse Princi-
pal Geodesic Analysis (SPGA) to compute spatially localized dominant modes with widely disjoint
supports. Those modes are easier to interpret semantically and in addition allow for efficient approx-
imations of the exponential map. SPGA is performed by adding an appropriate sparsity-inducing
regularization functional R to the variational formulation of PCA to compute such sparse deforma-
tion modes. The concrete choice of R depends on the specific shape space. Hence, for a given set
V ∈ Rn×K of K logarithms, the SPGA problem to compute the first m dominant modes U ∈ Rn×m
reads

g + λ R(U )

minimize
U ∈Rn×m
W ∈Rm×K
subject to uj ∈ T¯zS and |wj|∞ ≤ 1 for j ∈ {1, . . . , m}.
The bound on the magnitude of the weights ensures that the coordinates of the uj do not shrink
while the weights grow inverse proportionally. As before, the manifold approximating the data
points is then parametrized using the exponential map. It can be equipped with a product structure
by grouping the modes and applying the exponential map to the corresponding subspaces. This will
be elucidated more in Section 4.

∥V − U W ∥2

(1)

NRIC Manifold As shapes we will consider triangular surfaces with fixed connectivity, i.e. with
shared sets of vertices V, edges E, and faces F. For vertex positions X ∈ R3|V|, we denote by

3

Published as a conference paper at ICLR 2023

l(X) = (le(X))e∈E the vector of edge lengths and by θ(X) = (θe(X))e∈E the vector of dihedral
angles. We consider the vectors z(X) = (l(X), θ(X)) ∈ R2|E| combining both. The manifold of
all z ∈ R2|E| corresponding to immersed triangular surfaces is given by
(cid:12) T (z) > 0, Q(z) = 0(cid:9),

S := (cid:8)z ∈ R2|E| (cid:12)

(2)

where T (z) > 0 encodes the triangle inequalities and Q(z) = 0 are the discrete integrability
conditions from Wang et al. (2012), see also Sassen et al. (2020a) for more details. S is called the
NRIC manifold, short for Nonlinear Rotation-Invariant Coordinates, and can be equipped with an
elasticity-based Riemannian metric (Heeren et al., 2014). To evaluate the logarithm and exponential
map, we use the time-discretization developed by Rumpf & Wirth (2015).

To obtain vertex positions for given lengths and angles, we use the nonlinear least-squares method
from Fr¨ohlich & Botsch (2011) based on Lp-norms to measure the difference between arbitrary z,

∥za − zb∥p

p,¯z :=

(cid:88)

e∈E

wp

e,l|lb

e − la

e |p + η

wp

e,θ|θb

e − θa

e |p.

(cid:88)

e∈E

(3)

The weights are computed from a reference shape ¯z with vertex positions ¯X, e.g. the center of mass
from above, as we,l = le( ¯X)−1 and we,θ = le( ¯X)ae( ¯X)−1/2, where ae is (one third of) the area of
both faces adjacent to e. The bending weight η is the same as used in the elasticity-based metric.

To apply SPGA to this shape space, we need to specify the sparsity-inducing regularization R.
Sassen et al. (2020b) observed that the simple mode-wise L1-norms

R(U ) =

m
(cid:88)

j=1

∥uj∥1

(4)

suffice due to the natural connection of NRIC variation with elastic distortions. We provide more
details on NRIC in Appendix D.

Neural Networks We indicate functions that are implemented as neural networks by the super-
script ζ as in φζ. This ζ represents the network parameters and is the same for all occurring networks,
with the implicit convention that different networks depend on different subsets of these parameters.

We denote by MLPζ
ρ[N1, . . . , NT ] a fully-connected network with layer sizes N1, . . . , NT , the non-
linear activation function ρ : R → R after each layer, and parameters ζ. For graph convolutional
e ∈ RN0 on edges e: The tth
networks, we adapt the approach by Kipf & Welling (2017) to data z0
layer is given by


W t+1
1

zt+1
e = ρ



e + W t+1
zt

2

(cid:88)

˜e∈N (e)

˜e + bt+1
zt



(5)

1

∈ RNt+1×Nt and W t+1

∈ RNt+1×Nt are small matrices with learnable parameters
where W t+1
and bt+1 ∈ RNt+1 is the bias, all stored in the parameters ζ. We define the neighborhood of an edge
in a triangle mesh as N (e) := {˜e ∈ E | e and ˜e share a vertex}. We used the Exponential Linear
Unit (Clevert et al., 2016) as activation function in all our experiments.

2

4 COMPOSITE NETWORK APPROXIMATION

As previously explained, we want to learn an efficient parametrization Φ : Rm → M of an m-
dimensional Riemannian data manifold M. Below, we will explain our structural assumptions to
achieve this, detail how we include them in the network architecture and training, and finally discuss
their applicability to practical examples.

Structural Assumptions One can think of the parametrization as the decoder part of an autoen-
coder, however, for the purpose of this article we want to be more specific: We would like the
parametrization to have a geometric meaning, so we will focus on the situation in which Φ should
encode the Riemannian exponential map at some point z ∈ M. In principle, our approach can also
be combined with alternative concepts in which no ground truth parametrization is available, but this
setting allows a particularly simple quantification of the results.

4

Published as a conference paper at ICLR 2023

If no additional structure of the data manifold is known, one can of course not improve over com-
puting the Riemannian exponential directly or approximating it by some standard neural network.
In contrast, we consider the case where a specific structure is known. For the purpose of our article,
this structure is a priori given, that is, we do not study how such a structure can be found or obtained,
though we give an example for a corresponding procedure on Riemannian shape spaces later on. We
require that the data manifold M to be parametrized is embedded in some Rn for possibly large n.
The structure of M we want to exploit has three components:

(0) Correlation We assume a structural correlation between the different coordinates, e.g. graph-
neighbour relations for triangular meshes or pixel-neighbour relations for image data, so that
convolutional networks can be applied.

(1) Factorization We assume that the manifold can be smoothly approximated by a product of
much lower-dimensional manifolds M1, M2, . . . MJ , which we will parametrize separately.
Thereby, we exploit that the necessary network size as well as the required training effort de-
crease with smaller manifold dimension: For m-dimensional manifolds the network size should
scale at least linearly in m, while the required training set and thus also training time will scale
exponentially in m.

(2) Combination It is not sufficient that the single factor manifolds are easy to parametrize since a
generic point on M has components in all factors. Thus, we assume that the direct sum of all
factors

M1 ⊕z . . . ⊕z MJ := {z + (z1 − z) + . . . + (zJ − z) | z1 ∈ M1, . . . , zJ ∈ MJ }
for some z ∈ M already approximates the actual manifold M with a (possibly large, but) very
smooth approximation error. This will ensure that a suitable map from (z1 − z, . . . , zJ − z) to
M can be efficiently learned.

The toy shape manifold M of tori from Figure 1 has the flat metric of S1 × S1 × T2 (the factors
representing the orientation of the longitudinal and latitudinal ellipsoidal cross-sections as well as a
bump position, see subsection 5.1) and thus satisfies condition (1). Condition (2) holds since each
torus is represented as NRIC: For instance, the creation of the bump (which corresponds to changing
the position in T2) is well described by simply adding fixed numbers in the right places of the edge
length vector l(X). However, the direct sum is indeed only an approximation – otherwise there
would be no error in Figure 1 (green), but one clearly sees a remnant of the bump from the reference
shape.

Figure 1: Approximation of the Freaky Torus. We show the reference shape z in grey, the approxi-
mation of expz v by affine combination of exact factors in green, by a monolithic network in blue,
and by our composite network in yellow. The correct vertex positions of expz v are shown as purple
points. These purple points should ideally lie on the shaded surface, which would indicate a good
fit. Indeed, for the approximation by our composite network, this is mostly the case, while for the
other two approaches the approximation does not match the point cloud in many places.

Network Representation and Training Before we discuss the previous conditions, let us describe
how we exploit them in our network architecture and training procedure. To learn the parametriza-
tion Φ : Rm → M, we decompose it as

1, . . . , ψζ
where ψζ
j : Rmj → Rn is the parametrization of the mj-dimensional factor manifold Mj
and Ψζ : (Rn)J → Rn is the combination of the single factors, behaving approximately like
(x1, . . . , xn) (cid:55)→ z + (x1 − z) + . . . + (xn − z), see Figure 2.

Φ = Ψζ ◦ (ψζ

J ),

5

Published as a conference paper at ICLR 2023

Rmj

ψζ
j

Mj

Ψζ

M

Figure 2: Structure of composite network.

The maps ψζ
j will be fully connected neural networks. Because they approximate the smooth expo-
nential map on rather low-dimensional spaces, they are expected to achieve a high approximation
quality that is typically stable under variation of the concrete network architecture (number and size
of layers). For the maps Ψζ, we exploit that they operate on a structured domain, e.g. a mesh, and
thus they will be convolutional neural networks. This allows for efficient training and storage even
though they operate on high-dimensional data. One could alternatively also use fully connected
networks for Ψζ, but the observed quality of the results was similar despite significantly increased
memory requirements to train, store, and evaluate.
These networks are trained separately: To train the map ψζ
j , we consider a parametrization
1ej
1 + . . . + aj
ej
mj
mj
) and a basis ej
1, . . . , ej

with a coefficient vector aj = (aj
mj of the Riemannian logarithm
of Mj at point z as a linear subspace of the tangent space TzM. We then consider a set of random
samples Sj ⊂ Rmj as training data (for instance normally distributed or uniformly on a ball) and
minimize the loss function

ωj : Rmj → TzM, ωj(aj) = aj

1, . . . , aj

mj

Jj(ζ) =

1
|Sj|

(cid:88)

aj ∈Sj

∥expz(ωj(aj)) − ψζ

j (aj)∥2

in some norm ∥ · ∥ depending on the application. To train the map Ψζ, we subsequently consider a
random training set S ⊂ Rm1 × . . . × RmJ = Rm and minimize the loss function

J (ζ) =

1
|S|

(cid:88)

(a1,...,aJ )∈S

∥expz(ω1(a1) + . . . + ωJ (aJ )) − Ψζ(ψζ

1(a1), . . . , ψζ

J (aJ )∥2,

where we may or may not keep the maps ψζ

j fixed.

Applicability of Assumptions While condition (0) essentially has to hold for any approach em-
ploying neural networks, conditions (1) and (2) are specific to our approach. Condition (1) expresses
that the intrinsic geometry of the data manifold M has a simplifying structure, while condition (2) is
about the extrinsic geometry of how M is embedded in Rn – only both conditions together charac-
terize a structure that can efficiently be learned. Typical situations where our conditions hold include
the following two examples:

• The different factor manifolds correspond to different spatial regions. For instance, images
may sometimes be partitioned into different regions that can vary more or less independently
of each other. If the regions are fully disjoint, the manifold M can exactly be written as
a direct sum of factor manifolds; if on the other hand the regions slightly overlap, then
this is only fulfilled approximately. Examples in shape spaces are where the single factor
manifolds correspond to shape variations with disjoint support such as articulation of the
different extremities of a character. Such a manifold structure is ubiquitous in computer
graphics applications, and our computational examples mostly belong to this category.

• On shapes one can also often observe variations at different length scales that are independent
of each other, for example, geometric texture on small length scales versus global shape
variations at large length scales. The above toy example of the torus, in which the small
bump is more or less independent of the global shape variations, belongs to this category.

6

Published as a conference paper at ICLR 2023

As mentioned before, we assume the product manifold structure to be given a priori (in the torus
example, for instance, it was given by design of the data manifold). The product manifold struc-
ture would typically be a result of some data manifold analysis, for example from disentanglement
learning. In the setting of Riemannian shape spaces, one way to obtain this structure is the SPGA
introduced earlier: It makes use of the decomposition into independent product manifolds due to
disjoint support of the corresponding shape variations.

5 EXPERIMENTS AND APPLICATIONS

In this section, we present experimental results on the aforementioned synthetic shape manifold of
deformed tori and manifolds extracted via SPGA. We will compare our method to approaches based
on (Sassen et al., 2020b) and a straightforward approximation of the parametrization by a single
fully connected network. To quantify the approximation quality of different approaches, we use the
coefficient of determination R2. For approximations ˜zi of NRIC zi with mean ¯z, it is defined as

R2(z, ˜z) = 1 −

(cid:80)
(cid:80)

i∥˜zi − zi∥2
i∥zi − ¯z∥2

2,¯z

2,¯z

.

From a statistical point of view, it quantifies the proportion of variation of the data that is explainable
by a given model. This means an R2 of one is optimal, the smaller it is the worse is the approxima-
tion, and a negative R2 means that the model is worse than simply using the mean.

Training & Implementation We used Adam (Kingma & Ba, 2015) as descent method for training
all networks, where the initial learning rate was 10−3 and was reduced by a factor of 10 every time
the loss did not decrease for multiple iterations. For regularization, we used batch normalization
after each layer and a moderate dropout regularization (p = 0.1) after each convolutional layer. We
implemented the neural networks in PyTorch (Paszke et al., 2019) using the PyTorch Geometric
library (Fey & Lenssen, 2019). The tools for the NRIC manifold were implemented in C++ based
on OpenMesh (Botsch et al., 2002), where we use the Eigen library (Guennebaud et al., 2010)
for numerical linear algebra. We follow an approach similar to Kilian et al. (2007) to perform all
computations on a coarsened mesh and prolongate solutions to a fine one only for visualizations.

5.1 SYNTHETIC DATA: FREAKY TORUS

For the Freaky Torus dataset, we construct a synthetic shape space with factors S1 × S1 × T2, where
T2 refers to the flat 2-dimensional torus. It is realized in NRIC by (i) deforming the two cross-
sectional circles of a torus to ellipses of fixed aspect ratio and orientation controlled by the first two
S1 factors and (ii) growing a bump in normal direction whose position is controlled by the last T2
factor. These torus deformations are applied to a regular mesh of a regular torus embedded in R3,
and the deformed meshes’ NRIC are extracted to obtain our datapoints. We used a mesh with 2048
vertices and uniformly drew 1000 samples from S1 × S1 × T2.1 More details on the data generation
can be found in Appendix A.

Figure 1 shows that a single fully connected network struggles with approximating the high fre-
quency detail of the bump, while our composite network is able to handle this well. This can also be
observed in the approximation quality quantified using the R2. The composite network achieves an
R2 of 0.99 and the monolithic network one of 0.95. This difference may sound small, however, this
is because the bump is a detail and the error is dominated by the overall shape of the torus.

5.2 APPLICATION: SPGA MANIFOLDS

Lastly, we report the results of applying our method to shape manifolds whose approximate product
structure is found with the help of SPGA. To this end, we repeat three of the examples discussed by
Sassen et al. (2020b) and consider one new dataset. The repeated examples are a humanoid dataset
from (Anguelov et al., 2005), a dataset of face meshes from (Zhang et al., 2004), and a set of hand
meshes from (Yeh et al., 2011). For the new example, we examine a humanoid dataset based on
SMPL-X (Pavlakos et al., 2019), where we consider their expressive hands and faces (EHF) dataset,

1The data and the generating code can be found at https://gitlab.com/jrsassen/

freaky-torus.

7

Published as a conference paper at ICLR 2023

containing 100 shapes, and 49 additional shapes from the SMPL+H dataset, which feature more
expressive arm and leg movements, adding to a total of 149 input shapes. We interpret all shapes
as elements of the nonlinear NRIC space so that this small amount of data points already suffices to
span a high-dimensional, nonlinear NRIC submanifold that serves as our data manifold.

Based on this data, we follow the numerical approach of Sassen et al. (2020b) to solve (1) and
thereby compute the sparse tangent modes. We report the chosen number m of included modes in
Table 1, where we used the same number as Sassen et al. (2020b) for the repeated examples. To
factor the resulting data manifold, we again follow the approach outlined by Sassen et al. (2020b),
which is a clustering based on the spatial overlap of the modes. For the hand and face examples, we
choose the same number of factors J, while for the SCAPE example we decreased the number to
account for the possibility to handle higher-dimensional factors with our method. Our choices are
again documented in Table 1. Each cluster then spans exactly one of the factor manifolds and the
range of their dimensions is also reported in said table.

Figure 3: Examples from the face and hands datasets. We use the same colors as in Figure 1.

Data Generation Next, we sample the exponential map on the space spanned by the SPGA modes
to generate the training (and test) data. To this end, we consider the hypercube in Rm given by the
minimal and maximal coefficients of projections of the input data onto the SPGA subspace. Then we
draw our parametrization coefficient samples S ⊂ Rm uniformly from this hypercube. To create the
samples Sj for the factor manifolds, we simply take the corresponding subcomponents of coefficient
vectors from S. The corresponding shapes were then computed by evaluating the exponential map
for each of them. Overall, we sampled approximately |S| = 4000 points for each of the considered
examples. The dataset was split randomly into a training (80 %) and a test (20 %) set, with the
training set being used for the descent method of the loss functionals and the test set being used to
evaluate the performance of the networks.

Figure 4: Two examples from the SCAPE dataset. We use the same colors as in Figure 1.

Example m J

mj

Affine Monolithic

SMPL+X 80
40
SCAPE
12
Hands
10
Faces

10
6
4
6

3 – 24
5 – 9
2 – 4
1 – 4

0.78
0.77
0.88
0.96

0.85
0.60
0.95
0.95

Composite
(Ours)
0.93
0.91
0.98
0.99

Sassen et al. (2020b)

—
—
0.80
0.95

Table 1: Approximation quality R2 on SPGA examples.

Comparison to Affine Approximation Sassen et al. (2020b) also proposed a scheme based on
multilinear interpolation of precomputed exponentials for each of the factor manifolds and subse-
quent affine combination of the results to approximate the exponential map and parametrize M. A
natural question is how our network-based approach compares to this.

8

Published as a conference paper at ICLR 2023

For the humanoid examples, it was not possible to precompute Riemannian exponentials on a regular
grid for all factor manifolds due to their high dimensionality. Hence, instead of multilinearly inter-
polating precomputed exponentials we simply compute the exponentials within each factor mani-
fold exactly before combining them affinely. We dub this method simply ‘affine combination’ and
present its results in Figure 4 and Table 1; it is computationally heavy, but yields an upper bound
on the quality of the method by Sassen et al. (2020b). The limitation does not apply to our new ap-
proach, which for example allows us to learn an efficient parametrization for the SMPL+X dataset,
where the expressive movements of hand and face require a higher-dimensional data manifold.

In all examples, our composite network approach achieves higher approximation accuracy than the
‘affine combination’. This shows that the network Ψζ is able to correct the approximation errors of
the direct sum structure. For the lower-dimensional examples, this difference is not as pronounced
since their sparse modes have a better support separation.

Furthermore, storing our network-based approximation requires less memory than the approach by
Sassen et al. (2020b). For example, on the SCAPE dataset storing grids with approx. 20000 samples
as reported by Sassen et al. (2020b) requires about 1.7 GB of storage, while our networks only
require 0.6 GB (without optimizing for a small memory footprint).

Figure 5: Two examples from the SMPL-X dataset. We use the same colors as in Figure 1.

Comparison to Monolithic Network Another obvious question is whether our composite ap-
proach shows any benefit over training a simple, single network. For evaluation, we also trained
one fully connected network (cid:101)Φζ : Rm → M to approximate the parametrization at once, dubbed
‘monolithic’ approach. The corresponding approximation qualities are reported in Table 1. One
sees that for the lower-dimensional examples this monolithic approach achieves an approximation
quality close to the one of our composite network. However, for the higher-dimensional, humanoid
examples the approximation quality of the monolithic approach is noticeably lower.

6 CONCLUSION

Our results suggest that the most fundamental geometric operation on Riemannian data manifolds,
the parametrization of the manifold via the Riemannian exponential map, can in principle be learned.
We illustrated this on shape manifolds of triangular meshes, for which the exponential map is com-
putationally expensive so that an approximation is attractive. However, while naive implementations
via deep neural networks proved7 ineffective, we achieved consistently satisfying results by match-
ing our training and network architecture to a typical structure of shape manifolds: that they can
be approximated by an affine sum of submanifolds. We thus learned both, the lower-dimensional
Riemannian exponential map on each submanifold as well as the (close to affine) composition of the
different submanifolds. We furthermore illustrated in our examples that such manifold structures
arise from basic principles like support or scale separation of different shape variations.

While we implemented the above concept of composite networks only for shape manifolds, it should
also be applicable to image manifolds. However, first the corresponding tools to identify approx-
imate product manifold structures (such as the SPGA) would have to be developed for images. It
is also conceivable to replace the SPGA by learning approaches akin to disentanglement learning.
Furthermore, we did not touch upon further optimization for the specific setting of our shape man-
ifolds: The single factor manifolds typically describe localized shape variations so that a sparsity
regularization of the corresponding networks would make sense and could substantially reduce the
parameter size. Furthermore, one could add a regularization favoring those NRIC that correspond to
an immersed mesh, thereby reducing postprocessing.

9

Published as a conference paper at ICLR 2023

ACKNOWLEDGMENTS

This work was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research
Foundation) via project 211504053 – Collaborative Research Center 1060, project 212212052 –
“Geodesic Paths in Shape Space” (part of the NFN Geometry + Simulation), and via Germany’s
Excellence Strategy project 390685813 – Hausdorff Center for Mathematics and project 390685587
– Mathematics M¨unster: Dynamics–Geometry–Structure.

REFERENCES

Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, and James
Davis. SCAPE: shape completion and animation of people. ACM Transactions on Graphics, 24
(3):408–416, 2005.

Karthik Bharath, Sebastian Kurtek, Arvind Rao, and Veerabhadran Baladandayuthapani. Radiologic
image-based statistical shape analysis of brain tumours. Journal of the Royal Statistical Society:
Series C (Applied Statistics), 67(5):1357–1378, 2018.

Mario Botsch, Stephan Steinberg, Stephan Bischoff, and Leif Kobbelt. Openmesh - a generic and

efficient polygon mesh data structure. 1st OpenSG Symposium, 2002.

Christopher Brandt, Christoph von Tycowicz, and Klaus Hildebrandt. Geometric flows of curves
in shape space for processing motion of deformable objects. Computer Graphics Forum, 35(2),
2016.

Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and Accurate Deep Network
Learning by Exponential Linear Units (ELUs). In International Conference on Learning Repre-
sentations, 2016.

Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In

ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.

P. Thomas Fletcher, Conglin Lu, Stephen M. Pizer, and Sarang Joshi. Principal geodesic analysis for
the study of nonlinear statistics of shape. IEEE Transaction on Medical Imaging, 23(8):995–1005,
2004.

Stefan Fr¨ohlich and Mario Botsch. Example-driven deformations based on discrete shells. Computer

Graphics Forum, 30(8):2246–2257, 2011.

Marco Fumero, Luca Cosmo, Simone Melzi, and Emanuele Rodol`a. Learning disentangled repre-
sentations via product manifold projection. In Marina Meila and Tong Zhang (eds.), Proceedings
of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Ma-
chine Learning Research, pp. 3530–3540, 2021.

Ga¨el Guennebaud, Benoˆıt Jacob, and Others. Eigen v3. http://eigen.tuxfamily.org, 2010.

Behrend Heeren, Martin Rumpf, Max Wardetzky, and Benedikt Wirth. Time-discrete geodesics in

the space of shells. Computer Graphics Forum, 31(5):1755–1764, 2012.

Behrend Heeren, Martin Rumpf, Peter Schr¨oder, Max Wardetzky, and Benedikt Wirth. Exploring

the geometry of the space of shells. Computer Graphics Forum, 33(5):247–256, 2014.

Behrend Heeren, Chao Zhang, Martin Rumpf, and William Smith. Principal geodesic analysis in

the space of discrete shells. Computer Graphics Forum, 37(5), 2018.

Zhichao Huang, Junfeng Yao, Zichun Zhong, Yang Liu, and Xiaohu Guo. Sparse localized decom-

position of deformation gradients. Computer Graphics Forum, 33(7):239–248, 2014.

Martin Kilian, Niloy J. Mitra, and Helmut Pottmann. Geometric modeling in shape space. ACM

Transactions on Graphics, 26(64):1–8, 2007.

Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International

Conference on Learning Representations, 2015.

10

Published as a conference paper at ICLR 2023

Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-

works. In International Conference on Learning Representations, 2017.

Sebastian Kurtek, Eric Klassen, Zhaohua Ding, Sandra Jacobson, Joseph B. Jacobson, Malcolm Avi-
son, and Anuj Srivastava. Parameterization-invariant shape comparisons of anatomical surfaces.
IEEE Transactions on Medical Imaging, 30(3):849–858, 2011a.

Sebastian Kurtek, Eric Klassen, John C. Gore, Zhaohua Ding, and Anuj Srivastava. Classification
of mathematics deficiency using shape and scale analysis of 3d brain structures. In Benoit M.
Dawant and David R. Haynor (eds.), Medical Imaging 2011: Image Processing, volume 7962 of
SPIE Proceedings. SPIE, 2011b.

Sebastian Kurtek, Qian Xie, Chafik Samir, and Michel Canis. Statistical model for simulation of
deformable elastic endometrial tissue shapes. Neurocomputing, 173:36–41, 2016. ISSN 0925-
2312.

Hamid Laga, Sebastian Kurtek, Anuj Srivastava, and Stanley Miklavcic. Landmark-free statistical

analysis of the shape of plant leaves. Journal of theoretical biology, 363:41–52, 08 2014.

Michael I. Miller, Alain Trouv´e, and Laurent Younes. Geodesic shooting for computational anatomy.

Journal of Mathematical Imaging and Vision, 24(2):209–228, 2006.

Thomas Neumann, Kiran Varanasi, Stephan Wenger, Markus Wacker, Marcus Magnor, and Chris-
tian Theobalt. Sparse localized deformation components. ACM Transactions on Graphics, 32(6):
1–10, 2013.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035, 2019.

Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dim-
itrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from
a single image. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 10975–10985, 2019.

Xavier Pennec.

Intrinsic statistics on riemannian manifolds: Basic tools for geometric measure-

ments. Journal of Mathematical Imaging and Vision, 25(1):127–154, 2006.

Xavier Pennec. Statistical Computing on Manifolds: From Riemannian Geometry to Computational

Anatomy, pp. 347–386. Springer Berlin Heidelberg, Berlin, Heidelberg, 2009.

David Pfau, Irina Higgins, Aleksandar Botev, and S´ebastien Racani`ere. Disentangling by subspace
diffusion. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020.

Martin Rumpf and Benedikt Wirth. Variational time discretization of geodesic calculus. IMA Journal

of Numerical Analysis, 35(3):1011–1046, 2015.

Chafik Samir, Sebastian Kurtek, Anuj Srivastava, and Michel Canis. Elastic Shape Analysis of
Cylindrical Surfaces for 3D/2D Registration in Endometrial Tissue Characterization. IEEE Trans-
actions on Medical Imaging, 33(5):1035–1043, 2014.

Josua Sassen, Behrend Heeren, Klaus Hildebrandt, and Martin Rumpf. Geometric optimization
using nonlinear rotation-invariant coordinates. Computer Aided Geometric Design, 77, 2020a.

Josua Sassen, Klaus Hildebrandt, and Martin Rumpf. Nonlinear Deformation Synthesis via Sparse

Principal Geodesic Analysis. Computer Graphics Forum, 39(5), 2020b. ISSN 1467-8659.

11

Published as a conference paper at ICLR 2023

Philipp von Radziewsky, Elmar Eisemann, Hans-Peter Seidel, and Klaus Hildebrandt. Optimized
subspaces for deformation-based shape editing and interpolation. Computers & Graphics, 58:
128–138, 2016.

Guan Wang, Hamid Laga, Ning Xie, Jinyuan Jia, and Hedi Tabia. The shape space of 3d botanical

tree models. ACM Transaction on Graphics, 37(1):7, 2018.

Yuanzhen Wang, Beibei Liu, and Yiying Tong. Linear surface reconstruction from discrete funda-

mental forms on triangle meshes. Computer Graphics Forum, 31(8):2277–2287, 2012.

Yupan Wang, Guiqing Li, Zhichao Zeng, and Huayun He. Articulated-motion-aware sparse local-

ized decomposition. Computer Graphics Forum, 36(8):247–259, 2017.

Yupan Wang, Guiqing Li, Huiqian Zhang, Xinyi Zou, Yuxin Liu, and Yongwei Nie. Panoman:
Sparse localized components–based model for full human motions. ACM Transactions on Graph-
ics, 40(2):1–17, 2021.

Qian Xie, Ian H. Jermyn, Sebastian Kurtek, and Anuj Srivastava. Numerical inversion of SRNFs
for efficient elastic shape analysis of star-shaped objects. In David J. Fleet, Tom´as Pajdla, Bernt
Schiele, and Tinne Tuytelaars (eds.), ECCV, volume 8693 of Lecture Notes in Computer Science,
pp. 485–499. Springer, 2014.

I-Cheng Yeh, Chao-Hung Lin, Olga Sorkine, and Tong-Yee Lee. Template-based 3d model fitting
using dual-domain relaxation. IEEE Transactions on Visualization and Computer Graphics, 17
(8):1178–1190, 2011.

Laurent Younes. Shapes and Diffeomorphisms. Springer, 2010.

Li Zhang, Noah Snavely, Brian Curless, and Steven M. Seitz. Spacetime faces: High resolution
capture for modeling and animation. ACM Transactions on Graphics, 23(3):548–558, 2004. ISSN
0730-0301.

Sharon Zhang, Amit Moscovich, and Amit Singer. Product manifold learning. In Arindam Baner-
jee and Kenji Fukumizu (eds.), The 24th International Conference on Artificial Intelligence and
Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, volume 130 of Proceedings of Ma-
chine Learning Research, pp. 3241–3249. PMLR, 2021.

Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal of

Computational and Graphical Statistics, 15(2):265–286, 2006.

12

Published as a conference paper at ICLR 2023

S1

S1

T2

Figure 6: Factors of the Freaky Torus. We visualize our synthetic shape space by demonstrating the
effect of moving along the individual factors to the final shape. In the first row, we see how the first
factor, an S1, controls the deformation of the latitudinal cross-section. We show the deformed torus
from the top. In the second row, we see how the next factor, another S1, controls the deformation
of the longitudinal cross-section. Here, we show the torus cut in half to better highlight the cross-
section’s shape. In the last row, we show how the third factor, a two-dimensional flat torus T2,
controls the position of a bump on the deformed torus.

A FREAKY TORUS

In this appendix, we provide more details on the construction of our shape space Freaky Torus of
deformed tori, a synthetic shape space with factors S1 ×S1 ×T2, whose action is also summarized in
Figure 6. We will first derive its continuous version leading to a family of parametrization and then
arrive at the discrete version via a simple spatial discretization. To begin, we recall the parametriza-
tion f : [0, 2π)2 → R3 of a standard torus of revolution with radii R and r of the latitudinal and
longitudinal circular cross-sections respectively, which is given by

f (u, v) := R





cos(u)
sin(u)
0





 + r



cos(u) cos(v)
sin(u) cos(v)
sin(v)



 .

(6)

The first two factors S1 × S1 of our shape space control the deformation of these cross-sections into
ellipses and hence we want to replace the parametrizations of the cross-sectional circles by ones of
appropriate ellipses. To this end, a circle deformed into an ellipse with semi-axes’ lengths a and b
rotated by η/2 against the coordinate axis is parametrized by

τ η,a,b(t) :=

(cid:18)a cos( η
a sin( η

2 ) cos(t − η
2 ) cos(t − η

2 ) − b sin( η
2 ) + b cos( η

2 ) sin(t − η
2 )
2 ) sin(t − η
2 )

(cid:19)

.

(7)

The phase-shift in the parametrization comes from the fact that the rotation of the semi-axes is also
achieved by a warping of the circle instead of rotating it. We introduced this parametrization with
the half-scaling of η for cosmetic reasons so that later all deformations are parametrized over the
same interval [0, 2π).

Now, we use this parametrization as replacement for the circular cross-section in the parametrization
f of the torus. We fix the semi-axes’ lengths aR, bR and ar, br of the latitudinal and longitudinal
cross-sections respectively and introduce their rotation as parameters α, β ∈ [0, 2π). This leads to
the parametrization

f α,β(u, v) := R






τ α,aR,bR
1
τ α,aR,bR
2

0

(u)
(u)






 + r




cos(u) τ β,ar,br
1
sin(u) τ β,ar,br
1
τ β,ar,br
(v)
2

(v)
(v)




 .

(8)

The last factor T2 controls the position on a bump on such a torus. To describe the corresponding
deformation, we first need the normal of the surface in which direction the bump will point. It is

13

Published as a conference paper at ICLR 2023

given by the usual formula for smooth surfaces namely

nα,β(u, v) :=

∂uf α,β × ∂vf α,β
∥∂uf α,β × ∂vf α,β∥

(u, v).

(9)

Then the bump is a deformation in the direction of this normal around the position determined by
(γ, ζ) ∈ [0, 2π)2 with maximal height h. To limit the support of the deformation, we use a simple
Gaussian with parameter ε on the distance to the center point.

We finally arrive at the parametrization of the continuous version of our synthetic shape space

F : [0, 2π) × [0, 2π) × [0, 2π)2 → (cid:0)[0, 2π)2 → R3(cid:1)

(cid:32)

(α, β, γ, ζ) (cid:55)→

(u, v) (cid:55)→ f α,β(u, v) + h e−

∥f α,β (u,v)−f α,β (γ,ζ)∥2
ε2

(cid:33)

nα,β(γ, ζ)

,

(10)

where the images are parametrizations of embedded surfaces. To obtain discrete surfaces, we apply
these to a fixed triangulation of the torus, which yields the discrete version of the shape space.
For the dataset we used in our experiments, we chose aR = ar = 1, bR = br = 1
2 , R = 0.375, r =
0.125, h = 0.075, and ε = 0.05. It is available along with the code at https://gitlab.com/
jrsassen/freaky-torus.

B ADDITIONAL RESULTS

Figure 7: Exemplary SPGA modes extracted from the SCAPE dataset. We show a selection of
modes generating two of the factor manifolds (indicated by the blue frames; not all modes of each
factor manifold are shown as indicated by the purple dots). Note that while these deformations
move many nodal positions, their support in NRIC is indeed localized. For example, the fifth mode
is supported mainly in the hip region of the shape. This also links it with the other modes in the
same group even though they might move a different leg.

Sparsity In Section 4, we postulated several assumptions on
the structure of our data manifold, which raises the question if
they are actually satisfied in our experiments. Since we work
with NRIC, assumption (0) is fulfilled as they are given as data
on the edges of a mesh (see also Appendix D). Then, for the
freaky torus example, we explicitly constructed a product man-
ifold (hence assumption (1) is fulfilled) and chose the factors
such that they act on different length scales, which entails the
fulfillment of assumption (2). For the examples using a decom-
position via SPGA, we rely on it producing sparsely supported
modes that can be grouped by their spatial overlap. As already
Sassen et al. (2020b) observed, the resulting factorization of the
data manifold fulfills our assumptions. In Figure 7, we show ex-
emplary modes from the SCAPE dataset highlighting that this is
indeed the case.

extracted

Exemplary PGA
Figure 8:
from the
modes
SCAPE dataset
showing the
global support typical for such
modes.

The sparsity of the SPGA modes is a result of the regularization
R in the SPGA problem (1). To still achieve a good approximation of the input data, the modes

14

Published as a conference paper at ICLR 2023

typically localize in different regions of the shape. This allows us to group them as explained before
to obtain the factor manifolds that fulfill assumptions (1) and (2). In contrast, this is not the case for
PGA modes, i.e. if we do not use any regularization there is no reason to expect localized support,
which is also illustrated in Figure 8. This means our method is not applicable for such modes.

Animation One possible application of our composite network is efficient animation of shapes.
In this context, we can consider shape interpolation and extrapolation problems, which correspond
to the evaluation of the Riemannian logarithm and exponential map as explained in Section 3 and
Appendix C. For the case of shape interpolation, we are given two shapes by their latent coordinates
a(0) ∈ Rm and a(1) ∈ Rm respectively. Then, the latent coordinates of intermediate shapes are
obtained by linear interpolation, i.e. we define a(t) := t a(1) + (1 − t)a(0) for t ∈ [0, 1]. By
evaluating our composite network on these coordinates, we obtain the approximate NRIC z(t) :=
Ψζ(ψζ
J (aJ (t)) of these shapes, where aj(t) ∈ Rmj are the factorized coordinates
as before. This leads to a smooth interpolation between shapes as demonstrated in Figure 9 and
the supplementary video. Shape extrapolation can be equivalently phrased by considering linear
extrapolation in the latent space Rm.

1(a1(t)), . . . , ψζ

t = 0

1
8

2
8

3
8

4
8

5
8

6
8

7
8

t = 1

Ψζ

ψζ
1 (a1(1))

ψζ
2 (a2(1))

ψζ
3 (a3(1))

ψζ
4 (a4(1))

ψζ
5 (a5(1))

ψζ
6 (a6(1))

Figure 9: For two given shapes with latent coordinates a(0) and a(1), we compute interpolating
NRIC z(t) using our composite network. In the top row, we see the surfaces reconstructed from
these NRIC for intermediate time steps exhibiting smooth deformations. Below, we also show the
elements ψζ
j (aj(1)) from the factor manifolds Mj which lead to the final shape by applying the
combination network Ψζ. These individual factors lead primarily to deformations of the legs for
ψ1 and ψ3, of the arms for ψ4 and ψ5, of the wrists for ψ2, and of the head for ψ6. See also the
supplementary video.

Number of Samples We observed that our composite network can also be trained with smaller
amounts of samples than we used in subsection 5.2. For example on the SCAPE dataset, if we only
use 20 % of the data as training set (about 800 samples) then we still achieve an R2 of 0.86. Even if
we use a mere 5 % (200 samples) we still reach an R2 of 0.77. We observed a similar behavior on
the SMPL+X dataset with an R2 of 0.85 at 20 % training data and of 0.74 at 5 % training data.

Runtimes Our network-based approach enables runtime efficient approximation of the exponen-
tial map. For example, on the SCAPE dataset, we used K = 16 time steps to evaluate the time-
discrete exponential map (see Appendix C) when generating the training samples. The computation
for each such evaluation required around 8 seconds. In contrast, evaluating the networks takes about
10 milliseconds. To render the result, we have to reconstruct the nodal positions of the triangle
mesh from the NRIC, for which we use the nonlinear least-squares method from Fr¨ohlich & Botsch
(2011). This requires a small number (e.g. 2 to 3 in Figure 9) of Gauß–Newton iterations taking
about 20ms each. Overall the performance is comparable to the approach by Sassen et al. (2020b),
which in contrast to our approach is limited in the amount of latent dimensions it can handle, though.

15

Published as a conference paper at ICLR 2023

C RIEMANNIAN OPERATORS AND THEIR DISCRETIZATIONS IN BRIEF

The tangent space TzS of a manifold S in the point z ∈ S is the vector space of all velocities a path
in S can have when passing through z. A Riemannian manifold equips each of these tangent spaces
TzS with an inner product gz(·, ·) so that norms of velocity vectors and angles between them can be
measured. The length of a path γ : [0, 1] → S in the Riemannian manifold then is the time integral
of the norm of its velocity ˙γ, L[γ] = (cid:82) 1
(cid:112)gγ(t)( ˙γ(t), ˙γ(t)) dt. Given two points z0, z1 ∈ S, the
shortest connecting path γ with γ(0) = z0, γ(1) = z1 is called a geodesic, and the Riemannian
distance dist(z0, z1) between both points is defined as its length. It is essentially an application of
Jensen’s inequality that the geodesic connecting z0 and z1 can equivalently be found by minimizing
the path energy

0

(cid:90) 1

E[γ] =

gγ(t)( ˙γ(t), ˙γ(t)) dt

0

instead of L[γ]. Yet another view of geodesics, following from the optimality conditions of the
above minimization, is that they are paths that always go straight and at constant speed, i.e. they do
not accelerate (neither do they change the motion direction, nor do they change the velocity along
this direction; on the earth’s surface, for instance, geodesics are great circles). This last viewpoint
suggests to associate to every v in the tangent space TzS at an arbitrary z ∈ S a point y ∈ S, which
is defined as follows: Given v ∈ TzS, just start out at z with initial velocity v and continue straight
(i.e. along a geodesic) for total time 1. The map which maps v to the arrival point y is the so-called
Riemannian exponential map

expz : TzS → S,

expz v = y.

Via this exponential map one can identify the tangent space TzS with the manifold S (at least if
expz is injective, otherwise TzS can be identified with a multiple covering of S). The inverse map
is the Riemannian logarithm logz : U → TzS, defined for a small enough neighborhood U ⊂ S of
z. For a point z1 ∈ U , it tells us the initial velocity logz z1 of the geodesic from z to z1.
Riemannian geodesics, logarithms and exponentials are expensive to approximate computationally.
The computation typically has to be performed in charts or local parametrizations of the manifold,
i.e. S is identified with (an open subset of) Rn and the Riemannian metric with a symmetric positive
definite matrix Gz ∈ Rn×n depending on z ∈ Rn. A variational discretization by Rumpf & Wirth
(2015) approximates continuous paths γ by time-discrete paths (γ0, . . . , γK) to be interpreted as
polygonal paths in Rn with vertices γ0, . . . , γK at times 0
K . The path energy is then
approximated by a discrete path energy

K , . . . , K

K , 1

E[(γ0, . . . , γK)] = K

K
(cid:88)

k=1

W (γk−1, γk),

where W (z0, z1) is a second order accurate approximation to the squared Riemannian distance
dist2(z0, z1) (for instance, W (z0, z1) = (z0 − z1)T Gz0(z0 − z1)). E may be viewed as a Riemann
sum approximation of the integral in E. Minimizing E under fixed end points then yields a discrete
K-geodesic, i.e. a discrete approximation (γ0, . . . , γK) to a geodesic between γ0 and γK. The
initial velocity K(γ1 − γ0) of this polygonal path is the discrete approximation of the Riemannian
logarithm logγ0 γK. The discretization of the Riemannian exponential works the other way round:
Given a velocity vector v ∈ Rn we approximate expγ0 v ∈ Rn by that point γK ∈ Rn such that the
discrete K-geodesic (γ0, . . . , γK) has initial velocity K(γ1 − γ0) = v. This point can be found by a
time stepping procedure — one first sets γ1 = γ0 + v
K and then iteratively computes γ2, γ3, . . . γK
as follows: Since each triplet γk−1, γk, γk+1 of a discrete K-geodesic forms a discrete 3-geodesic
(much like any subsegment of a continuous geodesic is itself again a geodesic), γk must minimize
W [γk−1, γk] + W [γk, γk+1]. The corresponding nonlinear optimality condition

0 = ∂2W [γk−1, γk] + ∂1W [γk, γk+1]

is then solved via Newton’s method for γk+1, given γk−1 and γk.
Instead of working on charts, one can also consider an implicitly defined manifold M = (cid:8)z ∈
Rm (cid:12)
(cid:12) Q(z) = 0(cid:9), for suitable smooth functions Q : Rm → Rr. In this case, one proceeds analo-
gously constraining the search for points on the manifold via a Lagrangian approach.

16

Published as a conference paper at ICLR 2023

D A RECAP OF NONLINEAR ROTATION-INVARIANT COORDINATES

Let us briefly review the tools introduced by Wang et al. (2012) for a discrete version of the funda-
mental theorem of surfaces. Consider a simply connected, triangular surface with the set of vertices
V, edges E ⊂ V × V, and faces F ⊂ V × V × V. For a given vector of vertex positions X ∈ R3|V|,
we introduce the vector of all edge lengths l(X) = (le(X))e∈E and the vector of all dihedral angles
θ(X) = (θe(X))e∈E . As discussed by Wang et al. (2012), to ensure that the edge length and dihe-
dral angle data z = (l, θ) ∈ R2|E| actually corresponds to a triangular surface immersed in R3, two
admissibility conditions have to be fulfilled.

The obvious first condition is the triangle inequality for the edge lengths on all
triangles.
We write this condition in formulas as Tf (l) > 0 for all f ∈ F where Tf (l) =
(li + lj − lk
li − lj + lk −li + lj + lk) for a face f ∈ F with edge lengths li, lj, lk, and the
above inequality is meant componentwise. Fulfillment of these conditions guarantees that we can
construct individual triangles from given edge lengths. However, we need a second condition assur-
ing that these triangles fit together with the given dihedral angles to form a surface, i.e. to guarantee
integrability of z. For simply-connected discrete surfaces, this can be broken down to individual
conditions for the fans of triangles surrounding any vertex v in the set of interior vertices V0. For-
mally, we express this individual condition as Qv(z) = 0, which guarantees that we can construct
the geometry of this fan from z. The explicit formula for this was introduced by Wang et al. (2012).
If this condition is fulfilled for all interior vertices, one can show that it is indeed possible to construct
the geometry of the entire surface. Sassen et al. (2020a) demonstrated that Qv and its derivatives can
be robustly and efficiently computed using quaternions. The conditions can be extended to higher-
genus surfaces by including integrability conditions along non-contractible paths that generate the
fundamental group on the triangular surface, but this is not used here.
The manifold of all z ∈ R2|E| corresponding to immersed triangular surfaces of the given mesh
connectivity can be given by

M = (cid:8)z ∈ R2|E| (cid:12)

(cid:12) T (z) > 0, Q(z) = 0(cid:9),

where we collect all constraints in vector-valued functionals T = (Tf )f ∈F and Q = (Qv)v∈V0.
As Sassen et al. (2020a), we call the manifold M the NRIC manifold (Nonlinear Rotation-Invariant
Coordinates). The differential structure of this manifold is at first described in terms of the tangent
space, which is given at position z ∈ M by TzM = ker DQ(z) := {w ∈ R2|E| | DQ(z)w = 0} .
Here the matrix DQ(z) ∈ R3|V0|×2|E| is the Jacobian of Q. The triangle inequalities define an open
set of R2|E| and are thus not needed to define the tangent space.

The advantage of NRIC is that they allow a local description of shell deformations based on the local
variation of the edge lengths, which encodes membrane distortions, and the local variation of the di-
hedral angles, which encodes bending distortions. We use a Riemannian metric g on the manifold M
that reflects the physical dissipation caused by these infinitesimal variations of the discrete surface.
In detail, the metric coincides with the Hessian of an elastic energy W , i.e. gz : R2|E| × R2|E| → R
with gz = 1
2 HessW [z, ·] restricted to TzM × TzM. We use an elastic energy describing a de-
formation from a configuration z to a configuration ˜z that decomposes into a membrane energy
and a bending energy, i.e. W [z, ˜z] = Wmem[z, ˜z] + Wbend[z, ˜z]. The bending energy is given by
Wbend[z, ˜z] = (cid:80)
3 (af + af ′) for the two faces f and f ′ adja-
cent to e ∈ E (af is the area of f ). Furthermore, the membrane energy is given by Wmem[z, ˜z] =
(cid:1) log det A − µ − λ
(cid:80)
4 .
The constants µ and λ are positive material constants, and G[z, ˜z] denotes the Cauchy–Green strain
tensor of the deformation — describing the face-wise distortion — as a function of the edge lengths
on each face. Let us remark that the logarithmic term in the energy density Wmem acts as a barrier
ensuring the triangle inequalities for finite-energy deformations.

f ∈F af · Wmem(G[z, ˜z]|f ), where Wmem(A) := µ

4 det A − (cid:0)µ + λ

e∈E (θe − ˜θe)2d−1

e, where de = 1

2 tr A + λ

e l2

2

17

Published as a conference paper at ICLR 2023

E LIST OF SYMBOLS

Riemannian Shape Spaces (see also Appendix C)
S

Shape space, p. 3

Point of a shape space, p. 3
Tangent space of S at point z, p. 3
Riemannian metric at point z, p. 3
Riemannian logarithmic map at point z, p. 3
Riemannian exponential map at point z, p. 3

z
TzS
gz
logz
expz
Principal Geodesic Analysis
¯z

Riemannian center of mass, p. 3

R

Regularization functional, p. 3

Triangle Meshes and NRIC (see also Appendix D)
V

Vertices of a triangular surface, p. 3

E

Edges of a triangular surface, p. 3

F
Triangles of a triangular surface, p. 3
N (e) Neighbors of an edge in a triangle mesh, p. 4
X

Vector storing the vertex positions of all vertices of a triangular surface, p. 3

l
Vector of edge lengths, p. 3
Length of edge e, p. 3
le
θ
Vector of dihedral angles, p. 3
θe
Dihedral angle of triangles meeting at edge e, p. 3
∥ · ∥p,¯z Weighted Lp-norm for NRIC with reference shape ¯z, p. 3
Neural Networks
Nt
T
ρ

Number of layers of a network, p. 4
Nonlinear activation function, p. 4

Layer sizes of a network, p. 4

Learnable bias, p. 4

Network parameters, p. 4

Matrix of learnable weights, p. 4

ζ
W t
i
bt
J , Jj Loss functions used for training, p. 6
ψζ
j
Ψζ

Learnt parametrization of the factor manifolds, p. 5
Learnt map combining the outputs of the ψζ

j , p. 5

Data Manifolds
M Riemannian data manifold, p. 4
Mi
Φ

Factors of a product manifold, p. 5

Parametrization of a data manifold, p. 4

18

