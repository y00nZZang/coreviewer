Published as a conference paper at ICLR 2023

OUTCOME-DIRECTED REINFORCEMENT LEARNING
BY UNCERTAINTY & TEMPORAL DISTANCE-AWARE
CURRICULUM GOAL GENERATION

Daesol Cho*, Seungjae Lee*, H. Jin Kim

Seoul National University, Automation and Systems Research Institute (ASRD,
Artificial Intelligence Institute of Seoul National University (AIIS)
{dscho1234, ysz0301, hjinkim}@snu.ac.kr

ABSTRACT

Current reinforcement learning (RL) often suffers when solving a challenging
exploration problem where the desired outcomes or high rewards are rarely ob-
served. Even though curriculum RL, a framework that solves complex tasks by
proposing a sequence of surrogate tasks, shows reasonable results, most of the
previous works still have difficulty in proposing curriculum due to the absence
of a mechanism for obtaining calibrated guidance to the desired outcome state
without any prior domain knowledge. To alleviate it, we propose an uncertainty
& temporal distance-aware curriculum goal generation method for the outcome-
directed RL via solving a bipartite matching problem. It could not only provide
precisely calibrated guidance of the curriculum to the desired outcome states but
also bring much better sample efficiency and geometry-agnostic curriculum goal
proposal capability compared to previous curriculum RL methods. We demon-
strate that our algorithm significantly outperforms these prior methods in a variety
of challenging navigation tasks and robotic manipulation tasks in a quantitative
and qualitative way.

1 INTRODUCTION

While reinforcement learning (RL) shows promising results in automated learning of behavioral
skills, it is still not enough to solve a challenging uninformed search problem where the desired
behavior and rewards are sparsely observed. Some techniques tackle this problem by utilizing the
shaped reward (Hartikainen et al. or combining representation learning for efficient explo-
ration (Ghosh et al.|/2018). But, these not only become prohibitively time-consuming in terms of
the required human efforts, but also require significant domain knowledge for shaping the reward
or designing the task-specific representation learning objective. What if we could design the algo-
rithm that automatically progresses toward the desired behavior without any domain knowledge and
human efforts, while distilling the experiences into the general purpose policy?

An effective scheme for designing such an algorithm is one that learns on a tailored sequence of
curriculum goals, allowing the agent to autonomously practice the intermediate tasks. However, a
fundamental challenge is that proposing the curriculum goal to the agent is intimately connected to
the efficient desired outcome-directed exploration and vice versa. If the curriculum generation is
ineffective for recognizing frontier parts of the explored and feasible areas, an efficient exploration
toward the desired outcome states cannot be performed. Even though some prior works propose
to modify the curriculum distribution into a uniform one over the feasible state space (Pong et al}
(2022) or generate a curriculum based on the level of difficulty (Florensa et al.
bots leaner tay most of these methods show slow curriculum progress due to the
process of skewing the curriculum distribution toward the uniform one rather than the frontier of
the explored region or the properties that are susceptible to focusing on infeasible goals where the
agent’s capability stagnates in the intermediate level of difficulty.

“Equal contribution.

‘Code is available :|nttps://github.com/ jayLEE0301/outpace_official
Published as a conference paper at ICLR 2023

he : Curriculum goals
t : Agent (@>@>@>@)

: Desired outcome
[Uncertainty]

[Temporal dist. from initial]

= _
Temporally ‘Temporally
Close Far

Figure 1: OUTPACE proposes uncertainty and temporal distance-aware curriculum goals to enable
the agent to progress toward the desired outcome state automatically. Note that the temporal distance
estimation is reliable within the explored region where we query the curriculum goals.

Conversely, without the efficient desired outcome-directed exploration, the curriculum proposal
could be ineffective when recognizing the frontier parts in terms of progressing toward the de-
sired outcomes because the curriculum goals, in general, are obtained from the agent’s experiences
through exploration. Even though some prior works propose the success-example-based approaches
(Fu et al. 2018} Singh et al} 2019} Li et al} 2021p, these are limited to achieving the only given
example states, which means these cannot be generalized to the arbitrary goal-conditioned agents.
Other approaches propose to minimize the distance between the curriculum distribution and the de-
sired outcome distribution [Klink et al.|/2022), but these require an assumption that
the distance between the samples can be measured by the Euclidean distance metric, which cannot
be generalized for an arbitrary geometric structure of the environment. Therefore, we argue that
the development of algorithms that simultaneously address both outcome-directed exploration and
curriculum generation toward the frontier is crucial to benefit from the outcome-directed curriculum
RL.

In this work, we propose Outcome-directed Uncertainty & TemPoral distance-Aware Curriculum
goal gEneration (OUTPACE) to address such problems, which requires desired outcome examples
only and not prior domain knowledge nor external reward from the environment. Specifically, the
key elements of our work consist of two parts. Firstly, our method addresses desired outcome-
directed exploration via a Bayesian classifier incorporating an uncertainty quantification based on
the conditional normalized maximum likelihood (Zhou & Levine 2021} Li et al. 2021), which
enables our method to propose the curriculum into the unexplored regions and provide directed
guidance toward the desired outcomes. Secondly, our method utilizes Wasserstein distance with a
time-step metric not only for a temporal distance-aware intrinsic reward but
also for querying the frontier of the explored region during the curriculum learning. By deploying
the above two elements, we propose a simple and intuitive curriculum learning objective formalized
with a bipartite matching to generate a set of calibrated curriculum goals that interpolates between
the initial state distribution and desired outcome state distribution.

To sum up, our work makes the following key contributions.

¢ We propose an outcome-directed curriculum RL method which only requires desired out-
come examples and does not require an external reward.

* To the best of the author’s knowledge, we are the first to propose the uncertainty & tempo-
ral distance-aware curriculum goal generation method for geometry-agnostic progress by
leveraging the conditional normalized maximum likelihood and Wasserstein distance.

¢ Through several experiments in goal-reaching environments, we show that our method
outperforms the prior curriculum RL methods, most notably when the environment has a
geometric structure, and its curriculum proposal shows properly calibrated guidance toward
the desired outcome states in a quantitative and qualitative way.

2 RELATED WORKS

While a number of works have been proposed to improve the exploration problem in RL, it
still remains a challenging open problem. For tackling this problem, prior works include state-
visitation counts (Bellemare et al.||2016} (2017), curiosity/similarity-driven explo-
ration (Pathak et al.| 2018), the prediction model’s uncertainty-based
Published as a conference paper at ICLR 2023

(2019), mutual information-based exploration
2019} [Zhao et al [2021 2022), maximizing the
. Unfortunately,
these techniques are uninformed about the desired outcomes: the trained agent only knows how to
visit frontier states as diverse as possible. On the contrary, we consider a problem where the de-
sired outcome can be specified by the given desired outcome examples, allowing for more efficient
outcome-directed exploration rather than naive frontier-directed exploration.

Some prior methods that try to accomplish the desired outcome states often utilize the provided suc-
cess examples (Fu et al} 2018} Singh et al] 2019} Eysenbach et al. 2021} Li et al} 2021p. However,
they do not provide a mechanism for distilling the knowledge obtained from the agent’s experiences
into general-purpose policies that can be used to achieve new test goals. In this work, we utilize
the Wasserstein distance not only for an arbitrary goal-conditioned agent but also for querying the
frontier of the explored region during curriculum learning. Although the Wasserstein distance has
been adopted by some previous research, they are often limited to imitation learning or skill discov-
ery (Dadashi et al.| 2020} [Haldar et al-|/2022} [x {2019} [Durugkar et al} 202Ta} [Fickinger|
(2021). Another work (Durugkar et al. ies to utilize the Wasserstein distance with
the time-step metric for training the goal-reaching agent, but it requires a stationary goal distribution
for stable distance estimation. Our work is different from these prior works in that the distribution
during training is non-stationary for calibrated guidance to the desired outcome states.

Suggesting a curriculum can also make exploration easier where the agent learns on a tailored se-
quence of tasks, allowing the agent to autonomously practice the intermediate tasks in a training
process. However, prior works often require a significant amount of samples to measure the curricu-
lum’s level of difficulty (Florensa et al.| {2018} [Sukhbaatar et al.||2017), learning progress
(2020), regret (Jiang et al.|]2021). Curricula are often generated by modifying the goal dis-
tribution into the frontier of the explored region via maximizing a certain surrogate objective, such
as the entropy of the goal distribution (Pong et al.|{2019), disagreement between the value function
ensembles (Zhang et al.| |2020), but these methods do not have convergence mechanism to the de-
sired outcome distribution. While some algorithms formulate the generation of a curriculum as an
explicit interpolation between the distribution of target tasks and a surrogate task distribution
el these still depend on the Euclidean distance metric when measuring
the distance between distributions, which cannot be generalized for an arbitrary geometric structure
of the environment. In contrast, our method not only provides calibrated guidance to the desired out-
come distribution in a sample efficient way but also shows geometry-agnostic curriculum progress
by leveraging the bipartite matching problem with uncertainty & temporal distance-based objective.

3. PRELIMINARY

We consider the Markov decision process (MDP) M = (S,A,G,T, 0,7), where S denotes the
state space, A the action space, G the goal space, T'(s’|s, a) the transition dynamics, po the initial
distribution, p, the state visitation distribution when the agent follows the policy 7, and 7 the dis-
count factor. The MDP in our framework is provided without a reward function and considers an
environment in which only the desired outcome sample {g* }/_, are given, assuming that g* is
obtained from the desired outcome distribution G*+. Therefore, our work employs a trainable in-
trinsic reward function r : S x G x A > R, which is detailed in Section|4.1] Also, we represent
the set of curriculum goals as {g* }_, and assume that these are sampled from the curriculum goal
distribution G° obtained by our algorithm.

3.1 WASSERSTEIN DISTANCE OVER THE TIME-STEP METRIC

The Wasserstein distance represents how much “work” is required to transport one distribution to
another distribution following the optimal transport plan (Villani 2009} Durugkar et al.| 2021p.
In this section, we describe the Wasserstein distance over the time-step metric and how it can be
obtained with a potential function f. Consider a metric space (Vd), where ¥ is a set and dis a
metric on 4’, and two probability measures ju, v on V. The Wasserstein-p distance for a given metric
dis defined as follows,

Wr(nev) = inf Buxyyayl@(X YP? "2" sup Byrlf (Wl —Eewulf@)ll
ye (u,v) Wfllns1

Published as a conference paper at ICLR 2023

where a joint distribution + denotes a transport plan, and II(y,1) denotes the set of all possible
joint distributions 7, and the second equality is held by the Kantorovich-Rubinstein duality with

1-Lipschitz functions (f : Y > R) Villani 2009} |Arjovsky et al.| 2017).

If we could define the distance metric as d™(s, s,), which is a time-step metric (quasimetric) based
on the number of transition steps experienced before reaching the goal s, € G for the first time
when executing the goal-conditioned policy 7(a|s, s,), we could design a temporal-distance aware
RL by minimizing the Wasserstein distance W (p,, G) that gives an estimate of the work needed to
transport the state visitation distribution p, to the goal distribution G:

Wi(px,G) = ip. Bso~olf(s0)] — Eswp, Lf(s)] (2)

Then, the potential function f is approximately increasing along the optimal goal-reaching trajec-
tory. That is, if p(s) consists of the states optimally reaching toward the goal s,, f(s) increases
along the trajectory and f(s,) has the maximum value Adopting these
prior works, the 1-Lipschitz potential function f with respect to d™(s, 8,) could be ensured by en-
forcing that the difference in values of f on the expected transition from every state is bounded by 1
as follows, (detailed derivations are in Appendix[B])

sup{Eaer(le.sy)s'erClsayllF(s) — f(s)} <1. (3)

3.2 CONDITIONAL NORMALIZED MAXIMUM LIKELIHOOD (CNML)

For curriculum learning, our work utilizes conditional normalized maximum likelihood (CNML)
that can perform an uncertainty-aware classification based
on previously observed data by minimizing worst-case regret. Let D = {(sp, ep) }nat be a set
of data containing pairs of states 1,1 and success labels €1:,-1 € {0,1}, where ‘1’ represents
the occurrence of the desired event. Given a query point s,,, CNML in our framework defines the
distribution ponmL(en|Sn) which predicts the probability that the state s,, belongs to the desired
outcome distribution G* (e = 1).

To explain how CNML predicts the label of s,,, we suppose © is a set of models, where each
model 6 € © can represent a conditional distribution of labels, pg (€1:n|S1:n). CNML considers the
possibilities that all the possible classes (0,1) will be labeled to the query point s,,, and obtains the
models 6(€1:n|Sizn) € © that represent the augmented datasets D U (sn, en) well by solving the
maximum likelihood estimation (MLE) problem (LHS of Eq. (4)). Then, CNML that minimizes the
regret over those maximum likelihood estimators can be written as follows (Bibas et al.}/2019),

(4)

4 F 79,(€ = tl8n)
6; = arg max E(s,¢)eDu(s,,e,=1) [log pa(e|s)], PoNML(En = #|8n) ; -
Tere) Yo j=0 Pa, (e = J/8n)

If the query point s,, is close to one of the data points in the datasets, CNML will have difficulty in
assigning a high likelihood to labels that are significantly different from those of nearby data points.
However, if s,, is far from the data points in the dataset, each MLE model 8-01 will predict the label
€n as its own class, which leads to a large discrepancy in the predictions between the models and
provides us with normalized likelihoods closer to uniform (RHS of Eq 4). Thus, by minimizing the
regret through labeling all possible classes to the new query data, CNML can provide a reasonable
uncertainty estimate (Li et al. 2021} Zhou & Levine} 2021) on the queried s,, and classify whether
the queried s,, is similar to the previously observed data either the forms of label 0, 1, or out-of-
distribution data, which is predicted as 0.5.

However, the classification technique via CNML described above is in most cases computationally
intractable, as it requires solving separate MLE problems until convergence on every queried data.
Previous methods proposed some ideas to amortize the cost of computing CNML distribution

2021 2021). Following those prior methods, our work adopts MAML (

Published as a conference paper at ICLR 2023

Figure 2: Visualization of the uncertainty quantification along training progress (left) and trained
£3 (s) (right) in the Point-N-Maze environment. In the right figure, high reward means temporally
close to the desired outcome states, and low reward means the opposite.

to address the computational intractability of CNML by training one meta-learner net-
work that can quickly adapt to each model 6, rather than training each model separately. As
requires samples from G* and replay buffer B for the inference, the probability should
be represented as ponmi(e = i|s;G*, B), but we use pcnmi(e = i|s) for notational simplicity in

this work. More details about the meta-learning-based classification are included in Appendix [B]

4 METHOD

For a calibrated guidance of the curriculum goals to the desired outcome distribution, we propose to
progress the curriculum towards the uncertain & temporally distant area before converging to Gr,
as it is not only the most intuitive way for exploration but also enables the agent to progress without
any prior domain knowledge on the environment such as obstacles. In short, our work tries to obtain
the distribution of curriculum goals G° that are considered (a) temporally distant from po and, (b)
uncertain and, (c) being progressed toward the desired outcome distribution G*.

4.1 TEMPORAL DISTANCE-AWARE RL WITH THE INTRINSIC REWARD

This section details the intrinsic reward for the RL agent as well as the method of training the
parameterized potential function 7, trained with the data collected by the policy 7(a|s,s,). We
consider a 1-Lipschitz potential function f3 whose value increases as the state is far from the initial
state distribution and getting close to the goals s, € G proposed by curriculum learning. Then, we
can train an agent that reaches the goals s, in as few steps as possible by minimizing the Wasserstein
distance Wj (pz, G).

Considering that we can obtain the estimate of Wi (p,,G) by Eq (2), the loss for training the param-
eterized potential function ff can be represented as follows (Durugkar et al.}/202 1b] fap:
Ly = Ess,~Blf3(s) — £3 (9)] +A Es.91,s,~elmax(|f3(s) — f3(s')|-1,0))"] G)

The penalty term with coefficient \ in Eq (5) is from Eq (3) for ensuring the smoothness requirement
since we consider the Wasserstein distance over the time-step metric. Then, assuming the parameter
¢ is trained by Eq (5) at every training iteration, we could obtain the supremum of Eq (2p. Thus, the
reward can be represented as r = f3(s) — f3 (sq), which corresponds to —Wj (pz, 9).

4.2 CURRICULUM LEARNING

As CNML can provide a near-uniform prior (prediction of 0.5) for out-of-distribution data given the
datasets (Section[3.2), we could utilize it by treating the desired outcome states in G* as (e = 1),
and data points in the replay buffer B as (e = 0). Then, we could quantify the uncertainty of a state
s based on CNML as

Mucert (8,G*) = 1 — |powmi(e = 0|s) — ponmi(e = 1\s)|. (6)

which is proportional to the uncertainty of the queried data s. However, 7ucert alone cannot provide
curriculum guidance toward the desired outcome states because it only performs an uninformed
search over uncertainties rather than converging to the desired outcome states. Thus, we modify Eq
() with an additional guidance term:
Published as a conference paper at ICLR 2023

fucert(G°,G*) = Esa ge[log(mucert($,9*) + €* Nguidance(s,9*))]- ie)

where Nguidance($,9*) = (ponmi(e = 1]s) — 0.5) - l(penmn(e = 1\s) > 0.5), and c is a
hyperparameter that adjusts the preference on the desired outcome states. Since the CNML provides
near-uniform prior for out-of-distribution data, 7ucert provides large values in the uncertain areas.
Also, the guidance term Neuidance(S, G +) reflects the preference for the states considered to be closer
to the desired outcome state distribution.

However, in practice, we found the uncertainty quantification itself sometimes has numerical errors,
and it makes pcnmt erroneously predict the states near the initial states or boundaries of the already
explored regions as uncertain areas. Therefore, we assume that the curriculum should incorporate
the notion of not only the uncertainty but also the temporally distant states from po for frontier and
desired outcome-directed exploration. Thus, we formulate the final curriculum learning objective as
follows:

arg max{fucort(9",9*)] + L- Wi(po.9"), (8)

where the temporal distance bias term with a coefficient L is represented by the Wasserstein distance
from initial state distribution po to the curriculum goal distribution G°. And, given a parameterized
1-Lipschitz potential function {7 over the time-step metric d”(s, 8g), we can obtain the estimate of
Wi (po, 9°) by RHS of Eq {i}. Also, if we assume G° to be a finite set of K particles that is sampled
from already achieved states in the replay buffer B, the objective function we aim to maximize can
be represented as follows:

K
,, max SC litncert(s"s 9) +L. [f3(s") - £3(s0)]]; se GF, 86 E po (9)
G°|Ge|=B Gy

It enables to propose the curriculum that reflects not only the uncertainty of the states and preference
on the desired outcomes but also temporally distant states from po, while not requiring prior domain
knowledge about the environment such as an obstacle.

4.3 SAMPLING CURRICULUM GOAL VIA BIPARTITE MATCHING

Since we assume that desired outcome examples from G+ are given rather than its distribution, we
could approximate it by the sampled set Gt (\G+| = Kk). Then, to solve the curriculum learning
problem of Eq (9), we should address the combinatorial setting that requires assigning G° from the
entire curriculum goal candidates in the replay buffer B to the G+, which is addressed via bipartite
matching in this work. With the hyperparameter c = 4, we can rearrange Eq (9) as a minimization
problem with the costs of cross-entropy loss (CE) and temporal-distance bias (f3) term: (Refer to
the Appendix [B]for the detailed derivation.)

min w(s', gi 10
gon, ws 4) (10)
st EGe,gi EGt
w(s',g',) = CE(ponmi(e = 1s"); y = pon (e = 1g',.)) —L.- £3 (s') (1)

Intuitively, before discovering the desired outcome states in Gt, the curriculum goal s’ is proposed
in a region of the state space considered to be uncertain and temporally distant from po in order
to recognize the frontier of the explored regions. And it is kept updated to converge to the desired

outcome states for minimizing the discrepancy between the predicted labels of G+ and Ge.

Then we can construct a bipartite graph G with the cost of the edges w. Let V, and Vy» be the

sets of nodes representing achieved states in replay buffer and Gt respectively. We define a bipartite
graph G({V,,, V,}, E) with the weight of the edges E(.,-) = —w/(.-,-) and separated partitions (V,
and V,). To solve the bipartite matching problem, we utilize the Minimum Cost Maximum Flow
Published as a conference paper at ICLR 2023

Inittat
curriculum

Final
curriculum

(a) OUTPACE(ours) (b) CURROT (c) HGG

Figure 3: Visualization of the proposed curriculum goals. First row: Ant Locomotion, Second row:
Point-N-Maze.

algorithm to find K edges with the minimum cost w connecting V, and Vy. (Ahuja et al.|/1993
(2019)

). The overall training process is summarized in Algorithm[]]in Appendix|B]

5 EXPERIMENTS

We include 6 environments to validate our proposed method. Firstly, various maze environ-
ments (Point U-Maze, N-Maze, Spiral-Maze) are used to validate the geometry-agnostic curricu-
lum generation capability. Also, we experimented with the Ant-Locomotion and Sawyer-Peg Push,
Pick&Place environments to evaluate our method in more complex dynamics or other domains rather
than navigation tasks.

We compare with other previous curriculum or goal generation methods, where each method has the

following properties. HGG (Ren et al.||2019): Minimize the distance between the curriculum and
desired outcome state distributions based on the Euclidean distance metric and value function bias.

CURROT (Klink et al.| (2022): Interpolate between the curriculum and desired outcome state distri-

bution based on the agent’s current performance via optimal transport. GoalGAN (Florensa et al.
(2018): Generate curriculum goals that are intermediate level of difficulty by training GAN ica
2014). PLR Sample increasingly difficult tasks by prioritizing the
levels of tasks with high TD errors. ALP-GMM Fit a GMM with an absolute
learning progress score approximated by the absolute reward difference. VDS (Zhang et al] 2020}:
ewFit

Prioritize goals that maximize the epistemic uncertainty of the value function ensembles.
(Pong et al. 9): Maximize the entropy of the goal distribution to be uniform on the feasible state

space via skewing the distribution trained by VAE (Kingma & Welling} 2013).

5.1 EXPERIMENTAL RESULTS

Firstly, to show how each module in our method is trained, we visualized the uncertainty quantifica-
tion by CNML, and f3(s) values which are proportional to the required timesteps to reach s from
po. The uncertainty quantification results Figure} show that the classifier poy (-|s) successfully
discriminates the queried states as already explored region or desired outcome states, otherwise, un-
certain states. Due to the geometry-agnostic property of the classifier pcnm(-|s), we could propose
the curriculum in the arbitrary geometric structure of the environments, while most of the previous
curriculum generation methods do not consider it.

We also visualized the values of the trained potential function f(s) to show how the intrinsic reward
is shaped (Figure 2). As the potential function 7 (s) is trained to have high values near the desired

outcome states due to the Wasserstein distance with the time-step metric, the results show gradual
increases of f%(s) values along the trajectory toward the desired outcome states. That is, high values

of f(s) indicate the smaller required timesteps to reach the desired outcome state, and this property
brings the advantage in identifying the frontier of the explored region.

To validate whether the curriculum goals are properly interpolated from initial states to desired
outcome states by combining both objectives for curriculum learning (Eq (8), we evaluated the
Published as a conference paper at ICLR 2023

distance

steps (k) steps (k) steps (k)

(a) U-Maze (b) N-Maze (c) Spiral-Maze

distance

steps (k) steps (k) steps tk)
(d) Ant Locomotion (e) Sawyer Push (f) Sawyer Pick&Place
= OUTPACE(ours) © SkewFit @ CURROT = PLR
= HGG = ALPGMM =m GoalGAN DS

Figure 4: Average distance from the curriculum goals to the final goals (Lower is better). Our
method’s increasing tendencies at initial steps in some environments are due to the geometric struc-
ture of the environments themselves. Shading indicates a standard deviation across 5 seeds.

success rate
success rate
success rate

steps (k) steps (k)

(a) U-Maze (b) N-Maze (c) Spiral-Maze

steps (k)

success rate
success rate
success rate

steps (k) steps (k) steps (k)

(d) Ant Locomotion (e) Sawyer Push (f) Sawyer Pick&Place
Figure 5: Episode success rates of the evaluation results. The legends and seeds are the same as Fig
Al Note that the curves of the baselines in some environments are not visible as they overlap at zero
success rates.

progress of the curriculum goals in a quantitative and qualitative way. For quantitative evalua-
tion, we compare with other previous works described above with respect to the distance from the
proposed curriculum goals to G+. As we can see in Figure |4| our method is the only one that
consistently interpolates from initial states to G+ as training proceeds, while others have difficulty
with complex dynamics or geometry of the environments. For qualitative evaluation, we visualized
the curriculum goals proposed by our method and other baselines that show somewhat comparable
results as training proceeds (Figure|3). The results show that our method consistently proposes the
proper curriculum based on the required timesteps and uncertainties regardless of the geometry and
dynamics of the various environments, while other baselines have difficulty as they utilize the Eu-
clidean distance metric to interpolate the curriculum distribution to the G+. We also evaluated the
desired outcome-directed RL performance. As we can see in Figure] our method is able to very
quickly learn how to solve these uninformed exploration problems through calibrated curriculum
goal proposition.

5.2 ABLATION STUDY

Types of curriculum learning cost. We first evaluate the importance of each curriculum learning
objective in Eq (8). Specifically, we experimented only with uncertainty-related objective (only-
enml) and timestep-related objective (only-f) when curriculum learning progresses. As we can see
Published as a conference paper at ICLR 2023

12 —— ourpace(ours)
Ablation only
‘Ablation_only_enm!

— ourrace(ours) OUTPACE(ours)
‘Ablation sparseReward
‘Ablation GAN

—— ourPace(ours) os
oe Ablation_only.f
Ablation_only_enm!

—— ourrace(ours)
‘Ablation sparseReward
‘Ablation GAN .

—— ourpacttours)

ss
— 3

steps (k) steps (k) steps (k)

(a) Cost type (b) Reward & Goal proposition type (c) Effect of ¢ in Nguidance

Figure 6: Ablation study in terms of the distance from the proposed curriculum goals to the desired
final goal states (Lower is better). First row: Ant Locomotion. Second row: Sawyer Pick & Place.
Shading indicates a standard deviation across 5 seeds.

in Figure [6a] both objectives play complementary roles, which support the requirement of both
objectives. Without one of them, the agent has difficulty in progressing the curriculum goals toward
the desired outcome states due to the local optimum of ff or the numerical error of poy, and
more qualitative/quantitative results and analysis about this and other ablation studies are included
in Appendix[C]

Reward type & Goal proposition method. Secondly, we replace the intrinsic reward with the
sparse reward, which is typically used in goal-conditioned RL problems, to validate the effects of
the timestep-proportionally shaped reward. Also, for comparing the curriculum proposition method,
we replace the Bipartite Matching formulation with a GAN-based generative model, which is similar
to[Florensa et al-|(2018), but we label the highly uncertain states as positive labels instead of success
rates. As we can see in Figure the timestep-proportionally shaped reward shows consistently
better results due to the more informed reward signal compared to the sparse one, and the generative
model has difficulty in sampling the proper curriculum goals because GAN shows training instability
with the drastic change of the positive labels, while our method is relatively insensitive because
curriculum candidates are obtained from the experienced states from the buffer B rather than the
generative model.

Effect of cin Neuidance- Lastly, we experiment with different values of hyperparameter c to validate
the effect of Nguidance On curriculum learning. When c is smaller than the default value 4, it is still
possible to explore most of the feasible state space except the area near the desired outcome states
due to the uncertainty & temporal distance-aware curriculum (Figure But, we could verify that
Neuidance’S effect becomes smaller as c decreases and Nguidance helps to guide the curriculum goals
to the desired outcome states precisely. This is consistent with our analysis that the uncertainty &
temporal distance themselves can provide curriculum goals in the frontier of the explored region
while Neuidance Can further accelerate the guidance to the desired outcome states.

6 CONCLUSIONS

In this work, we consider an outcome-directed curriculum RL where the agent should progress
toward the desired outcome automatically without the reward function and prior knowledge about
the environment. We propose OUTPACE, which performs uncertainty, temporal distance-aware
curriculum RL with intrinsic reward, based on the classifier by CNML, and Wasserstein distance
with time-step metric. We show that our method can outperform the previous methods regarding
sample efficiency and curriculum progress quantitatively and qualitatively. Even though our method
shows promising results, there are some issues with computational complexity due to the innate
properties of the meta-learning inference procedure itself. Thus, it would be interesting future work
to find a way to reduce the inference time for less training wall-clock time.
Published as a conference paper at ICLR 2023

7 ACKNOWLEDGEMENT

This work was supported by AI based Flight Control Research Laboratory funded by Defense Acqui-
sition Program Administration under Grant UD200045CD. Seungjae Lee would like to acknowledge
financial support from Hyundai Motor Chung Mong-Koo Foundation.

REFERENCES

R K Ahuja, T L Magnanti, and J B Orlin. Network Flows: Theory, Algorithms, and Applications.
Prentice Hall, Englewood Cliffs, NJ, 1st edition, 1993.

Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. arXiv
preprint arXiv:1707.01495, 2017.

Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223. PMLR, 2017.

Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. Advances in neural information pro-
cessing systems, 29, 2016.

Koby Bibas, Yaniv Fogel, and Meir Feder. Deep pnml: Predictive normalized maximum likelihood
for deep neural networks. arXiv preprint arXiv: 1904. 12286, 2019.

Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. arXiv preprint arXiv:1810.12894, 2018.

Robert Dadashi, Léonard Hussenot, Matthieu Geist, and Olivier Pietquin. Primal wasserstein imita-
tion learning. arXiv preprint arXiv:2006.04678, 2020.

Ishan Durugkar, Steven Hansen, Stephen Spencer, and Volodymyr Mnih. Wasserstein distance max-
imizing intrinsic control. arXiv preprint arXiv:2110.15331, 2021a.

Ishan Durugkar, Mauricio Tec, Scott Niekum, and Peter Stone. Adversarial intrinsic motivation
for reinforcement learning. Advances in Neural Information Processing Systems, 34:8622-8636,
2021b.

Ben Eysenbach, Sergey Levine, and Russ R Salakhutdinov. Replacing rewards with examples:
Example-based policy search via recursive classification. Advances in Neural Information Pro-
cessing Systems, 34:11541-11552, 2021.

Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. arXiv preprint arXiv: 1802.06070, 2018.

Arnaud Fickinger, Samuel Cohen, Stuart Russell, and Brandon Amos. Cross-domain imitation
learning via optimal transport. arXiv preprint arXiv:2110.03684, 2021.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, pp. 1126-1135. PMLR,
2017.

Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for
reinforcement learning agents. In International conference on machine learning, pp. 1515-1528.
PMLR, 2018.

Yaniv Fogel and Meir Feder. Universal supervised learning for individual data. arXiv preprint
arXiv: 1812.09520, 2018.

Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse control with

events: A general framework for data-driven reward definition. Advances in neural information
processing systems, 31, 2018.

10
Published as a conference paper at ICLR 2023

Dibya Ghosh, Abhishek Gupta, and Sergey Levine. Learning actionable representations with goal-
conditioned policies. arXiv preprint arXiv:1811.07819, 2018.

Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv: 1406.2661, 2014.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861-1870. PMLR, 2018.

Siddhant Haldar, Vaibhav Mathur, Denis Yarats, and Lerrel Pinto. Watch and match: Supercharging
imitation with regularized optimal transport. arXiv preprint arXiv:2206.15469, 2022.

Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey Levine. Dynamical distance
learning for semi-supervised and unsupervised skill discovery. arXiv preprint arXiv: 1907.08225,
2019.

Peide Huang, Mengdi Xu, Jiacheng Zhu, Laixi Shi, Fei Fang, and Ding Zhao. Curriculum re-
inforcement learning using optimal transport via gradual domain adaptation. arXiv preprint
arXiv:2210.10195, 2022.

Mingi Jiang, Edward Grefenstette, and Tim Rocktischel. Prioritized level replay. In Jnternational
Conference on Machine Learning, pp. 4940-4950. PMLR, 2021.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv _ preprint
arXiv:1312.6114, 2013.

Pascal Klink, Haoyi Yang, Carlo D’Eramo, Jan Peters, and Joni Pajarinen. Curriculum reinforce-
ment learning via constrained optimal transport. In International Conference on Machine Learn-
ing, pp. 11341-11358. PMLR, 2022.

Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and Pieter Abbeel. Cic:
Contrastive intrinsic control for unsupervised skill discovery. arXiv preprint arXiv:2202.00161,
2022.

Kevin Li, Abhishek Gupta, Ashwin Reddy, Vitchyr H Pong, Aurick Zhou, Justin Yu, and Sergey
Levine. Mural: Meta-learning uncertainty-aware rewards for outcome-driven reinforcement
learning. In Jnternational Conference on Machine Learning, pp. 6346-6356. PMLR, 2021.

Hao Liu and Pieter Abbeel. Aps: Active pretraining with successor features. In Jnternational
Conference on Machine Learning, pp. 6736-6747. PMLR, 2021a.

Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. Advances in
Neural Information Processing Systems, 34:18459-18473, 2021b.

Georg Ostrovski, Marc G Bellemare, Adron Oord, and Rémi Munos. Count-based exploration with
neural density models. In International conference on machine learning, pp. 2721-2730. PMLR,
2017.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. 0 vai F. d'Alché-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Proces ‘ems 32, Be.

8024-8035. Curran Associates, Inc., 2019. URL/http://papers.neurips.cc/paper/|

9015-pytorch-an-imperative-style-high nce = earning— A brary.
pdf

Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International conference on machine learning, pp. 2778-2787.
PMLR, 2017.

11
Published as a conference paper at ICLR 2023

Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.
In International conference on machine learning, pp. 5062-5071. PMLR, 2019.

Vitchyr H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-
fit: State-covering self-supervised reinforcement learning. arXiv preprint arXiv: 1903.03698,
2019.

Rémy Portelas, Cédric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for
curriculum learning of deep rl in continuously parameterized environments. In Conference on
Robot Learning, pp. 835-853. PMLR, 2020.

Zhizhou Ren, Kefan Dong, Yuan Zhou, Qiang Liu, and Jian Peng. Exploration via hindsight goal
generation. Advances in Neural Information Processing Systems, 32, 2019.

Jorma Rissanen and Teemu Roos. Conditional nml universal models. In 2007 Information Theory
and Applications Workshop, pp. 337-341. IEEE.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897. PMLR,
2015.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv: 1707.06347, 2017.

Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware
unsupervised discovery of skills. arXiv preprint arXiv: 1907.01657, 2019.

Archit Sharma, Kelvin Xu, Nikhil Sardana, Abhishek Gupta, Karol Hausman, Sergey Levine, and
Chelsea Finn. Autonomous reinforcement learning: Benchmarking and formalism. arXiv preprint
arXiv:2112.09605, 2021.

Avi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, and Sergey Levine. End-to-end robotic
reinforcement learning without reward engineering. arXiv preprint arXiv: 1904.07854, 2019.

Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob
Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv preprint
arXiv: 1703.05407, 2017.

Cédric Villani. Optimal transport: old and new, volume 338. Springer, 2009.

David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and
Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. arXiv
preprint arXiv: 1811.11359, 2018.

Huang Xiao, Michael Herman, Joerg Wagner, Sebastian Ziesche, Jalal Etesami, and Thai Hong
Linh. Wasserstein adversarial imitation learning. arXiv preprint arXiv: 1906.08113, 2019.

Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with pro-
totypical representations. In International Conference on Machine Learning, pp. 11920-11931.
PMLR, 2021.

Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey
Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
In Conference on Robot Learning, pp. 1094-1100. PMLR, 2020.

Yunzhi Zhang, Pieter Abbeel, and Lerrel Pinto. Automatic curriculum learning through value dis-
agreement. Advances in Neural Information Processing Systems, 33:7648-7659, 2020.

Rui Zhao, Yang Gao, Pieter Abbeel, Volker Tresp, and Wei Xu. Mutual information state intrinsic
control. arXiv preprint arXiv:2103.08107, 2021.

Aurick Zhou and Sergey Levine. Amortized conditional normalized maximum likelihood: Reliable
out of distribution uncertainty estimation. In International Conference on Machine Learning, pp.
12803-12812. PMLR, 2021.

12
Published as a conference paper at ICLR 2023

A TRAINING & EXPERIMENTS DETAILS

A.1 TRAINING DETAILS

Baselines. The baseline curriculum RL algorithms are trained as follows,

* HGG : We follow the default setting in the original implementation from
https://github.com/Stilwell-Git/Hindsight-Goal-Generation

* CURROT (Klink et al.|/2022): We follow the default setting in the original implementation
fromhttps://github.com/psclkink/currot
* GoalGAN (Florensa et al.| |2018), PLR (Jiang et al. 2021), VDS (Zhang et al.| 2020),

ALP-GMM 2020) : We follow the default setting in implementation from
https://github.com/psclkink/currot

* SkewFit (Pong et al.||2019): We follow the state-based version of SkewFit. The original

implementation was modified and used since only the image-based version is provided in

it. (hnttps://github. com/rail-berkeley/rlkit)
All the baselines are trained by SAC (Haarnoja et al.||2018) with the sparse reward except for the

SkewFit as it uses a reward based on the conditional entropy. Even though some algorithms’ original
implementation is based on on-policy algorithms such as TRPO or PPO
2017), for comparing the sample efficiency, we replace the on-policy algorithm with the off-policy

algorithm, SAC, following the referred implementation.
Table 1: Conceptual comparison between our work and the previous curriculum RL algorithms.
Uncert. Timestep Target Off- Curriculum Without Without non-
-Aware -Aware __curriculumdist. policy proposal _ext. reward _ forgetting mechanism
HGG x x Gg v B x v
GoalGAN x x x x GAN x x
CURROT x x U or GT v Uu x x
PLR x x x x B x x
VDS v x x v B x v
ALP-GMM x x x v GMM x x
SkewFit vo x x v VAE v x
OUTPACE (ours) vo v G vo B vo vo

We included a conceptual comparison between our work and previous other curriculum or goal
generation methods in Table[I]

¢ Uncert.-Aware: whether the curriculum goal proposal process is aware of the uncertainty
of the candidate goals.

¢ Timestep-Aware: whether the curriculum goal proposal process is aware of the temporal
distance from the initial states or from the desired outcome states.

¢ Target curriculum dist: whether there exists a mechanism for the curriculum goals to
converge to the target distribution. When there is no target distribution (e.g. just exploring
or expanding the curriculum goal distribution as diverse as possible.), we denoted it as X.

¢ Off-policy: whether the off-policy RL can be applied. Some baselines need to measure a
kind of difficulty, which means they require repeated trials and on-policy RL algorithms

with multi-processing such as TRPO, and PPO (Schulman et al. (2017).

¢ Curriculum proposal: where the curriculum goals are proposed from.

¢ Without ext. reward: whether the algorithm requires external environmental reward or
not.

¢ Without non-forgetting mechanism: whether the algorithm requires implicit or explicit
non-forgetting mechanisms. Some baselines mix the previously practiced curriculum goals
with a fixed or varying ratio or make the curriculum distribution into uniform over the state
space to cover all possible test goal states.

13
Published as a conference paper at ICLR 2023

Training details. To train the potential function fy via Eq. (5), s and s, in the buffer should ideally
contain all feasible states in the environment. However, until the policy is learned enough to explore
the map, obtaining such ideal distribution is difficult. To mitigate this issue, following [Durugkar|
jet al.] (20216), we approximate such a distribution with a small replay buffer By containing recent
trajectories and the relabelling technique 2017). While this approximation

does not provide f, with the ideal state distribution covering all feasible states, we empirically

found that this assumption works well since OUTPACE only queries fy for the explored area when
it generates curriculum goals.
Table 2: Hyperparameters for OUTPACE
Critic hidden dim 512 discount factor 7 0.99
Critic hidden depth 3 fs update frequency 1000
Critic target 7 0.01 # of gradient steps for f update 10
Critic target update frequency 2 # of ensemble networks for f, 5
Actor hidden dim 512 learning rate for fy le-4
Actor hidden depth 3 RL optimizer adam
Actor update frequency 2 Meta-learner network hidden size | [2048,2048]
RL batch size 512 Meta-learner train sample size 512
Init temperature a;,;, of SAC | 0.3 Meta-learner test sample size 2048
Replay buffer B size 3e6 Meta-learner test batch size 2048

A.2

Table 3: Env-specific hyperparameters for OUTPACE

Env name By size Afor fy adameforSAC  L
Point-U-Maze-v0 10000 25 le-2 0.02
Point-N-Maze-v0 10000 25 le-2 0.02

Point-Spiral-Maze-vO 20000 50 le-8 0.02

Ant Locomotion-v0 50000 25 le-8 0.02
Sawyer-Peg-Push 30000 25 le-2 2

Sawyer-Peg-Pick&Place | 30000 25 le-2 0.02

ENVIRONMENT DETAILS

* Point-U-Maze : The observation consists of the xy position, angle, velocity, and angular
velocity of the ‘point’. The action space consists of the velocity and angular velocity of the
‘point’. The initial state of the agent is [0,0] and the desired outcome states are obtained
by adding uniform noise to the default goal point [0, 8]. The size of the map is 12 x 12.

¢ Point-N-Maze : It is the same as the Point-U-Maze environment except that the desired
outcome states are obtained by adding uniform noise to the default goal point [8, 16], and
the size of the map is 12 x 20.

¢ Point-Spiral-Maze : It is the same as the Point-U-Maze environment except that the desired
outcome states are obtained by adding uniform noise to the default goal point [8, —8], and
the size of the map is 20 x 20.

¢ Ant Locomotion : The observation consists of the xyz position, xyz velocity, joint angle,
and joint angular velocity of the ‘ant’. The action space consists of the torque applied on
the rotor of the ‘ant’. The initial state of the agent is [0,0] and the desired outcome states
are obtained by adding uniform noise to the default goal point [0, 8]. The size of the map is
12x 12.

¢ Sawyer-Peg-Push : The observation consists of the xyz position of the end-effector, the
object, and the gripper’s state. The action space consists of the xyz position of the end-
effector and gripper open/close control. The initial state of the object is [0.4, 0.8, 0.02] and

the desired outcome states are obtained by adding uniform noise to the default goal point
[-0.3, 0.4, 0.02]. We referred to the metaworld 2020) and EARL (Sharma et al.
2021) environments.

14
Published as a conference paper at ICLR 2023

eo | Oi

(a) U-Maze (b) N-Maze (c) Spiral-Maze (d) Ant Locomotion (e) Sawyer Manip.

Figure 7: Environments used for evaluation: (a)-(c) the agent must navigate various kinds of maze
environments. (d) the quadruped ant must navigate the maze to a particular location. (e) the robot
has to push or pick&place a peg to the desired location.

¢ Sawyer-Peg-Pick&Place : It is the same as the Sawyer-Peg-Push environment except that
the desired outcome states are obtained by adding uniform noise to the default goal point
[-0.3, 0.4, 0.2].

B ALGORITHM & DERIVATIONS

B.1 ALGORITHM

Algorithm 1 Overview of OUTPACE algorithm

1: Input: desired outcome examples G*, total training episodes N, Env, environment horizon H,
actor network 7, critic network Q, potential function network f%, replay buffer B, By

2: for iteration=1,2,...,N do

3: G* + sample K curriculum goals that minimize

DV weplCé(Ponmi(e = 1/8"); y = ponmi(e = 1G+)) — L- f2(s°)] (Section|4.3}

4: for 7=1,2,...,.K do
5: Env.reset()
6: g < G°.pop
7: for t=0,1,...,H-1 do
8: if achieved g then
9: g <- random goal (randomly sample a state with high uncertainty measured by ponmL
ina ball B,(s,).)
10: end if
lI: a, — 7(-|82,9)
12: $t41 < Env.step(az)
13: end for
14: B+ BU {80, a0, 81...}, By — By U {80, a0, 81...}
15: end for
16: for i=0,1,....M do
17: Sample a minibatch b from B and label reward using {7 (sz) (Section
18: Train 7 and Q with b via SAC (Haarnoja et al.|{2018).
19: Meta-train pon With b via meta-NML (Algorithm [2])
20: Sample a minibatch by from By
21: Train f% with by via Eq.
22: end for
23: end for

15
Published as a conference paper at ICLR 2023

Training Goal Sampling Modules. Sampling Curriculum Goals

y

Augment data
Du (sie; € {0,1})

Looe to.. '. 1} Calculate Calculate
Uncertainty || Temp. Dist.

‘

Sample K curriculum
\

PcnmL

v

MAML
(Fin et al., 2017)

| goals that minimize Eq.11

emporal Distance-Aware Module

Train fg "| Off-Policy RL
by minimizing Ly (Eq.5)
T
—— Replay Data —-+—> Neural Network Model » Single step adaptation

Figure 8: The overall diagram of OUTPACE

Algorithm 2 Meta-NML

Input: desired outcome examples G+, RL buffer B

Sample G ~ from RL buffer B

D «<G- UG* (Let the size of D is n)

Create 2n meta-training tasks by relabelling each data points in D as 0 and 1.
(DU (ai, y’) where x; ~ D and y’ € {0,1})

5: Meta-learn with 2n meta-training tasks (Finn et al

(Let 6 be the parameter of pcnmi and £ be standard classification loss.

ming Ex,~D,y'~{0,1}L(D U (xi, y’), 62.)

st. = 0 —aVoL(DU (ai, yi), 8)

B.2 DERIVATIONS

This section contains the definition of time-step metric d”, derivation of Lipschitz smoothness of
f, and derivation of Eq. (Up. Regarding d* and Lipschitz smoothness of f, we refer the reader to

Durugkar et al. for more detailed explanation.

B.2.1  TIME-STEP METRIC d*, LIPSCHITZ SMOOTHNESS OF f

Definition B.1. Given a state space S, action space A, transition dynamics T : S x A — S, and
agent policy 7, the time-step metric d” is a quasi-metric where the distance from s € S to 8, € S is
the expected number of transitions required with the policy 7.

The time-step metric can be expressed by the expectation of the number of transitions taken under
the policy 7, where T”(-|s, sg) is the probability distribution of the timestep required to go from s
to sy. d™ can also be written recursively as

d" (8, 8g): = ExsaT*(.\s,s4) 7]
0 ifs = sy (12)
1+ Equa(.|s,sy)Es’27(-\s,a) [47 (8’, 89)] otherwise.

16
Published as a conference paper at ICLR 2023

Lipschitz smoothness of f If the difference in values of f on the expected transition from every
state is bounded by 1, and the policy 7 can reach the goal s, within a finite number of transitions,
then f is the 1-Lipschitz function.

Proof. We can write |f (sg) — f(so)| via telescopic sum,

r-1
|f(89) — f(80)| = Ex,r27*(-\s0.84) ibs f(st41) — v0)
t=0
a (13)
< Ex. (-\50,89) |f(st41) — Fe
L 1=0
Since E[| f(s’) — f(s) |] < 1 by Eq @). we can write
rl
Ext AT (150559) b |f(se+41) - ste] S Ex,r0T*(-|s0,84) | 1
_ '=0 (14)
= Ex,r0T*(-|50.89) 7]

= d"(s0, 8g)

Thus, f is the 1-Lipschitz function with respect to the time-step metric d”.

B.2.2 DERIVATION OF EQUATION

By substituting Eq. (7) into Eq (>), and omitting {3 (sq) which is not related to G°, we obtain the
following terms,

K
max log {1 — |penmi(e = 0|s") — perm (e = 1s"

oom [los {2 — | ( \s’) ( Is')| (15)

+ ¢(ponmi(e = 1\s") — 0.5) - U(ponmi(e = 1s") 5)} +L f5( (s‘)], siege.

Also, if we use the default value of the hyperparameters c = 4, we can express the above terms as
follows,

K
max log {1 — |1 — 2pcenm(e = 1 Is"
Ge:|Ge\= ou! {1 | dt (16)
A(ponmi(e = 1\s°) — 0.5) - 1(ponmn(e = 1|s") > 0.5)} +L- f5(s')], siege
which can be simplified as
K
min SI log(penmi(e = 1\s")) —L- fZ(s')], s' eG (17)

Ge:|Ge|=K

If we could assume that pcnmr is well trained to classify the desired outcome examples gi. €
Gy from the states s in the already explored region, ponmi(e = 1|9'.) is approximately equal

to 1 (penmi(e = 1| g'.) = 1). Then, the terms inside the above minimization objective can be
approximately represented as

—ponmi(e = 19',) log(ponma(e = 1[s")) — L- fs (s'). (18)

17
Published as a conference paper at ICLR 2023

Then, in practical implementation, we can implement the above terms by cross-entropy loss as

w(s', 9’) = CE(ponmi(e = 1[s");y = ponmu(e = 19',.)) — L- f3(s'), (19)

which is Eq (Up. Even though it is not exactly equivalent to the mathematical definition of the
cross-entropy, we can just implement it with cross-entropy loss developed in standard deep learning
framework such as PyTorch (Paszke et al.||2019) because choosing s’ to maximize log(ponm(e =
1|s‘)) and choosing s‘ close to g', in order to be classified as desired outcome examples by the
PCNML (predicted labels of s‘ to be close to 1) have the same intuitive meaning.

B.3. A DETAILED DESCRIPTION OF META-NML

Conditional normalized maximum likelihood (CNML) can perform a conservative k-way classi-

fication based on previously seen data 2021} [Zhou & Levine} (2021). Let Dirain =

{(zi, yt be a set of data containing pairs of inputs %1.,—1 and labels yin—1 € {1,--- ,k},
where k is the number of possible labels, and © is a set of models. Given a new input x,,, CNML
defines the distribution pono (Yn = itn ie, sk} by minimizing the regret R of the worst-case
label y,, as

PCNML = arg min max RQ, @1:n, Yin, 9), (20)
4 ”

where we define a regret R for label y,, of a distribution g and maximum likelihood estimator 6’ as

RQ, Lins Yins 9) = lOSgr(y, Jerry) Yan |2ien) — log g(Yt:n)- (21)

By solving Eq. (20), CNML predicts the distribution of the new label y,, as Eq. (Bibas et al]

Pao (y = Man)

ra
jai Por (y=j
J

PONML(Yn = M, Ln) (22)

Y°

where a model oi”) is a model that represents the augmented dataset D = Dyrain U (@n, Yn = M)
well. Thus, the number of total MLE models required is n x k since we should address each data

by augmenting with the label 1, ..., k, respectively Ofer”). Since our algorithm utilizes 2-way

classification (k = 2), we define 2n tasks 7;_,,,, and 7;},,,, Which are constructed by augmenting

negative (z;, y = 0) and positive (x;, y = 1) labels respectively for each data point (x;).

To amortize the training cost of the tasks (tasks Tatn) we can apply the meta-learning algorithm

to this setting and train a model 6 which can quickly adapt to the
optimal solution after a single step of gradient update with standard classification loss £ as

minE,,.py~{o1}L(D U (21, 9"), jy) (23)
8.t.6 =60-—aVol(DU (xi, yi), 9), (24)

where Eq. and Eq represent the objective of meta-learning and quick adaptation respec-
tively. Training CNML via meta-learning and leveraging CNML are shown in lines 3 and 19 of the
algorithm overview (Algorithm|[I). Also, we provide the pseudo-code of meta-nml in Algorithm[2}

C MORE EXPERIMENTAL RESULTS

C.1 FULL RESULTS OF THE MAIN SCRIPT

We included the full results of the main script in this section. The uncertainty quantification is
visualized in Figure [| the visualization of the trained f3(s) is in Figure [10] the visualization of

18
Published as a conference paper at ICLR 2023

the proposed curriculum goals is in Figure[I]] We do not include the visualization results of the
Point-U-Maze, and Sawyer-Peg-Pick&Place results as these environments share the same map with
the Ant Locomotion, and Sawyer-Peg-Push environments, respectively.

Desired
goal
Uncertain
state
Explored
state
Desired
goal
Uncertain
state
Explored
state
Desired
goal
Uncertain
state
 ] \ ‘ Explored
‘ state

Figure 9: Visualization of the uncertainty quantification along training progress. First row: U-Maze
(Point, Ant Locomotion), Second row: N-Maze, Third row: Spiral-Maze, Fourth row: Sawyer Peg
Push, Pick & Place environments.

Desired
goat

Uncertain
state

Explored
state

High
O reward

Initial Point

(a) Ant Locomotion (b) N-Maze (c) Spiral-Maze (d) Sawyer Push

Figure 10: Visualization of the trained f5(s . High reward means temporally close to the desired
outcome states (required timesteps are small to reach the goal), and low reward means the opposite
(required timesteps are large to reach the goal).

19
Published as a conference paper at ICLR 2023

Initial

Meo om man curriculum

Final
curriculum

Initial
curriculum

Final
curriculum

Initial
curriculum

Final
curriculum

Initial
curriculum

Final
curriculum

(a) OUTPACE(ours) (b) CURROT (c) HGG
Figure 11: Visualization of the proposed curriculum goals. First row: Ant Locomotion (same
map size with U-Maze), Second row: Point-N-Maze, Third row: Point-Spiral-Maze, Fourth row:
Sawyer Peg Push.

20
Published as a conference paper at ICLR 2023

C.2 EVALUATION WITH THE GOALS SAMPLED FROM THE UNIFORM DISTRIBUTION

In some curriculum RL algorithms, they incorporate a mechanism for remembering previously prac-

ticed curricula either implicitly or explicitly. For example, GoalGAN (Florensa et al.|/2018) mixes
the previously generated goals with currently generated goals in a specified ratio (e.g. 20 % of pre-

201

viously used goals), and SkewFit set the objective as targeting the uniform goal
distribution (by maximizing the entropy of the goals H(g)), and CURROT also
utilizes uniform target curriculum distribution in practice, and so do some of the other baselines.
Due to these algorithmic designs, they require many iterations for explicitly practicing previously
used curriculum goals, or show slow progress of curriculum to match the uniform target distribution.

In contrast, our method is based on an intrinsic reward, which is shaped according to the required
timesteps proportional values f3(s) as described in our main script. Thus, our method does not
need to explicitly consider the non-forgetting mechanism when we design the algorithm because the
reward is already shaped with respect to the timesteps for reaching the arbitrary goal points along
the trajectory that reaches the desired outcome state. Because of this property, our method does not
consider uniform target distribution or explicitly mixing the previously practiced curriculum goals,
and it enables our method to be much faster and show sample-efficient curriculum progress.

We experimented with the goals sampled from the uniform distribution on the feasible state space for
validating the previous hypothesis, and the experimental results are shown in Figure{12] Even though
our method does not explicitly consider the previously practiced curriculum goals, it shows success
in reaching arbitrary goal points sampled from the uniform distribution. Performance degrades are
observed in sawyer manipulation environments and these are because we set the uniform distribution
as areas within the tables in the environment. But the curriculum goals proposed by our method are
converged before the agent explores the entire state space on the table, thus the agent does not have
the opportunity to practice the goals from the entire state space.

success _rate

400 600 800 1000
steps (k)

success _rate

steps (k)

success _rate

° 200

steps (k)

1000

(a) U-Maze (b) N-Maze (c) Spiral-Maze
10 10 10
os os os
Bos Bos Bos
——_—— = _
oo = oo
‘0500 «10001300 000-2500 «3000 300 1000" 15002000 <2500—~3000 ‘50010001500 200025003000
steps (k) steps (k) steps (k)
(d) Ant Locomotion (e) Sawyer Push (f) Sawyer Pick&Place
™ OUTPACE(ours) m@  Skewrit = CURROT = PLR
= HGG = ALP-GMM  @ GoalGAN  ™ = VDS.

Figure 12: Episode success rates of the evaluation results with the goals uniformly sampled from
the feasible state space.

C.3. ABLATION STUDY

We conducted ablation studies described in our main script in all environments. Figure|13|shows
the average distance from the proposed curriculum goals to the desired final goal states along the
training steps, and Figure [14] shows the episode success rates along the training steps. As we can
see in these figures, even though there are not many differences in some environments with simple

21
Published as a conference paper at ICLR 2023

dynamics, we could obtain consistent analysis with the results in the main script in most of the
environments.

We also visualized the curriculum goals obtained by each ablation study as training proceeds in Fig-
ure[T5] As we can see in these figures, the curriculum proposal only based on the uncertainty (only-
enml) shows progress toward uncertain areas, but it shows unstable curriculum progress, which is
depicted by separated curriculum goals despite the same colors or out of order with respect to human
intuitive optimal curriculum progress. This is because the meta-learning-based inference procedure
of pont has some numerical errors that could lead to the wrong prediction of the states near the
boundaries of the already explored regions as uncertain areas (For example, in Figure[9] there exist
some states predicted as uncertain area despite already explored region).

Also, even though the curriculum proposal based on the temporal distance only obtained by f7
(only-f) shows some progress that deviates from the initial states, it still has difficulty in most of
the environments because f7 is trained to reflect the observed transition data rather than entire state
space. That is, the temporal distance estimation is reliable within the explored region. Once the
trained f has local optimum before discovering temporally more distant states (Figure
proposed ‘curriculum goals can be stuck in some areas that are wrongly predicted to be the most far
from the initial states in terms of the temporal distance, and it leads to the ineffective exploration of
the agent and recurrent failures.

22
Published as a conference paper at ICLR 2023

— ouTPAce(ours)
— Ablation only f —_
— Ablation only enml

distance

ED ‘OUTPACE(ours)
ws — Ablation_only f

— Ablation only enmi

— ouTPAce(ours)
— Ablation_only f
Ablation_only_enm|

— ouTPace(ours)
Ablation_only_f
Ablatian_only_cnm|

distance

— ourPace(ours)
— Ablation_only f
Ablation_only_enml

distance

— ourPace(ours)
08 — Ablation_only f
Ablation_only_enm|

0 5001000

1500
steps (k)

(a) Cost type

20002500 3000

2 — oUTPACE(ours)
— Ablation SparseReward
10 — Ablation GAN

ge
Be
4
2
o, aan ann enn oo nnn
vs — ourpacetours)
so — Ablation SparseReward
— Ablation_Gan
bs
g
E100
Bos
so
as
vo — ourpacetours)
— Ablation SparseReward
— Ablation_Gan 4
1s
g
5
B.
5
°
7 —— OUTPACE(ours)
wo — lation SparseReward
— blation-GAN
Ce Ad
£ 6
4
2
- — ouracetours)
— Ablation SparseReward
2 — Abation-Gan
aio
B08
Sos
os
o2
os — oureacetours)
on —— Ablation SparseReward
— Ablation_Gan
os
2
Bos
8
Boa
oa
o2
oa
0 50010003500 200025003000
‘steps (k)

(b) Reward & Goal proposition type

— ourPace{ours)
1
2

distance

OUTPACE(ours)

— ourPacetours)
cl
— 2
— cn

ge
8
ge
4
2
— ouTPACE{ours)
07
=
06 —
=
gos
Boa
os
02
01
— ouTPACE{ours)
or —
os td
=
Sos
8
Boa
03
02
oa
© 500 1000-1500 2000 ©2500-3000,
steps (k)

(c) Effect of ¢ in Nguidance

Figure 13: Ablation study in terms of the distance from the proposed curriculum goals to the desired
final goal states. First row: Point-U-Maze. Second row: Point-N-Maze. Third row: Point-Spiral-
Maze. Fourth row: Ant Locomotion. Fifth row: Sawyer Push. Sixth row: Sawyer Pick&Place.
Shading indicates a standard deviation across 5 seeds.

23
Published as a conference paper at ICLR 2023

10 10 10
— ouTPAcE(ours) — onTPAce(ours) — ouTPack(ours
pation only f — bation SparseReward =

oe — Ablation only cnmt oe — Ablation GAN oe =

g g g =

Bos Soe Soe

8 8 8

Soa Soa Soa

a a a

oa oa oa

oo oo oo

a a eT) a a eT) a a eT)
‘steps (k) ‘steps (k) ‘steps (k)

10 10 10
— ‘ouTPAcE(ours) — ourPace(ours) COUTPACE(ours)
lation only f — bation SparseReward

oe — Ablation only nm! oe — Ablation GAN oe

g g g

Bos Soe Soe

8 8 8

Soa Soa Soa

a a a

oa oa oa

oo oo oo

20000 goo.Ssan0.SS—«0 re a a eT) re a a eT)
‘steps (k) ‘steps (k) ‘steps (k)

10 10 10
— ourpace(ours) ‘OUTPACE(ours) COUTPACE(ours)
pation only £ — lation SparseReward 1

oe — Ablation only nm! oe — Ablation GAN oe

g g g

Bos 8 Bos

8 8 8

Soa g Soa

a a a

oa oa

oo oo

200 ~~ ~—~—«aoo~S~SC«OOS« a rr eT) e700 «aan cooSmeo.S3000
‘steps (k) ‘steps (k) ‘steps (k)

10 — 10 10
> oiTpacetours) —OrTPACE(ours)
Ablation only £ — Ablation SparseReward

oe — Ablation only_cnmt oe — Ablation GAN oe

g g g

Bos Soe Soe

8 8 8

Soa Soa Soa

a a a

oa oa oa

oo oo oo

@ 30010601500 200025003000 @ 30010601500 200025003000 e300 1060 1s00 2000 25003000
‘steps (k) ‘steps (k) ‘steps (k)

10 — 10 ——
= oureacciours) | SOTA |
— Ablation_onlyf —— Ablation SparseReward

oe — Ablation only_enmt oe — Ablation GAN

g g g

Boe Boe — is

8 8 8

Soe gos g

a a a

oa oa

oo oo

@ $00 1000 1s00 2000 25003000 ‘@ S00 10001500 200025003000 eso 1000 1s00 2000 25003000
‘steps (k) ‘steps (k) ‘steps (k)

10 10 10 —
— oureace(ours) — oureace(oirs) — ovTPacz\ours)
pation only £ — bation SparseReward el

oe — Ablation only_cnmt oe — Ablation GAN oe =

g g g =

Bos Soe Soe

8 8 8

Soa Soa Soa

a a a

oa oa oa

oo oo oo

0500 1000 1500 2000 +2500 3000
steps (k)

(a) Cost type

0500 10001500 2000 +2500 —=3000
steps (k)

(b) Reward & Goal proposition type

05001000 1500. © 200025003000
steps (k)

(c) Effect of ¢ in Nguidance

Figure 14: Ablation study in terms of the episode success rates. First row: Point-U-Maze. Second

row: Point-N-Maze. Third row: Point-Spiral-Maze. Fourth row: Ant Locomotion. Fifth ro

Sawyer Push. Sixth row: Sawyer Pick&Place. Shading indicates a standard deviation across 5

seeds.

24
Published as a conference paper at ICLR 2023

Initial
curriculum

eT

‘

Final
curriculum

Initial
curriculum

Final
curriculum

Initial
curriculum

Final
curriculum

x ae
Initial
curriculum

| Final
curriculum

(a) OUTPACE(ours) (b) Ablation: only-cnml (c) Ablation: only-f

Figure 15: Ablation study in terms of curriculum goals visualization. First row: Ant Locomotion,
Second row: Point-N-Maze, Third row: Point-Spiral-Maze, Fourth row: Sawyer Manipulation.

Temporally
far
i Poi
oint
itial Point Temporally

close
(a) Ant Locomotion (b) N-Maze (c) Spiral-Maze (d) Sawyer Push

Figure 16: Visualization of the not properly trained {3 (s) when ablation study only-f is performed.

25
