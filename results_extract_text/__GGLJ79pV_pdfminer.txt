Under review as a conference paper at ICLR 2023

GU A R DHFL: PRIVACY GUARDIAN FOR HETEROGE-
NEOUS FEDERATED LEARNING

Anonymous authors
Paper under double-blind review

ABSTRACT

Heterogeneous federated learning (HFL) enables clients with different computa-
tion and communication capabilities to collaboratively train their own customized
models via a query-response paradigm on auxiliary datasets. However, such
paradigm raises serious privacy issues due to the leakage of highly sensitive query
samples and response predictions. Although existing secure querying solutions
may be extended to enhance the privacy of HFL with non-trivial adaptation, they
suffer from two key limitations: (1) lacking customized protocol designs and (2)
relying on heavy cryptographic primitives, which could lead to poor performance.
In this work, we put forth GuardHFL, the first-of-its-kind efficient and privacy-
preserving HFL framework. GuardHFL is equipped with a novel HFL-friendly
secure querying scheme that is built on lightweight secret sharing and symmetric-
key techniques.
Its core is a set of customized multiplication and comparison
protocols, which substantially boost the execution efficiency. Extensive evalua-
tions demonstrate that GuardHFL outperforms the state-of-the-art works in both
runtime and communication overhead.

1

INTRODUCTION

As a promising variant of federated learning (FL), heterogeneous federated learning (HFL) (Li &
Wang, 2019) enables clients equipped with different computation and communication capabilities
to collaboratively train their own customized models that may differ in size, numerical precision or
structure (Lin et al., 2020). In particular, the knowledge of models is shared via a query-response
paradigm on auxiliary datasets, such as unlabeled datasets from the same task domain (Choquette-
Choo et al., 2021) or related datasets from different task domains (Li & Wang, 2019; Lin et al.,
2020). In such a paradigm, each client queries others with samples in the auxiliary querying dataset,
and obtains aggregated response predictions via a centralized cloud server1. Then he retrains his
local model on the query data and corresponding predictions. This flexible approach facilitates
customized FL-driven services in areas like healthcare and finance (Kairouz et al., 2019), while
resolving the intellectual property concerns of FL models (Tekgul et al., 2021).

However, HFL suffers from several privacy issues. First, directly sharing query samples violates
their privacy. For example, in healthcare applications, the auxiliary datasets may contain patients’
medical conditions. Disclosure of such information is illegal under current regulations like General
Data Protection Regulation. Second, sharing response predictions may still compromise the privacy
of local data (Papernot et al., 2016). Several works have shown that given black-box access to a
model, adversaries can infer the membership (Salem et al., 2019) and attribute information (Ganju
et al., 2018) of the target sample or even reconstruct the original training data (Yang et al., 2019).

Although in traditional FL systems, the privacy issue could be mitigated through well-studied secure
gradient aggregation protocols (Bell et al., 2020), it becomes more challenging to realize this guar-
antee in HFL, due to the heterogeneity of the clients’ models (refer to Appendix A.2.3). To bridge
this gap, a possible solution is to structurally integrate into HFL existing secure querying (a.k.a.
private inference) schemes (Rathee et al., 2020; Huang et al., 2022; Wagh et al., 2019; Tan et al.,
2021). These schemes utilize various cryptographic primitives, including homomorphic encryption

1As demonstrated in Bonawitz et al. (2017); Bell et al. (2020), clients (e.g., mobile devices) in real-world

applications are generally widely distributed and coordinated only by the server.

1

Under review as a conference paper at ICLR 2023

(HE) (Gentry, 2009), garbled circuit (GC) (Yao, 1986) or oblivious transfer (OT) (Asharov et al.,
2013), to provide rigorous privacy guarantees for query data and prediction results. While it is possi-
ble to non-trivially extend these secure querying schemes (refer to Section 2.3), they have two major
limitations: (1) the underlying protocols are not customized for HFL; (2) they incur huge overhead
due to the adoption of heavy cryptographic primitives. These bottlenecks lead to poor performance
and hinder the efficient instantiation of HFL. Therefore, it is necessary but challenging to provide
customized protocols and implement a privacy-preserving HFL with desirable performance.

In this work, we introduce GuardHFL, the first efficient and privacy-preserving HFL framework to
address the above challenges2. GuardHFL is built upon the standard HFL training paradigm (Li
& Wang, 2019), which contains three stages: local training, querying and local re-training (refer to
Section 2.1). We formalize the workflow of HFL, and present a novel HFL-friendly secure querying
scheme as an important building block. The core of our scheme is a set of customized multiplication
and comparison protocols, which substantially boost the execution efficiency compared to existing
works. More precisely, we optimize the parallel prefix adder (PPA) logic (Harris, 2003) to build a
more advanced comparison protocol. Over an ℓ-bit ring, these optimizations reduce the evaluation
of log ℓ AND gates and also the number of the communication rounds, which are two crucial factors
that dominate the performance. Moreover, our PRF-based multiplication protocol only communi-
cates 3 elements in an ℓ-bit ring, achieving significant improvement compared to the widely used
Beaver’s multiplication solution (Beaver, 1991; Demmler et al., 2015). We provide formal privacy
guarantees for the designed protocols, and evaluate GuardHFL on different datasets (SVHN, CI-
FAR10, Tiny ImageNet), system configurations (IID and Non-IID training sets) and heterogeneous
models. Results show that GuardHFL outperforms the state-of-the-art works in efficiency while
ensuring the model utility.

2 BACKGROUND

2.1 HETEROGENEOUS FEDERATED LEARNING

We briefly review the workflow of HFL (Li & Wang, 2019), where clients independently design
their own unique models. Due to such model heterogeneity, clients cannot directly share model
parameters with each other as in the traditional FL. Instead, they learn the knowledge of other
models via a query-response mechanism, which is similar as the knowledge distillation technique
(Hinton et al., 2015). To be more precise, each client PQ (called the querying party) performs three-
phase operations collaboratively with a server: (1) Local training: PQ first trains the local model
(2) Querying: The server selects C fraction of clients as the responding
on his private dataset.
parties PA. Given the auxiliary querying dataset, the server receives the prediction results from
these PA, computes the aggregated result and returns it back to PQ. (3) Local re-training: PQ then
retrains the local model based on the private dataset, as well as the query samples and corresponding
predictions. Each client in HFL can play the roles of the querying party and the responding party
at the same time, and the above process is iterated until each local model meets the pre-defined
accuracy requirement. Note that as illustrated in existing works (Bonawitz et al., 2017; Bell et al.,
2020), the server is responsible for routing the messages between clients, since the clients (e.g.,
mobile devices) generally cannot establish direct communication channels with others.

GuardHFL is in line with the above paradigm with the additional benefit of privacy protection.
In
The only difference lies in the acquisition of auxiliary query samples in the querying stage.
general HFL (Li & Wang, 2019), there is a large public auxiliary dataset (used as query samples)
that every party can access. However, considering the privacy limitation, such public dataset is
hard to collect in real-world scenarios such as healthcare. To tackle this problem, in GuardHFL,
each party can locally construct a synthesized querying set based on its private training samples, by
utilizing existing data augmentation strategies (refer to Section 3.4).

2.2 THREAT MODEL

As described in Section 1, in the querying phase of HFL, the query samples, prediction results
and model parameters may contain sensitive information that is of interest to adversaries. In line

2Choquette-Choo et al. (2021) presented a general collaborative learning scheme, called CaPC, which en-
ables each party to improve his local model from others’ models using the secure querying scheme (Boemer
et al., 2019b). However, it cannot be directly applied to the HFL scenario as it requires cross-client communi-
cation. Meanwhile, it causes prohibitively high overhead (refer to Section 4.1).

2

Under review as a conference paper at ICLR 2023

Figure 1: High-level view of GuardHFL

with prior works (Phong et al., 2018; Sun & Lyu, 2021; Choquette-Choo et al., 2021), we consider
an honest-but-curious adversary setting (Goldreich, 2009), where each entity (including the clients
and the server) strictly follows the specification of the designed protocol but attempts to infer more
knowledge about these private information of other clients. Moreover, to maintain the reputation and
provide more services, the server does not collude with any clients, namely that an attacker either
corrupts the server or a subset of clients but not both.

2.3 EXTENDING EXISTING SECURE QUERYING SOLUTIONS TO HFL

To provide privacy guarantees against adversaries in Section 2.2, the clients and the server need to
privately execute the querying process. Although this process consists of three entities (i.e., PQ,
the server and PA), it is non-trivial to directly extend existing secure 3-party computation proto-
cols (3PC) (Wagh et al., 2019; 2021; Knott et al., 2021; Tan et al., 2021) to instantiate this process.
The main reason is the incapability of direct communication between PQ and PA in realistic HFL
scenarios (Bonawitz et al., 2017; Bell et al., 2020), which hinders the usage of these 3PC solutions
in HFL, unless we redesign the underlying protocols and make substantial modifications to their
corresponding implementations. On the other hand, we can extend state-of-the-art 2PC solutions
(Rathee et al., 2020; Huang et al., 2022) into this process via using the server as the communication
medium with adaptive protocol modifications (refer to Appendix A.2.4 for more details). Unfor-
tunately, as mentioned in Section 1, such extension comes at the cost of heavy computational and
communication complexity. Motivated by these challenges, we design a set of lightweight and cus-
tomized protocols for improving the efficiency of the secure querying phase (Section 3), which show
significant performance gains over extending the advanced 2PC schemes to HFL (Section 4.1).

2.4 CRYPTOGRAPHIC PRIMITIVES
Secret sharing. GuardHFL adopts the 2-out-of-2 arithmetic secret sharing scheme over the ring
Z2ℓ (Shamir, 1979; Demmler et al., 2015). Share(x) denotes the sharing algorithm that takes x as
input and outputs random sampled shares [x]0, [x]1 with the constraint x = [x]0 + [x]1 in Z2ℓ . The
reconstruction algorithm Recon([x]0, [x]1) takes the two shares as input and outputs x = [x]0 + [x]1
in Z2ℓ. Besides, our comparison protocol adopts the boolean secret sharing (Shamir, 1979; Demmler
et al., 2015), where x ∈ Z2 is shared as [x]B
1 = x in Z2. The security
0 and [x]B
1 satisfying [x]B
ensures that given [x]0 or [x]1 (similarly, [x]B
0 or [x]B
1 ), the value of x is perfectly hidden. Arithmetic
operations on secret-shared values can be implemented with existing techniques (Appendix A.2.2).

0 ⊕[x]B

Pseudo-random Function. A pseudo-random function y ← PRF(Sk, x) is a deterministic function
that takes a uniformly random seed Sk and a payload x as input and outputs a fixed-length pseudo-
random string y. The security of PRFs ensures that the output is indistinguishable from the uniform
distribution. In GuardHFL, PRFs enable two parties to generate the same pseudo-random values
without communication. Details can be found in Appendix A.2.2.

3 GU A R DHFL

GuardHFL is built upon standard HFL systems as shown in Section 2.1 and enhances their privacy
protections with cryptographic techniques. Figure 1 shows the overview of GuardHFL and the
detailed description is given in Algorithm 1. Similar as vanilla HFL, it includes three phases: Local
training, Secure querying and Local re-training. Since Local training and Local re-training are
standard HFL training processes without privacy issues, below we focus on formalizing our core
construction, i.e., Secure querying. As detailed in Section 2.3, extending existing secure querying
solutions to HFL introduces expensive overhead due to the usage of heavy cryptographic primitives
and the lack of customized protocols. To tackle this challenge, we propose a tailored secure querying
scheme utilizing lightweight secret sharing and PRF techniques, which is decomposed into three
steps: secure query-data sharing, secure model prediction and secure result aggregation.

In generally, PQ first constructs querying samples locally using data argumentation strategies (Sec-
tion 3.4). Since querying samples imply the semantic information of private training data, they

3

LabelQuerying partyServerQuery dataPredictionTrainRetrainRespondingpartiesLocal dataSecure querying…Heterogeneous models123Under review as a conference paper at ICLR 2023

Algorithm 1 The GuardHFL framework

Input: Each client Pj, j ∈ [n], holds a private dataset Dj and a customized local model Mj.

iter is the
number of iterations. B is the number of query samples and C is the set of selected responding parties in
the current query-response phase.
Output: Trained models Mj, j ∈ [n].
1: for each j ∈ [n] do
2:
3: end for
4: for each iter do
5:
6:

Pj locally trains the local model Mj on Dj using the stochastic gradient descent optimization.

Q randomly samples query data {xb}b∈[B] from the auxiliary querying dataset that are generated

for each querying party P j

Q, j ∈ [n] do

P j

via the data argumentation strategies described in Section 3.4.

A and the server, based on protocol ΠShare in Figure 2.
Q and the server jointly perform the secure model prediction protocol in Section 3.2.

for each responding party P i

A, i ∈ C do
P j
Q secret-shares {[xb]}b∈[B] with P i
A, P j
P i
P i
A secret-shares the predictions {[yi
end for
Q computes {yb}b∈[B], where yb = (cid:80)
P j
P j
Q retrains Mj based on the query dataset {xb, yb}b∈[B] and Dj.
end for

i∈C yi

b]}b∈[B] to PQ and the server.

b, via protocol ΠAgg in Figure 5 with the server.

7:
8:
9:
10:
11:
12:
13:
14:
15: end for

cannot be directly exposed to the server and PA for prediction. Therefore, GuardHFL secret-shares
the query samples to the server and PA using the designed secure query-data sharing protocol (Sec-
tion 3.1). Then given the secret-shared samples, PA, PQ and the server can jointly execute the
proposed secure model prediction protocol (Section 3.2) to obtain the secret-shared inference log-
its. After that, the secure result aggregation protocol (Section 3.3) comes in handy, which takes as
input the secret-shared logits and returns the aggregated results to PQ. During the entire querying
phase, GuardHFL maintains the following invariant (Rathee et al., 2020; Huang et al., 2022): the
server and PA start each protocol with arithmetic shares of inputs, and end with arithmetic shares of
outputs over the same ring. This allows us to sequentially stitch the proposed protocols to obtain a
fully private querying scheme. The formal security analysis is given in Appendix A.3.

3.1 SECURE QUERY-DATA SHARING

To perform secure prediction based on secret sharing techniques, PQ first secret-shares the query
data x with the server and PA. Considering the communication constraint between PQ and PA, we
utilize PRFs to share x. Specifically, we first construct PRF seeds in pairs for PQ, PA and the server,
denoted as SkQA, SkSA, and SkSQ, which are used to generate the same random values between
two parties without communication (refer to Figure 12 in Appendix A.2.2). After that, PQ can share
x using protocol ΠShare as shown in Figure 2. In particular, PQ non-interactively shares [x]0 = r
with PA using PRFs on the seed SkQA. Then PQ computes [x]1 = x − r and sends it to the server.

Figure 2: Secure query-data sharing protocol ΠShare

3.2 SECURE MODEL PREDICTION

In this step, the server and PA execute secure prediction on the secret-shared query data with the
assistance of PQ. Figure 11 in Appendix A.2.1 gives an end-to-end graphic depiction. Below we
elaborate the customized protocols for three components: linear layers, ReLU and MaxPooling.

Linear layers. Linear layers consist of fully-connection, convolution, average pooling and batch
normalization, and the main operation of these layers is matrix multiplication (Wagh et al., 2019;
Huang et al., 2022). We utilize PRFs and secret sharing to design a customized matrix multiplication
protocol ΠMatmul, which is not only compatible with communication-limited HFL settings, but also
improves communication efficiency. Specifically, as shown in Figure 3, PA and the server aim to

4

Server𝑷𝑷𝑄𝑄𝑷𝑷𝐴𝐴Query data 𝑥𝑥𝑟𝑟←PRF(𝑆𝑆𝑆𝑆𝑄𝑄𝐴𝐴)𝑥𝑥−𝑟𝑟[𝑥𝑥]0=𝑟𝑟[𝑥𝑥]1=𝑥𝑥−𝑟𝑟𝑟𝑟←PRF(𝑆𝑆𝑆𝑆𝑄𝑄𝐴𝐴)Under review as a conference paper at ICLR 2023

compute ωx, where the model parameter ω is held by PA and the shares [x]0, [x]1 of x are held by
PA and the server, respectively. Given that ωx = ω[x]0 + ω[x]1, PA can compute ω[x]0 locally.
To evaluate ω[x]1, PQ first generates three random matrices a, b and [c]0 using PRFs, meanwhile,
computes and sends [c]1 that satisfies [c]1 + [c]0 = ab in Z2ℓ
3 to the server. At the same time, using
PRFs, the server generates the same b and PA generates the same a and [c]0. Then PA and the server
can learn [y]0 and [y]1 (i.e., the secret shares of ωx), respectively, through one round of interaction.
Overall, the communication cost is 3ℓ within 1 communication round.

Remark. In our fixed-point representations, to prevent values from overflowing due to the multiplica-
tion operations, we use the truncation technique from Mohassel & Zhang (2017), which is consistent
with existing methods (Mishra et al., 2020; Wagh et al., 2019). This technique simply truncates the
extra LSBs of fixed-point values, albeit at the cost of a 1-bit error, which is negligible on the model
accuracy (Mohassel & Zhang, 2017).

Figure 3: Secure matrix multiplication protocol ΠMatmul
ReLU. ReLU is computed as ReLU(x) = x · (1 ⊕ MSB(x)), where MSB(x) equals 0 if x ≥ 0 and
1 otherwise. Thus, the evaluation of ReLU consists of MSB and multiplication operations. In the
following, we propose an optimized MSB protocol building on the parallel prefix adder (PPA) logic
(Harris, 2003; Mohassel & Rindal, 2018; Patra et al., 2021). Before giving specific optimizations,
we first review the PPA-based MSB method.

PPA-based MSB method. Given that the bit decomposition of [x]0 and [x]1 are eℓ, . . . , e1 and
fℓ, . . . , f1, respectively, the PPA-based method evaluates MSB(x) = eℓ ⊕ fℓ ⊕ cℓ, where cℓ is the
ℓ-th carry bit. Note that cℓ = cℓ−1 ∧ (eℓ−1 ⊕ fℓ−1) ⊕ (eℓ−1 ∧ fℓ−1). Given the carry signal tuple
(g0

i , p0

i ) where g0

i = ei ∧ fi and p0
ℓ−1 ⊕ (p0

cℓ = g0

i = ei ⊕ fi for i ∈ [ℓ], cℓ can be reformulated as
ℓ−1 ∧ g0

1).

ℓ−2) ⊕ · · · ⊕ (p0
The PPA evaluates Eq.1 by constructing a log ℓ-layer tree and traversing from the leaves with inputs
(g0
i ) for i ∈ [ℓ] until reaching the root. Each node k at the n-th layer (n ∈ [log ℓ]) is adhered
with the following operation:

ℓ−1 ∧ · · · ∧ p0

2 ∧ g0

i , p0

(1)

j

∧ pn−1

j+1 ⊕ (gn−1

k = gn−1
j+1 ) and pn
gn
where it takes as inputs of two adjacent signal tuples (gn−1
), and outputs a
signal tuple (gn
. Overall, this
1
method requires 3ℓ − 4 AND gates within log ℓ + 1 communication rounds, where each AND gate is
evaluated by the standard Beaver triple-based multiplication protocol (Appendix A.2.2). Therefore,
it totally needs 30ℓ − 40 bits communication.

j+1 ∧ pn−1
,
j+1 ) and (gn−1
, and MSB(x) = eℓ ⊕ fℓ ⊕ glog ℓ

k ). Finally, cℓ is obtained as glog ℓ

j+1 , pn−1

k = pn−1

, pn−1
j

k , pn

(2)

1

j

j

Optimizations. Straightforwardly adopting the above PPA-based MSB method cannot achieve the
best efficiency. The performance bottleneck comes from: (1) unnecessary AND gates evaluation
introduces extra communication overhead; (2) more communication rounds due to the separation
of input computation and tree evaluation. To solve these challenges, firstly, we simplify the PPA
circuit via removing unnecessary AND gates. In each level n ∈ {1, 2, · · · , log ℓ}, we eliminate
the generation of “least significant” pn
1 that consumes an evaluation of AND but it is unnecessary
for computing MSB. For the same reason, we also remove the evaluation of g0
ℓ . Secondly, we can
further reduce the communication complexity by utilizing the above multiplication protocol ΠMatmul
to evaluate remaining AND gates, instead of the Beaver triple-based method. Thirdly, we integrate
the round of communication used to compute inputs g0
i for i ∈ {1, 2, · · · , ℓ − 1} into the evaluation
of the PPA circuit via a non-trivial modification of the evaluation logic. Overall, our MSB protocol
totally communicates 9ℓ−3 log ℓ−12 bits within log ℓ rounds, a 3.4× communication improvement
over the above PPA-based MSB method. Algorithm 2 gives the detailed construction of Πmsb.

3(a, b, [c]0, [c]1) with the constrain c = ab in Z

triple. Details refer to Appendix A.2.2.

2ℓ can be seen as a variant of the Beaver’s multiplication

5

Server𝑷𝑷𝑄𝑄𝑷𝑷𝐴𝐴𝑎𝑎,[𝑐𝑐]0←PRF(𝑆𝑆𝑆𝑆𝑄𝑄𝐴𝐴)𝑏𝑏←PRF(𝑆𝑆𝑆𝑆𝑆𝑆𝑄𝑄)𝜔𝜔+𝑎𝑎[𝑥𝑥]1-𝑏𝑏[𝑦𝑦]0=𝜔𝜔[𝑥𝑥]0+𝜔𝜔([𝑥𝑥]1−𝑏𝑏)-[𝑐𝑐]0𝑎𝑎,[𝑐𝑐]0←PRF(𝑆𝑆𝑆𝑆𝑄𝑄𝐴𝐴)[𝑐𝑐]1=𝑎𝑎𝑏𝑏−[𝑐𝑐]0[𝑦𝑦]1=(𝜔𝜔+𝑎𝑎)𝑏𝑏-[𝑐𝑐]1𝑏𝑏←PRF(𝑆𝑆𝑆𝑆𝑆𝑆𝑄𝑄)Model parameter𝜔𝜔,[𝑥𝑥]1[𝑥𝑥]0Under review as a conference paper at ICLR 2023

Algorithm 2 Secure MSB Protocol Πmsb
Input: The arithmetic shares [x]
Output: The boolean shares [msb(x)]B
1: Initiate g∗ and p∗ with size ℓ. Let g∗
2: P0 and P1 set the bit decomposition of [x]0 and [x]1 to eℓ, · · · , e1 and fℓ, · · · , f1 respectively.
3: P0 and P1 invoke ΠAND with inputs ei and fi to obtain [g∗
4: P0 sets [p∗
5: P0 and P1 invoke ΠAND with inputs [p∗
6: for r ∈ {2, · · · , log ℓ} do
for i ∈ {2, · · · ,
7:
8:

i are the i-th positions of g∗ and p∗ respectively.

If r = 2, P0 and P1 invoke ΠAND with inputs [g∗

0 = ei and P1 sets [p∗

i ]B for i ∈ {2, · · · , ℓ

2i−2]B to obtain [p∗

i ]B for i ∈ [ℓ − 1].

2i−1]B and [p∗

2r−1 } do

i and p∗

1 = fi.

i ]B

i ]B

ℓ

2 }.

2i−2]B and [p∗
2i−1]B ⊕ [ti]B. Else P0 and P1 invoke ΠAND with inputs [g∗
i ]B = [g∗

2i−1]B to obtain [ti]B, and sets
2i]B to obtain [ti]B,
2i−1]B and [p∗

1 ]B and [p∗

2]B to obtain [t1]B, and sets [g∗

1 ]B = [g∗

2 ]B ⊕ [t1]B.

[g∗
i ]B = [g∗
and sets [g∗
end for
P0 and P1 invoke ΠAND with inputs [g∗
ℓ
for i ∈ {2, · · · ,

2i]B ⊕ [ti]B.

2r−1 } do

9:
10:
11:
12:

If r = 2, P0 and P1 invoke ΠAND with inputs [p∗

P1 invoke ΠAND with inputs [p∗

2i]B and [p∗

2i−1]B to obtain [p∗

2i−1]B and [p∗
i ]B.

2i−2]B to obtain [p∗

i ]B. Else P0 and

end for

13:
14: end for
15: P0 sets [msb(x)]B

0 = eℓ ⊕ [g∗

1 ]B

0 and P1 sets [msb(x)]B

1 = fℓ ⊕ [g∗

1 ]B
1 .

After obtaining [MSB(x)]B, we need to compute [x] · (1 ⊕ [MSB(x)]B), i.e., the secret shares of
ReLU(x). Given z0 = [MSB(x)]B
0 and z1 = 1 ⊕ [MSB(x)]B
0 for simplicity, we have ReLU(x) =
([x]0 + [x]1)(z0 + z1 − 2z0z1) = z0[x]0 + z1[x]1 + z1(1 − 2z0)[x]0 + z0(1 − 2z1)[x]1. The first
two terms can be evaluated locally by PA and the server respectively, while the other two terms are
evaluated using protocol ΠMatmul. For example, to compute z1(1 − 2z0)[x]0, the protocol ΠMatmul
inputs t0 = (1 − 2z0)[x]0 and z1 provided by PA and the server, and outputs [t0z1]0 and [t0z1]1 to
PA and the server, respectively. Finally, PA and the server learn [y]0 and [y]1 respectively, such that
y = ReLU(x). The complete secure ReLU protocol ΠReLU is shown in Figure 4.

Figure 4: Secure ReLU protocol ΠReLU
Maxpooling. The evaluation of Maxpooling can be performed with the protocol ΠReLU as well
as a tree-based round optimization that recursively partitions the values into two halves and then
compares the elements of each half. Precisely, the entities arrange the input of m elements into a
2-ary tree with the depth of log m, and evaluate the tree in a top-down fashion. In each comparison
of two secret-shared elements [x] and [y], we utilize the observation of max([x], [y]) = ReLU([x] −
[y]) + [y]. Hence the protocol complexity of Maxpooling mainly comes from the evaluation of
m − 1 ReLU. Besides, as illustrated in Wagh et al. (2019); Mishra et al. (2020), AvgPooling can be
evaluated locally without communication.

3.3 SECURE RESULT AGGREGATION
After the secure prediction, the predicted logit [xi] is secret-shared between the server and each
responding party P i
A, where i ∈ [C] and C is the set of responding parties. To prevent privacy
leakage from a single prediction (Salem et al., 2019; Ganju et al., 2018; Yang et al., 2019), we return
the aggregated logit to PQ via the secure aggregation protocol ΠAgg in Figure 5. Specifically, P i
A
and PQ first generate a random value ri based on PRFs. Then each P i
A computes and sends [xi]0 −ri
to the server. The server sums all received values and sends the masked aggregation to PQ, which
will reconstruct the aggregated logits of the query data. Notice that our secure aggregation protocol
can be extended to output the aggregated label rather than the logit, using the above ΠReLU protocol.

6

Server𝑷𝑷𝑄𝑄𝑷𝑷𝐴𝐴[𝑥𝑥]1[𝑥𝑥]0∏msb[msb(𝑥𝑥)]1𝐵𝐵[msb(𝑥𝑥)]0𝐵𝐵=𝑧𝑧0∏Matmul(𝑎𝑎,𝑏𝑏,[𝑐𝑐]0,[𝑐𝑐]1)𝑧𝑧0,𝑧𝑧1=[msb(𝑥𝑥)]1𝐵𝐵⨁1,𝑡𝑡1=1−2𝑧𝑧1𝑥𝑥1[𝑡𝑡0𝑧𝑧1]1,[𝑡𝑡1𝑧𝑧0]1[𝑡𝑡0𝑧𝑧1]0,[𝑡𝑡1𝑧𝑧0]0𝑡𝑡0=(1−2𝑧𝑧0)𝑥𝑥0[𝑦𝑦]1=𝑧𝑧1[𝑥𝑥]1+[𝑡𝑡0𝑧𝑧1]1+[𝑡𝑡1𝑧𝑧0]1[𝑦𝑦]0=𝑧𝑧0[𝑥𝑥]0+[𝑡𝑡0𝑧𝑧1]0+[𝑡𝑡1𝑧𝑧0]0Under review as a conference paper at ICLR 2023

3.4 DISCUSSION

Figure 5: Secure result aggregation protocol ΠAgg

Query data construction. Unlike existing HFL works relying on auxiliary datasets as the query data
(Choquette-Choo et al., 2021; Lin et al., 2020), we demonstrate the feasibility of model knowledge
transfer in GuardHFL by constructing a synthesized query set based on private training data, to
alleviate potential limitations (e.g., privacy, acquisition and storage) of auxiliary datasets. A simple
solution is to directly use the private training data to query, like well-studied knowledge distillation
(Hinton et al., 2015). Moreover, we also construct a synthesized dataset via the mixup method
(Zhang et al., 2018) (Appendix A.1.2). The synthesized dataset construction is a universal and
modular method, and it can be readily extended with advanced data augmentation strategies, such
as cutout (DeVries & Taylor, 2017) and cutmix (Yun et al., 2019). Note that this process does
not reveal any private information, since the samples are constructed locally by the querying party,
without involving any other parties and their private datasets. We present some exploration and
experiments in Appendix A.1.2 and Figure 10(c).

GPU-friendly evaluation. Our scheme is friendly with GPUs and can be processed by highly-
optimized CUDA kernels (Tan et al., 2021). As discussed above, the cryptographic protocols of
GuardHFL only involve simple vectorized arithmetic operations, rather than homomorphic encryp-
tion and garbled circuits in prior works (Rathee et al., 2020; Huang et al., 2022; Choquette-Choo
et al., 2021). As a result, GuardHFL is suitable for batch querying (i.e., executing multiple query-
ing at the same time) with a lower amortized cost. We evaluate the designed protocols on GPUs in
Section 4.1 and show the advantage of GPU acceleration over CPUs in Appendix A.1.2.

4 EVALUATION
Datasets and models. We evaluate GuardHFL on three image datasets (SVHN, CIFAR10 and
Tiny ImageNet). By default, we assume independent and identically distributed (IID) training data
among clients. We also simulate disjoint Non-IID training data via the Dirichlet distribution Dir(α)
in Lin et al. (2020). The value of α controls the degree of Non-IID-ness, where a smaller α indicates
a higher degree of Non-IID-ness. Moreover, we simulate the heterogeneity property in HFL. In
particular, for SVHN and CIFAR10, we set the number of clients n = 50 and use VGG-7, ResNet-8
and ResNet-10 architectures as the clients’ local models. For Tiny ImageNet, we set n = 10 and
use ResNet-14, ResNet-16, and ResNet-18 architectures. Each model architecture is used by n/3
clients. Besides, the query data are constructed via two methods as shown in Section 3.4: using the
private training data (Q-priv) or synthesizing samples (Q-syn) via mixup (Zhang et al., 2018).

Cryptographic protocol. Following existing works (Rathee et al., 2020; Tan et al., 2021), we set
secret-sharing protocols over a 64-bit ring Z264 , and encode inputs using a fixed-point representation
with 20-bit precision. The security parameter κ is 128 in the instantiation of PRFs. Unless otherwise
stated, we only report the performance on the GPU accelerator. More experimental setup is given in
Appendix A.1.1.

4.1 EFFICIENCY
We report the efficiency of GuardHFL, and compare it with CaPC (Choquette-Choo et al., 2021)
and instantiations of HFL based on state-of-the-art secure querying protocols (Rathee et al., 2020;
Huang et al., 2022; Tan et al., 2021).

End-to-end performance. We show the extra overhead introduced by GuardHFL compared with
the vanilla HFL system in the plaintext environment. This is caused by the secure querying phase,
which consists of three steps described in Section 3. Table 1 reports the runtime of each step for
different models and datasets4. We observe that the cost is dominated by the secure model prediction
step. Specifically, it takes 16.9 minutes to evaluate 5000 query samples securely on VGG-7 and
CIFAR10, and only 11.32 second and 0.3 second are spent on the secure query-data sharing and
secure result aggregation steps. More time is required to evaluate Tiny ImageNet because of larger
input sizes and model architectures.

4To clearly illustrate the efficiency of GuardHFL, unless otherwise specified, we only show the overhead

of one user in each iteration as described in Section 3.

7

Server[𝑥𝑥𝑖𝑖]1,𝑖𝑖∈[𝐶𝐶]𝑷𝑷𝐴𝐴𝑖𝑖[𝑥𝑥𝑖𝑖]0𝑟𝑟𝑖𝑖←PRF(𝑆𝑆𝑆𝑆𝑄𝑄𝐴𝐴)[𝑥𝑥𝑖𝑖]0−𝑟𝑟𝑖𝑖𝑆𝑆=∑𝑖𝑖=1|𝐶𝐶|([𝑥𝑥𝑖𝑖]0−𝑟𝑟𝑖𝑖+[𝑥𝑥𝑖𝑖]1)𝑷𝑷𝑄𝑄𝑟𝑟𝑖𝑖←PRF(𝑆𝑆𝑆𝑆𝑄𝑄𝐴𝐴)𝑦𝑦←softmax(𝑆𝑆+∑𝑖𝑖=1|𝐶𝐶|𝑟𝑟𝑖𝑖)𝑆𝑆Under review as a conference paper at ICLR 2023

Table 1: Extra runtime (sec) of GuardHFL over vanilla HFL systems in the plaintext environment.
CIFAR10 and SVHN have the same runtime due to the same input size and model architecture.

Dataset

# of
Queries

1. Query
data sharing

2. Secure prediction

3. Result
aggreg.

CIFAR10
(SVHN)

TINY
IMAGENET

1000
2500
5000

1000
2500

5.08
7.16
11.32

9.87
18.78

VGG-7

RESNET-8

RESNET-10

205.46
511.63
1019.12

270.78
657.83
1346.79

305.46
758.16
1521.23

RESNET-14 RESNET-16 RESNET-18

2700.96
6815.69

2971.47
7217.28

3084.81
7503.5

0.09
0.12
0.30

0.18
0.32

Table 2: Comparison with CaPC on runtime (sec) over MNIST and three heterogeneous models as
the batch size (BS) of query data increases.

Model

BS=128
BS=256
BS=512
BS=1024

CryptoNets

CryptoNets-ReLU

MLP

GU A R DHFL
0.03
0.05
0.07
0.13

CAPC
17.75
17.56
17.62
17.77

GU A R DHFL
0.24
0.31
0.50
0.89

CAPC
48.83
70.14
112.42
201.42

GU A R DHFL
0.75
0.83
1.05
1.58

CAPC
65.01
86.37
129.81
216.61

Comparison with CaPC. As described in Section 1, similar to GuardHFL, CaPC (Choquette-Choo
et al., 2021) was proposed to support private collaborative learning utilizing the secure querying
scheme (Boemer et al., 2019b), but with the unrealistic cross-client communication. In Table 2, we
compare the secure querying process of GuardHFL with CaPC. Following CaPC’s setup, we eval-
uate three small-scale models (CryptoNets (Gilad-Bachrach et al., 2016), CryptoNets-ReLU (Gilad-
Bachrach et al., 2016) and MLP (Boemer et al., 2019b)) on MNIST. We observe that GuardHFL
is two orders of magnitude faster than CaPC on the three models. In terms of communication over-
head, we provide a theoretical comparison. (1) For linear layers, CaPC requires to communicate 2
homomorphic ciphertexts within 2 rounds. GuardHFL needs communicating 3 ring elements (each
with 64-bit). Note that the size of ciphertexts is much larger than the size of the ring elements. (2)
For non-linear layers, e.g., ReLU, CaPC adopts the garbled circuit technique that requires 2 rounds
with 8ℓλ − 4λ communication bits (λ = 128 and ℓ = 64 in our setting) (Rathee et al., 2020).
GuardHFL only requires communicating 15ℓ − 3 log ℓ − 12 bits, a 70× improvement over CaPC.

Comparison with SOTA works. To further demonstrate the efficiency of GuardHFL, we instan-
tiate HFL based on SOTA 2PC schemes, including Cheetah (Huang et al., 2022) and CrypTFlow2
(Rathee et al., 2020), using the methods described in Appendix A.2.4. Table 3 reports the run-
time and communication comparison of the secure prediction phase over CIFAR10. We observe
that GuardHFL achieves a significant efficiency improvement on three heterogeneous models. For
example, GuardHFL requires 57.4∼75.6× less runtime and 8.6∼12.7× less communication com-
pared to CrypTFlow2. This is because the latter needs heavy HE-based multiplication and OT-based
comparison within multi-communication rounds. Moreover, as shown in Section 2.3, extending 3PC
protocols such as CryptGPU (Tan et al., 2021) to HFL is non-trivial. However, since GryptGPU is
one of the most advanced protocols under GPU analogs built on CrypTen (Knott et al., 2021), we
also compare with it assuming no communication limitation. We would like to mention that despite
such an unfair comparison, GuardHFL still has performance advantages, i.e., roughly 2.1× and
2.0× in computation and communication overheads, respectively.

4.2 ACCURACY

We report the accuracy of each heterogeneous model in GuardHFL, and explore the impact of
various factors on the model accuracy such as the Non-IID setting, and the number of query data.

End-to-end model accuracy. Table 4 reports the model accuracy on three datasets in GuardHFL.
We observe that for SVHN and CIFAR10, using Q-priv to query can increase the accuracy by about
4%, while the accuracy gain is about 10% when using 10K query samples with Q-syn. The main
reason is that synthetic samples could provide a good coverage of the manifold of natural data. We
also observe that more synthetic query data can achieve better performance from Table 4. Further-
more, with an increased number of participating clients, the accuracy improves slightly. Figure 6
shows the accuracy curves versus the number of iterations. We use SVHN and CIFAR10 as exam-
ples, as they converge much faster with better readable curves than Tiny ImageNet. We can observe
that each heterogeneous model on both datasets can converge well based on two types of query data,
and Q-syn shows better performance.

8

Under review as a conference paper at ICLR 2023

Table 3: Comparison with advanced secure prediction protocols on runtime (sec) and communica-
tion (MB) cost over three heterogeneous models.

Method

CRYPTFLOW2
CHEETAH
CRYPTGPU
GU A R DHFL

VGG-7

ResNet-8

ResNet-10

TIME
48.70
3.95
1.61
0.73

COMM.
651.51
116.14
144.51
75.52

TIME
56.21
4.29
2.02
0.98

COMM.
1110.39
94.51
131.39
87.60

TIME
97.46
6.79
2.79
1.29

COMM.
1395.18
169.35
221.57
120.26

Table 4: The model accuracy of three datasets in GuardHFL on different ratios of participating
clients (0.6, 0.8 and 1), and querying strategies (Q-priv and Q-syn).

Dataset

Ratio of clients
Before GuardHFL
Q-priv

Q-syn

2.5K
5.0K
7.5K
10K

SVHN

0.8
75.46
79.56
80.32
83.52
84.78
84.97

0.6

79.43
80.09
83.32
84.54
84.58

CIFAR10

Tiny ImageNet

1

0.6

80.29
81.69
83.82
85.12
85.62

60.82
62.87
63.04
62.97
63.79

0.8
56.66
61.01
63.05
63.44
63.64
63.82

1

0.6

61.49
63.23
63.69
63.88
64.56

24.89
25.82
26.22
27.14
27.67

0.8
22.26
25.11
26.03
26.46
27.54
28.19

1

25.23
26.23
26.75
27.75
28.46

Impact of Non-IID datasets. We il-
lustrate the impact of Non-IID data
on model accuracy in Figure 7, us-
ing CIFAR10 as an example. Figures
7(a), 7(b) and 7(c) visualize the dis-
tributions of Non-IID samples among
clients with different Dir(α). When
α = 100, the distribution is close to
uniform sampling. When α = 0.5,
the sample distribution of each class
among clients is extremely uneven. From Figure 7(d) we observe that the higher the degree of Non-
IID-ness, the lower the accuracy of models. Notably, GuardHFL can still significantly improve the
performance of models under the Non-IID environment.

Figure 6: Accuracy curves of each heterogeneous model in
GuardHFL. Left (SVHN); Right (CIFAR10)

(a) α = 100

(b) α = 1

(c) α = 0.5

(d) Accuracy

Figure 7: Visualization of Non-IID-ness among clients with different Dirichlet distribution α on
CIFAR10. The size of scattered points indicates the number of training samples of that class.
Impact of other factors. Due to space constraints, we report other experimental results in Ap-
pendix A.1.2. In particular, Figure 9 shows the accuracy of each heterogeneous model with different
numbers of query data. Figures 10(a) and 10(b) illustrate the impact of different numbers of pri-
vate training data on SVHN and CIFAR10. Figure 10(c) details the impact of different query data
construction methods.

5 CONCLUSION

We propose GuardHFL, an efficient and private HFL framework to formally provide the privacy
guarantees of query samples, model parameters and prediction results. The core constructions of
GuardHFL are customized multiplication and comparison protocols based on lightweight secret
sharing and PRFs techniques. Extensive experiments demonstrate that GuardHFL outperforms
prior art in both communication and runtime performance. We consider the following future direc-
tions. (1) The communication cost of GuardHFL, which is also the limitation of the standard HFL
paradigm, will be further improved. One possible mitigation is to extend the insight of the k-regular
graph in FL (Bell et al., 2020) to HFL, and carefully design protocols from scratch. The main idea
is that in FL it is enough for each party to speak to k < n − 1 other parties via the server, where n
is the number of parties. (2) The security of GuardHFL will be improved to defeat more powerful
malicious adversaries. Unfortunately, even using the best-known technique (Koti et al., 2021), the
overhead will be increased by several orders of magnitude. We leave these improvements as future
work.

9

0246810Iterations7075808590AccuracyVGG7 (Q-syn)Resnet8 (Q-syn)Resnet10 (Q-syn)VGG7 (Q-priv)Resnet8 (Q-priv)Resnet10 (Q-priv)0246810Iterations5055606570AccuracyVGG7 (Q-syn)Resnet8 (Q-syn)Resnet10 (Q-syn)VGG7 (Q-priv)Resnet8 (Q-priv)Resnet10 (Q-priv)012345678910111213141516171819Party IDs0123456789Class labels012345678910111213141516171819Party IDs0123456789Class labels012345678910111213141516171819Party IDs0123456789Class labels0.51100Degree of Non-IID-ness3040506070Accuracy (%)Before GuardHFLGuardHFL (Q-priv)GuardHFL (Q-syn)Under review as a conference paper at ICLR 2023

REFERENCES

Michel Abdalla, Mihir Bellare, and Phillip Rogaway. The oracle diffie-hellman assumptions and an

analysis of dhies. In Proceedings of CT-RSA, pp. 143–158. Springer, 2001.

Gilad Asharov, Yehuda Lindell, Thomas Schneider, and Michael Zohner. More efficient oblivious

transfer and extensions for faster secure computation. In Proceedings of ACM CCS, 2013.

Donald Beaver. Efficient multiparty protocols using circuit randomization.

In Proceedings of

CRYPTO, 1991.

James Henry Bell, Kallista A Bonawitz, Adri`a Gasc´on, Tancr`ede Lepoint, and Mariana Raykova.
Secure single-server aggregation with (poly) logarithmic overhead. In Proceedings of ACM CCS,
2020.

Fabian Boemer, Anamaria Costache, Rosario Cammarota, and Casimir Wierzynski. ngraph-he2: A
high-throughput framework for neural network inference on encrypted data. In Proceedings of
the ACM Workshop on Encrypted Computing & Applied Homomorphic Cryptography, 2019a.

Fabian Boemer, Yixing Lao, Rosario Cammarota, and Casimir Wierzynski. ngraph-he: a graph
In Proceedings of the ACM

compiler for deep learning on homomorphically encrypted data.
International Conference on Computing Frontiers, 2019b.

Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In proceedings of ACM CCS, 2017.

Alon Brutzkus, Ran Gilad-Bachrach, and Oren Elisha. Low latency privacy preserving inference.

In Proceedings of ICML, 2019.

Ran Canetti. Universally composable security: A new paradigm for cryptographic protocols. In

Proceedings of FOCS, 2001.

Hao Chen, Wei Dai, Miran Kim, and Yongsoo Song. Efficient multi-key homomorphic encryption
with packed ciphertexts with application to oblivious neural network inference. In Proceedings
of ACM CCS, 2019.

Christopher A Choquette-Choo, Natalie Dullerud, Adam Dziedzic, Yunxiang Zhang, Somesh Jha,
Nicolas Papernot, and Xiao Wang. Capc learning: Confidential and private collaborative learning.
In Proceedings of ICLR, 2021.

Daniel Demmler, Thomas Schneider, and Michael Zohner. Aby-a framework for efficient mixed-

protocol secure two-party computation. In Proceedings of NDSS, 2015.

Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks

with cutout. arXiv preprint arXiv:1708.04552, 2017.

Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient

federated learning for heterogeneous clients. In Proceedings of ICLR, 2021.

Whitfield Diffie and Martin Hellman. New directions in cryptography. IEEE Transactions on Infor-

mation Theory, 22(6):644–654, 1976.

Canh T Dinh, Nguyen H Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau

envelopes. In Proceedings of NeurIPS, 2020.

Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning with theo-
retical guarantees: A model-agnostic meta-learning approach. In Proceedings of NeurIPS, 2020.

Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. Property inference attacks
on fully connected neural networks using permutation invariant representations. In Proceedings
of ACM CCS, 2018.

Craig Gentry. Fully homomorphic encryption using ideal lattices. In Proceedings of ACM STOC,

2009.

10

Under review as a conference paper at ICLR 2023

Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Werns-
ing. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy.
In Proceedings of ICML, 2016.

Oded Goldreich. Foundations of cryptography. Cambridge university press, 2009.

David Harris. A taxonomy of parallel prefix networks. In Proceedings of Asilomar Conference on

Signals, Systems & Computers, pp. 2213–2217, 2003.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv

preprint arXiv:1503.02531, 2015.

Zhicong Huang, Wen-jie Lu, Cheng Hong, and Jiansheng Ding. Cheetah: Lean and fast secure

two-party deep neural network inference. In Proceedings of USENIX Security, 2022.

Bargav Jayaraman and David Evans. Evaluating differentially private machine learning in practice.

In Proceedings of USENIX Security, 2019.

Bargav Jayaraman and Lingxiao Wang. Distributed learning without distress: Privacy-preserving

empirical risk minimization. 2018.

Chiraag Juvekar, Vinod Vaikuntanathan, and Anantha Chandrakasan. Gazelle: A low latency frame-

work for secure neural network inference. In Proceedings of USENIX Security, 2018.

Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Ad-
vances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.

Marcel Keller and Ke Sun. Secure quantized training for deep learning. In Proceedings of ICML,

2022.

Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta, Mark Ibrahim, and Laurens
van der Maaten. Crypten: Secure multi-party computation meets machine learning. In Proceed-
ings of NIPS, 2021.

Nishat Koti, Mahak Pancholi, Arpita Patra, and Ajith Suresh. Swift: Super-fast and robust privacy-

preserving machine learning. In Proceedings of USENIX Security, 2021.

Junghyun Lee, Eunsang Lee, Joon-Woo Lee, Yongjune Kim, Young-Sik Kim, and Jong-Seon No.
Precise approximation of convolutional neuralnetworks for homomorphically encrypted data.
arXiv:2105.10879, 2021.

Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. In
Proceedings of NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality,
2019.

Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model

fusion in federated learning. In Proceedings of NeurIPS, 2020.

Pratyush Mishra, Ryan Lehmkuhl, Akshayaram Srinivasan, Wenting Zheng, and Raluca Ada Popa.
Delphi: A cryptographic inference service for neural networks. In Proceedings of USENIX Secu-
rity, 2020.

Payman Mohassel and Peter Rindal. Aby3: A mixed protocol framework for machine learning. In

Proceedings of ACM CCS, 2018.

Payman Mohassel and Yupeng Zhang. Secureml: A system for scalable privacy-preserving machine

learning. In Proceedings of IEEE S&P, 2017.

Nicolas Papernot, Mart´ın Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-
supervised knowledge transfer for deep learning from private training data. In Proceedings of
ICLR, 2016.

Arpita Patra, Thomas Schneider, Ajith Suresh, and Hossein Yalame. Aby2.0: Improved mixed-

protocol secure two-party computation. In Proceedings of USENIX Security, 2021.

11

Under review as a conference paper at ICLR 2023

Le Trieu Phong, Yoshinori Aono, Takuya Hayashi, Lihua Wang, and Shiho Moriai. Privacy-
preserving deep learning via additively homomorphic encryption. IEEE Transactions on Infor-
mation Forensics and Security, 13(5):1333–1345, 2018.

Deevashwer Rathee, Mayank Rathee, Nishant Kumar, Nishanth Chandran, Divya Gupta, Aseem
Rastogi, and Rahul Sharma. Cryptflow2: Practical 2-party secure inference. In Proceedings of
ACM CCS, 2020.

M Sadegh Riazi, Christian Weinert, Oleksandr Tkachenko, Ebrahim M Songhori, Thomas Schnei-
der, and Farinaz Koushanfar. Chameleon: A hybrid secure computation framework for machine
learning applications. In Proceedings of AsiaCCS, 2018.

Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes.
Ml-leaks: Model and data independent membership inference attacks and defenses on machine
learning models. In Proceedings of NDSS, 2019.

Sinem Sav, Apostolos Pyrgelis, Juan R Troncoso-Pastoriza, David Froelicher, Jean-Philippe
Bossuat, Joao Sa Sousa, and Jean-Pierre Hubaux. Poseidon: Privacy-preserving federated neural
network learning. In Proceedings of NDSS, 2021.

Adi Shamir. How to share a secret. Communications of the ACM, 22(11):612–613, 1979.

Lichao Sun and Lingjuan Lyu. Federated model distillation with noise-free differential privacy. In

Proceedings of IJCAI, 2021.

Sijun Tan, Brian Knott, Yuan Tian, and David J Wu. Cryptgpu: Fast privacy-preserving machine

learning on the gpu. In Proceedings of IEEE S&P, 2021.

Buse GA Tekgul, Yuxi Xia, Samuel Marchal, and N Asokan. Waffle: Watermarking in federated

learning. In Proceedings on IEEE SRDS, 2021.

Sameer Wagh, Divya Gupta, and Nishanth Chandran. Securenn: 3-party secure computation for
neural network training. Proceedings on Privacy Enhancing Technologies, 2019(3):26–49, 2019.

Sameer Wagh, Shruti Tople, Fabrice Benhamouda, Eyal Kushilevitz, Prateek Mittal, and Tal Rabin.
Falcon: Honest-majority maliciously secure framework for private deep learning. Proceedings on
Privacy Enhancing Technologies, 1:188–208, 2021.

Ziqi Yang, Jiyi Zhang, Ee-Chien Chang, and Zhenkai Liang. Neural network inversion in adversarial

setting via background knowledge alignment. In Proceedings of ACM CCS, 2019.

Andrew Chi-Chih Yao. How to generate and exchange secrets.

In Proceedings of IEEE FOCS,

1986.

Tehrim Yoon, Sumin Shin, Sung Ju Hwang, and Eunho Yang. Fedmix: Approximation of mixup

under mean augmented federated learning. In Proceedings of ICLR, 2021.

Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceed-
ings of ICCV, 2019.

Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In Proceed-
ings of ICML, 2019.

Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical

risk minimization. In Proceedings of ICLR, 2018.

Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous

federated learning. In Proceedings of ICML, 2021.

12

Under review as a conference paper at ICLR 2023

A APPENDIX

A.1 MORE DETAILS ON EXPERIMENT EVALUATION

A.1.1 EXPERIMENTAL SETUP

Datasets. We evaluate GuardHFL on the following standard datasets for image classification: (1)
SVHN is a real-world image dataset obtained from house numbers in Google Street View images,
which contains 600,000 32×32 RGB images of printed digits from 0 to 9. (2) CIFAR10 consists of
60,000 32×32 RGB images in 10 classes. There are 50,000 training images and 10,000 test images.
(3) Tiny ImageNet contains 100,000 images of 200 classes downsized to 64×64 colored images.
Each class has 500 training images, 50 validation images and 50 test images.

Experimental configuration. Each of entities, i.e., PQ, PA and the server, is run on the Ubuntu 18.4
system with Intel(R) 562 Xeon(R) CPU E5-2620 v4(2.10 GHz) and 16 GB of RAM and NVIDIA
1080Ti GPU. We leverage PyTorch’s torch.distributed package for communication similar as Knott
et al. (2021); Tan et al. (2021). We ran our benchmarks in the LAN setting, where following Huang
et al. (2022) the bandwidth is about 384 MBps and the latency is 0.3ms.

Training procedure. At the local training phase, each client trains the local model from scratch
using stochastic gradient descent optimization. For SVHN, CIFAR10, and Tiny ImageNet, the loss
function is cross-entropy with the learning rate of 0.5, 0.1, 0.01, respectively. Besides, the batch
size is 256, 64 and 64, respectively. When the clients retrain the local model at the local retraining
step, they use Adam optimizer for 50 epochs with learning rate of 2e-3 decayed by a factor of 0.1
on 25 epochs, where the batch size is 256 on SVHN, and 64 on both CIFAR10 and Tiny ImageNet.

A.1.2 EXPERIMENTAL RESULTS

Impact of GPU acceleration. To further explore the impact of GPU acceleration, we evaluate
GuardHFL on both CPU and GPU with different batch sizes of query data. Figure 8 reports the re-
sults of VGG-style and ResNet-style networks on CIFAR10, where the GPU-based setting is always
superior to the CPU analogs. As the batch size increases, the advantage of GPU-based protocols
becomes more pronounced, e.g., 14× runtime reduction on ResNet-8 over the batch size 64.

Figure 8: The runtime of GuardHFL under CPU/GPU with varied batch sizes of query data on
CIFAR10.

Impact of the number of query data. Figure 9 shows the accuracy of each heterogeneous model
with different numbers of query data. We observe that GuardHFL consistently improves the model
accuracy on above datasets and heterogeneous models. Specifically, as the number of query data
increases (from 2.5K to 10K), the accuracy of all three models increases by about 5%.

Impact of the number of private training data. Figures 10(a) and 10(b) illustrate the model
accuracy of GuardHFL under different number of private training data on SVHN and CIFAR10.
We can observe that as the number of training data increases, the model performance is on the
rise. The main reason is that models can learn better on more training data and can construct more
synthetic samples to query, so as to promote the transfer of model knowledge.

Impact of query data construction strategies. Figure 10(c) gives the model accuracy under three
advanced data augmentation strategies, including cutmix (Yun et al., 2019), cutout (DeVries &

13

0102030405060Batch Size050100150200250Runtime (sec)VGG7-GPUVGG9-GPUVGG11-GPUVGG7-CPUVGG9-CPUVGG11-CPU0102030405060Batch Size0100200300Runtime (sec)Resnet8-GPUResnet10-GPUResnet12-GPUResnet8-CPUResnet10-CPUResnet12-CPUUnder review as a conference paper at ICLR 2023

(a) SVHN

(b) CIFAR10
Figure 9: The accuracy of each heterogeneous model as the number of query data increases. Dashed
lines represent the model accuracy before GuardHFL.

(c) Tiny ImageNet

(a) SVHN

(b) CIFAR10

(c) Various query data construction
methods

Figure 10: The model accuracy under different number of training data and query data construction
methods on SVHN and CIFAR10.

Taylor, 2017), and mixup (Zhang et al., 2018). Cutmix (Yun et al., 2019) can be formulated as
˜xi,j = M · xi + (1 − M ) · xj, where M ∈ {0, 1}W ×H is a binary mask matrix of size W × H to
indicate the location of dropping out and filling from the two images xi and xj. Cutout (DeVries
& Taylor, 2017) augments the dataset with partially occluded versions of original samples. Mixup
(Zhang et al., 2018) constructs synthetic samples by a convex combination on two images xi and xj
with different coefficients λ, in which ˜xi,j = λ · xi + (1 − λ) · xj. We observe that those strategies
are good choices for query data construction in GuardHFL.

A.2 MORE DETAILS ON THE DESIGNED SCHEME

A.2.1 GRAPHIC DEPICTION OF END-TO-END SECURE PREDICTION SCHEME

Figure 11 gives a graphic depiction to illustrate the end-to-end secure prediction scheme, where
the input is secret-shared sample [x]. The whole process maintains the following invariant: the
server and PA begin with secret shares of the input and after each layer, end with secret shares (over
the same ring) of the output. Honest-but-curious security of GuardHFL will follow trivially from
sequential composibility of individual layers. To be specific, [x] first passes through a convolutional
layer that can be formalized as the secure matrix multiplication operation ω1[x] (ω1 is the parameter)
using protocol ΠMatmul in Figure 3. The secret-shared outputs of this layer, i.e., [y1]0 and [y1]1, are
obtained by PA and the server, respectively. For the subsequent ReLU layer, protocol ΠReLU in
Figure 4 is executed to return [y2]0 and [y2]1 to PA and the server respectively. Then Maxpooling
on [y2] can be evaluated via protocol ΠReLU as described in Section 3.2, to outputs the secret-shared
values [y3]0 and [y3]1. When the secure prediction reaches the final fully-connected layer with
inputs [yn−1]0 and [yn−1]1, protocol ΠMatmul is executed. In the end, PA and the server obtain the
secret-shared predicted logit, i.e., [logit]0 and [logit]1, respectively.

A.2.2 MORE DETAILS ON CRYPTOGRAPHIC PROTOCOLS

Secret sharing and Beaver’s multiplication protocol. As shown in Section 2.4, GuardHFL uti-
lizes the additive secret sharing primitive to protect the privacy of sensitive information. Assuming
two secret-shared values are [x] and [y] owned by two parties Pi, i ∈ {0, 1}, addition and subtraction
operations ([z] = [x] ± [y] in Z2ℓ ) can be realized locally without any communication, i.e., each Pi

14

25005000750010000Number of Query Data5060708090100Accuracy (%)VGG7ResNet8ResNet1025005000750010000Number of Query Data5055606570Accuracy (%)VGG7ResNet8ResNet1025005000750010000Number of Query Data101520253035Accuracy (%)ResNet14ResNet16ResNet182005006008001000Number of Private Data Samples30405060708090100Accuracy (%)Before GuardHFLGuardHFL (Q-priv)GuardHFL (Q-syn)2005006008001000Number of Private Data Samples203040506070Accuracy (%)Before GuardHFLGuardHFL (Q-priv)GuardHFL (Q-syn)CIFAR10SVHN30405060708090Accuracy (%)Before GuardHFLCutmixCutoutMixupUnder review as a conference paper at ICLR 2023

Figure 11: End-to-end secure prediction protocol. Green boxes represent linear layers (including
convolutional/fully connected/Avgpooling layers), and blue boxes represent non-linear layers (in-
cluding ReLU/Maxpooling layers).

computes [z]i = [x]i ± [y]i mod 2ℓ. Besides, multiplication operation, i.e., [z] = [x][y], is evalu-
ated using Beaver’s multiplication triples (Demmler et al., 2015), where each triple refers to (a, b, c)
with the constraint c = ab that is generated by cryptographic techniques (Demmler et al., 2015) or a
trusted dealer (Riazi et al., 2018). Specifically, the multiplication over secret-sharing values can be
evaluated in the following:

z = xy = ([x]0 + [x]1)([y]0 + [y]1) =

P0
(cid:122) (cid:125)(cid:124) (cid:123)
[x]0[y]0 +

P1
(cid:122) (cid:125)(cid:124) (cid:123)
[x]1[y]1 +[x]0[y]1 + [x]1[y]0

(3)

where for i ∈ {0, 1}, [x]i[y]i can be computed locally, and [x]i[y]1−i will be evaluated as follows.
Taking [x]0[y]1 as an example, assuming P0 and P1 already hold (a, [c]0) and (b, [c]1), respectively,
P0 first sends [x]0 + a to P1, while P1 sends [y]1 − b to P0. Then P0 computes one share as
[x]0([y]0 − b) − [c]0, and P1 computes another as ([x]1 + a)[y]1 − [c]1, locally. In this way, the
outputs are still in the form of secret sharing.

Diffie-Hellman key agreement protocol. In GuardHFL, we utilize PRFs to overcome the cross-
client communication limitation, where the consistent PRF seed between clients are generated using
the Diffie-Hellman Key Agreement (DH) protocol (Diffie & Hellman, 1976). Note that the consis-
tent seed between the server and the client can be directly sampled by the server and then sent to the
client without the DH protocol. Figure 12 gives the secure seed generation protocol Πseed. Formally,
the DH protocol consists of the following three steps:

• DH.param(k) → (G, g, q, H) generates a group G of prime order q, along with a generator

g, and a hash function H.

• DH.gen(G, g, q, H) → (xi, gxi) randomly samples xi ∈ Zq as the secret key and let gxi

as the public key.

• DH.agree(xi, gxj , H) → si,j outputs the seed si,j = H((gxj )xi).

Correctness requires that for any key pairs (xi, gxi ) and (xj, gxj ) generated by two paries
Pi and Pj using DH.gen under the same parameters (G, g, q, H), DH.agree(xi, gxj , H) =
DH.agree(xj, gxi, H). Besides, in GuardHFL security requires that for any adversary who steals
gxi and gxj (but neither of the corresponding xi and xj), the agreed secret si,j derived from those
keys is indistinguishable from a uniformly random value (Abdalla et al., 2001).

Figure 12: Secure PRF seed generation protocol ΠSeed

15

Conv(Protocol )[𝑦𝑦1]=𝜔𝜔1[𝑥𝑥]ReLU(Protocol)[𝑦𝑦2]=ReLU([𝑦𝑦1])MaxPooling(Protocol )[𝑦𝑦3]←maxpool([𝑦𝑦2])FC(Protocol )[logit]=𝜔𝜔𝑘𝑘[𝑦𝑦𝑛𝑛−1]…[𝑥𝑥]1[𝑥𝑥]0,[𝑦𝑦1]1[𝑦𝑦1]0[𝑦𝑦2]1[𝑦𝑦2]0[𝑦𝑦3]1[𝑦𝑦3]0[𝑦𝑦𝑛𝑛−1]1[𝑦𝑦𝑛𝑛−1]0,𝜔𝜔𝑘𝑘𝜔𝜔1[logit]1[logit]0ServerSecurePrediction𝑷𝑷𝑨𝑨ServerSample 𝑘𝑘𝑄𝑄∈ℤ𝑝𝑝𝑷𝑷𝑄𝑄𝑷𝑷𝐴𝐴Sample 𝑘𝑘𝐴𝐴∈ℤ𝑝𝑝𝐴𝐴=𝑔𝑔𝑘𝑘𝑄𝑄mod𝑝𝑝𝐵𝐵=𝑔𝑔𝑘𝑘𝐴𝐴mod𝑝𝑝𝐴𝐴𝑆𝑆𝑘𝑘𝑄𝑄𝐴𝐴=𝐷𝐷𝐷𝐷.𝑎𝑎𝑔𝑔𝑎𝑎𝑎𝑎𝑎𝑎(𝑘𝑘𝑄𝑄,𝐵𝐵,𝐷𝐷)mod𝑝𝑝𝐵𝐵,𝑆𝑆𝑘𝑘𝑆𝑆𝑄𝑄𝐴𝐴,𝑆𝑆𝑘𝑘𝑆𝑆𝐴𝐴𝑆𝑆𝑘𝑘𝑄𝑄𝐴𝐴=𝐷𝐷𝐷𝐷.𝑎𝑎𝑔𝑔𝑎𝑎𝑎𝑎𝑎𝑎(𝑘𝑘𝐴𝐴,𝐴𝐴,𝐷𝐷)mod𝑝𝑝Sample 𝑆𝑆𝑘𝑘𝑆𝑆𝑄𝑄,𝑆𝑆𝑘𝑘𝑆𝑆𝐴𝐴∈ℤ𝑝𝑝𝐵𝐵𝑆𝑆𝑘𝑘𝑆𝑆𝑄𝑄𝑆𝑆𝑘𝑘𝑆𝑆𝐴𝐴Under review as a conference paper at ICLR 2023

A.2.3 DISTINGUISH GU A R DHFL FROM OTHER PRIVATE SETTINGS.

GuardHFL is in line with the standard HFL paradigm (Li & Wang, 2019) with the additional ben-
efit of privacy protection. As declared in Introduction, GuardHFL is the first-of-its-kind privacy-
preserving HFL framework, which is different from existing privacy-preserving training efforts. The
latter can be divided into two categories: (1) privacy-preserving federated learning (Bonawitz et al.,
2017; Bell et al., 2020), and (2) secure multi-party training (Tan et al., 2021; Keller & Sun, 2022).
In the following, we give a detailed analysis.

Comparison to privacy-preserving federated learning. In the privacy-preserving federated learn-
ing (FL), each user locally computes the gradient with his private database, and then a secure ag-
gregation protocol is executed at the server side for aggregating the local gradients and updating the
global model. However, as shown in Introduction, secure gradient aggregation cannot be realized
in heterogeneous FL (HFL), due to the heterogeneity of the clients’ models. Instead, GuardHFL
follow a general HFL training paradigm (Li & Wang, 2019), which contains three steps: local train-
ing, querying, and local re-training. GuardHFL focuses on solving the privacy issue caused by the
querying stage, and mainly proposes a query datasets generation (refer to Section 3.4) and a secure
querying protocol (refer to Section 3.1 - Section 3.3).

Comparison to secure multi-party training. Secure multi-party training is typically an out-
sourced training setting, where resource-constrained clients outsource the entire training task to
non-colluding multiple servers in a privacy-preserving manner. It requires a secure training protocol
to finally yield a well-trained model. Different from secure multi-party training, GuardHFL enables
clients to collaboratively and securely train their own customized models that may differ in size and
structure. Moreover, as discussed above, the general HFL paradigm contains three steps:
local
training, querying and local re-training, where the local training and re-training stages are evaluated
locally without revealing privacy. Therefore, the privacy-preserving HFL requires an HFL-friendly
secure querying protocol (i.e., a customized inference protocol).

A.2.4 EXTEND EXISTING 2PC PROTOCOLS TO HFL

With non-trivial adaptation, existing secure 2-party querying schemes (Mishra et al., 2020; Rathee
et al., 2020; Huang et al., 2022) can be extended to the communication-limited HFL setting. How-
ever, as shown in Section 4.1, such extension introduces expensive communication and computation
overheads compared with our GuardHFL. In the following we divide these schemes into three cat-
egories, i.e., pure OT-based protocols, pure HE-based protocols, and hybrid protocols, and give the
corresponding extension designs.

To extend the pure OT-based secure querying protocols such as CrypTFlow2 (Rathee et al., 2020)
into HFL, PQ first secret-shares query samples to the server and PA using our protocol ΠShare in
Section 3.1. Then the server and PA execute secure prediction based on the method proposed in
Rathee et al. (2020). After that, adopting our secure aggregation protocol ΠAgg in Section 3.3, the
aggregated predictions will return to PQ. Although the OT-based schemes can be extended to HFL
by combining the designed protocols in GuardHFL, it requires too many communication rounds
due to the usage of OT primitives.

To extend the pure HE-based secure querying protocols (Gilad-Bachrach et al., 2016; Lee et al.,
2021) to HFL, PQ first encrypts the query samples and asks the server to pass them to PA. After
that, PA evaluates secure prediction non-interactively in the ciphertext environment. Then PA sends
encrypted predictions to the server. The server aggregates these encrypted predictions utilizing the
additive homomorphism of HE and sends the aggregated results to PQ. Although it is trivial to
extend the schemes equipped with the HE-based scheme to the communication-limited setting, they
have two key problems: 1) activation functions need to be approximated as low-degree polynomials,
which leads to serious accuracy loss; 2) due to the inherent high computation complexity, HE-based
secure prediction is difficult to extend to large-scale models.

For hybrid secure querying protocols that evaluates linear layers using HE and non-linear layers
using OT or GC, such as Cheetah (Huang et al., 2022), we discuss the extended algorithms for
each layer separately. For the linear layer, 1) PQ encrypts query samples with HE and sends the

16

Under review as a conference paper at ICLR 2023

ciphertext to PA through the server5. 2) PA evaluates linear layers locally, and returns the encrypted
masked outputs to PQ through the server. 3) PQ decrypts it to obtain the masked outputs, which are
then sent to the server. As a result, the masked outputs of linear layers are secret-shared between the
server and PA. For the non-linear layer, given that the server and PA hold shares of the linear layer’s
outputs, the two parties invoke the OT protocols to evaluate the non-linear functions. Therefore, such
extension comes at the cost of heavy computational and communication complexity.

In summary, although existing 2PC protocols can be extended to the HFL setting with the cross-
communication restriction, they sacrifice the efficiency due to the lack of customized protocols and
the adoption of heavy cryptographic primitives. Therefore, GuardHFL shows better adaptability
and efficiency in the natural HFL scenarios.

A.3 SECURITY ANALYSIS

Table 5: The ideal functionality

Input sharing functionality FShare:
• Input: PQ: query data x.
• Output: PA: [x]0 ∈ Z2ℓ. Server: [x]1 = x − [x]0 mod 2ℓ.

Matrix multiplication functionality FMatmul:

• Input: Server: [x]1 ∈ Z2ℓ. PA: [x]0 ∈ Z2ℓ, model parameter ω.
• Output: Server: [y]1 ∈ Z2ℓ. PA: [y]0 = ωx − [y]1 mod 2ℓ.

MSB functionality Fmsb:

• Input: Server: [x]1 ∈ Z2ℓ. PA: [x]0 ∈ Z2ℓ.
• Output: Server: [msb(x)]B

1 ∈ Z2. PA: [msb(x)]B

0 = msb(x) ⊕ [msb(x)]B

1 mod 2.

ReLU functionality FReLU:

• Input: Server: [x]1 ∈ Z2ℓ. PA: [x]0 ∈ Z2ℓ.
• Output: Server: [y]1 ∈ Z2ℓ. PA: [y]0 = ReLU(x) − [y]1 mod 2ℓ.

Maxpooling functionality FMaxpool:

• Input: Server: [x]1 ∈ Z2ℓ. PA: [x]0 ∈ Z2ℓ.
• Output: Server: [y]1 ∈ Z2ℓ. PA: [y]0 = Maxpool(x) − [y]1 mod 2ℓ.

Result aggregation functionality FAgg:

• Input: Server: [xi]1 ∈ Z2ℓ, i ∈ [C]. P i
• Output: PQ: y = softmax((cid:80)|C|
i=1 xi).

A: [xi]0 ∈ Z2ℓ.

Intuitively, GuardHFL only reveals the aggregated prediction to PQ without the responding parties’
model parameters, and the server and PA learn zero information about the querying parties’ data.
This is because all intermediate sensitive values are secret-shared. Next, we give a formal analysis.

Our security proof follows the standard ideal-world/real-world paradigm (Canetti, 2001): in the real
world, three parties (i.e., the server, PQ, and PA) interact according to the protocol specification,
and in the ideal world, they have access to an ideal functionality shown in Table 5. When a protocol
invokes another sub-protocol, we use the F-hybrid model for the security proof by replacing the sub-
protocol with the corresponding functionality. Note that our proof works in the FPRF-hybrid model
where FPRF represents the ideal functionality corresponding to the PRF protocol. The executions
in both worlds are coordinated by the environment Env, who chooses the inputs to parties and plays

5To be more precise, this step is for the input layer. In the hidden layer, one of the input shares of the linear

layer should be encrypted by the server and sent to PA.

17

Under review as a conference paper at ICLR 2023

the role of a distinguisher between the real and ideal executions. We will show that the real-world
distribution is computationally indistinguishable to the ideal-world distribution.
Theorem A.1. ΠShare securely realizes the functionality FShare in the FPRF-hybrid model.

Proof. Note that PQ and PA receive no messages in ΠShare, and hence the protocol is trivially secure
against corruption of PQ and PA. Next, the only message that the server receives is the value [x]1.
However, [x]1 = x − r, where given the security of PRF, r is a random value unknown to the server.
Thus, the distribution of [x]1 is uniformly random from the server’s view and the information learned
by the server can be simulated.

Theorem A.2. ΠMatmul securely realizes the functionality FMatmul in the FPRF-hybrid model.

Proof. Note that PQ receives no messages in ΠMatmul, and hence the protocol is trivially secure
against corruption of PQ. The only message that PA receives is the value [x]1 − b. However,
given the security of PRF, b is a random value unknown to PA. Thus, the distribution of [x]1 − b
is computationally indistinguishable from a uniformly random distribution in PA’s view, and the
information learned by PA can be simulated. Next, during the protocol, the server learns [c]1 and w+
a. However, the distribution of [c]1 and w + a is computationally indistinguishable from a uniformly
random distribution in the server’s view, since given the security of PRF, a and [c]1 are random
values unknown to the server. Thus, the information learned by the server can be simulated.

Theorem A.3. ΠReLU securely realizes the functionality FReLU in the (FMatmul, Fmsb)-hybrid model.

Proof. Note that as shown in Section 3.2, ΠReLU consists of Πmsb and ΠMatmul. Therefore, the ReLU
protocol is trivially secure in the (FMatmul, Fmsb)-hybrid model.

Theorem A.4. ΠMaxpool securely realizes the functionality FMaxpool in the FReLU-hybrid model.

Proof. As shown in Section 3.2, ΠMaxpool consists of several invocations of ΠReLU. Therefore, the
protocol ΠMaxpool is trivially secure in the FReLU-hybrid model.

Theorem A.5. ΠAgg securely realizes the functionality FAgg in the FPRF-hybrid model.

Proof. Note that PA receives no messages in ΠAgg, and hence the aggregation protocol is trivially
secure against the corruption of PA. Next, the only message that the server receives is the value
[xi]0 − ri. However, given the security of PRF, ri is a random value unknown to the server. Thus,
the distribution of [xi]0 − ri is computationally indistinguishable from a uniformly random distri-
bution in the server’s view and the information learned by the server can be simulated. After the
aggregation, PQ only learns the aggregated result (cid:80)
i∈[C] xi, but is unknown to each xi. Therefore,
the aggregation protocol is secure assuming the aggregation result will not reveal privacy.

A.4 RELATED WORK

A.4.1 HETEROGENEOUS FEDERATED LEARNING

Federated learning (FL) achieves collaboration among clients via sharing model gradients. While
successful, it still faces many challenges, among which, of particular importance is the heterogeneity
that appear in all aspects of the learning process. This consists of model heterogeneity (Li & Wang,
2019) and statistical heterogeneity (Zhu et al., 2021). Statistical heterogeneity means that parties’
data comes from distinct distributions (i.e., Non-IID data), which may induce deflected local opti-
mum. Solving the statistical heterogeneity has been extensively studied, such as Dinh et al. (2020);
Zhu et al. (2021); Yurochkin et al. (2019); Fallah et al. (2020); Yoon et al. (2021), and is out of the
scope of this work. However, GuardHFL may help alleviate the statistical heterogeneity due to the
customized model design and the knowledge distillation-based aggregation rule.

Our work mainly focuses on the model heterogeneity that has been explored in recent works (Li
& Wang, 2019; Lin et al., 2020; Choquette-Choo et al., 2021). In particular, Li & Wang (2019)
proposed the first FL framework FedMD supporting heterogeneous models by combining transfer
learning and knowledge distillation techniques. They first used a public dataset to pre-train the model

18

Under review as a conference paper at ICLR 2023

Table 6: Comparison with prior works on properties necessary for federated learning

Framework

Privacy

Usability

Efficiency

Data
Privacy

Model
Privacy

Model
Heterogeneity

w/o Dataset
Dependency

GPU
Compatibility

Protocol
Efficiency

Bonawitz et al. (2017)
Bell et al. (2020)
Sav et al. (2021)
Jayaraman & Wang (2018)

Li & Wang (2019)
Choquette-Choo et al. (2021)
Lin et al. (2020)
Sun & Lyu (2021)
Diao et al. (2021)
GuardHFL

✓
✓
✓
✓

✗
✓
✗
✗
✗
✓

✗
✗
✓
✗

✓
✓
✗
✓
✗
✓

✗
✗
✗
✗

✓
✓
✓
✓
✓
✓

✓
✓
✓
✓

✗
✗
✗
✗
✓
✓

✗
✗
✗
✗

✓
✗
✓
✓
✓
✓

✓
✓
✗
✓

-
✗

-
✓

-
✓

and transferred to the task of private datasets. After that, to exchange the knowledge, each party used
the public data and the aggregated predictions from others as carrier for knowledge distillation. To
further improve model accuracy, Lin et al. (2020) proposed FedDF, similar to FedMD, which also
used the model distillation technique for knowledge sharing. The difference is that they first per-
formed FedAvg on parties’ local models and integrated knowledge distillation on the aggregated
model. The dependence on model averaging leads to limited model heterogeneity. Besides, Diao
et al. (2021) focused on heterogeneous parties equipped with different computation and communi-
cation capabilities. In their framework, each party only updated a subset of global model parameters
through varying the width of hidden channels, which reduces the computation and communication
complexity of local models. However, this approach only learns a single global model, rather than
unique models designed by parties. Moreover, as described in Introduction, HFL suffers from sev-
eral privacy issues, which are not considered in the above works. The first is the direct leakage
of querying samples. The task-related querying samples may contain private information such as
disease diagnosis in healthcare. If the querying party directly delivers query samples to responding
parties for prediction via the server, such sensitive information will be leaked. The second is the
implicit disclosure based on inference attacks. Given black-box access to a model, adversaries can
infer the membership (Salem et al., 2019) and attribute information (Ganju et al., 2018) of the target
sample or even reconstruct the original training data (Yang et al., 2019). For example, based on the
prediction y of the querying sample x from the responding party PA, the querying party can launch
the membership inference attack (Salem et al., 2019) to infer whether x is a training sample of PA.
To address the privacy concern, GuardHFL provide end-to-end privacy-preserving HFL services.

The privacy-preserving techniques (i.e., secure aggregation) have been studied in federated learning
(Bonawitz et al., 2017; Bell et al., 2020; Sav et al., 2021; Jayaraman & Wang, 2018). However,
these techniques can not be directly extended to privacy-preserving HFL. More recently, Sun &
Lyu (2021) proposed a noise-free differential privacy solution for HFL to guarantee each party’s
privacy. However, as shown in Jayaraman & Evans (2019), there is a huge gap between the upper
bounds on privacy loss analyzed by advanced mechanisms and the effective privacy loss. Thus,
differentially private mechanisms offer undesirable utility-privacy trade-offs. To further formally
guarantee the privacy, Choquette-Choo et al. (2021) leveraged hybrid cryptographic primitives to
realize confidential and private collaborative learning. Specifically, parties learn from each other
collaboratively utilizing a secure inference strategy based on 2PC and HE protocols and a private
aggregation method. As noted in the Introduction, CaPC’s use of heavy cryptography leads to
significant efficiency and communication overheads.

In summary, we give a comparison between prior FL works and GuardHFL in Table 6.

A.4.2 SECURE NEURAL NETWORK PREDICTION

Since secure prediction is a critical component of GuardHFL, we briefly introduce its recent
progress. Neural networks present a challenge to cryptographic protocols due to their unique struc-

19

Under review as a conference paper at ICLR 2023

ture and exploitative combination of linear computations and non-linear activation functions. In real
scenarios, model prediction can be viewed as a two-party computation case, where one party with
private query data wants to obtain prediction results from the other party who owns the model. Dur-
ing the whole process, the cryptographic protocols, typically HE and secure multi-party computation
(MPC), are applied to ensure the confidentiality of the private data and model parameters.

Many existing works (Boemer et al., 2019b; Gilad-Bachrach et al., 2016; Brutzkus et al., 2019)
support pure HE protocols for secure predictions. Typically, nGraph-HE (Boemer et al., 2019b;a)
allows linear computations using the CKKS HE scheme. However, since a solution that builds
upon HE protocols should be restricted to compute low degree polynomials, the non-polynomial
activation functions, such as Maxpooling and ReLU, are forced to be evaluated in the clear by the
party who owns private query data. This leaks the feature maps, from which adversaries may deduce
the model weights. To solve this problem, Gilad-Bachrach et al. (2016) and Chen et al. (2019) use
low-degree polynomial approximation to estimate non-linear functions. Unfortunately, it will affect
the inference accuracy, while leading to huge computation overhead.

On the other hand, several libraries (Mohassel & Zhang, 2017; Knott et al., 2021; Wagh et al.,
2019) employ primarily MPC techniques in secure predictions, which provide linear and non-linear
protocols through the usage of oblivious transfer (OT), garbled circuit (GC) and secret sharing. In
particular, CryptTen (Knott et al., 2021) performs linear operations based on n-out-of-n additive
secret sharing over the ring Z2l . However, it requires a trusted third party to assist the secure pre-
diction process, which is unrealistic in the real-world setting. CrpytGPU (Tan et al., 2021) builds
on CrypTen, working in the 3-party setting using replicated secret shares. Although the scalability
is poor, it introduces an interface to losslessly embed cryptographic operations over secret-shared
values in a discrete somain into floating-point operations, which can implement the whole inference
process on the GPU. Recently, Keller & Sun (2022) proposed a secure quantized training protocol
that outperforms CryptGPU in the cryptographic performance. Unfortunately, this work cannot be
applied in HFL and is not comparable to GuardHFL. The main reasons are: (1) GuardHFL and
Keller & Sun (2022) are concerned with completely different tasks. GuardHFL builds on the stan-
dard HFL setting, where multiple parties collaboratively train individual models with the assistance
of a server. Keller & Sun (2022) focuses on the outsourced training scenario, i.e., multiple servers
jointly execute standard model training algorithm to obtain a well-trained model. (2) Moreover, the
protocols in Keller & Sun (2022) are designed for a specific network architecture, i.e., quantized
neural networks, which cannot be applied to the general models in GuardHFL. Therefore, Keller &
Sun (2022) and GuardHFL are two fully orthogonal works.

Many other works focus on hybrid protocols, in which they combines the advantages of HE and
MPC to improve prediction efficiency (Juvekar et al., 2018; Mishra et al., 2020; Rathee et al., 2020;
Huang et al., 2022). CrypTFlow2 (Rathee et al., 2020) implements two class of protocols, HE-
based and OT-based, for linear operations. For non-linear layers, they also design efficient protocols
based on OT. Cheetah (Huang et al., 2022) improves CrypTFlow2 with customized HE-based linear
protocols and improved OT-based non-linear protocols. HE-transformer (Boemer et al., 2019a)
employs nGraph-HE for evaluation of linear operations, and ABY framework for GC to evaluate
non-linear functions (Demmler et al., 2015). Since non-linear operations cannot be parallelized
between query data, GC is inefficient, especially for large networks with thousands of parameters.
In contrast, our GuardHFL avoids the use of heavy cryptographic tools like HE and OT, and only
employs secret sharing and PRFs to achieve high efficiency, confidentiality and practicability.

20

