Under review as a conference paper at ICLR 2023

PROTEIN STRUCTURE REPRESENTATION LEARNING
THROUGH ORIENTATION-AWARE GRAPH NEURAL
NETWORKS

Anonymous authors
Paper under double-blind review

ABSTRACT

By folding to particular 3D structures, proteins play a key role in living beings.
To learn meaningful representation from a protein structure for downstream tasks,
not only the global backbone topology but the local fine-grained orientational re-
lations between amino acids should also be considered. In this work, we pro-
pose the Orientation-Aware Graph Neural Networks (OAGNNS) to better sense
the geometric characteristics in protein structure (e.g. inner-residue torsion an-
gles, inter-residue orientations). Extending a single weight from a scalar to a
3D vector, we construct a rich set of geometric-meaningful operations to pro-
cess both the classical and SO(3) representations of a given structure. To plug
our designed perceptron unit into existing Graph Neural Networks, we further
introduce an equivariant message passing paradigm, showing superior versatility
in maintaining SO(3)-equivariance at the global scale. Experiments have shown
that our OAGNNs have a remarkable ability to sense geometric orientational fea-
tures compared to classical networks. OAGNNSs have also achieved state-of-the-
art performance on various computational biology applications related to protein
3D structures.

1 INTRODUCTION

Built from a sequence of amino-acid residues, a protein performs its biological functions by folding
to a particular conformation in 3D space. Therefore, untilizing such 3D structures accurately is the
key for downstream analysis. While we have witnessed remarkable progress in protein structure

predictions (Rohl et al.|/2004}/Kallberg et al.}/2012) 2021 2021), another
S protein

thread of tasks wi D structures as input starts to draw a great interest, such as function
prediction (Hermosilla et al. 2020} Gligorijevi¢ et al. 2021). decoy ranking (Lundstrom et al.|)2001
Kwon et al.|/2021}|Wang et al.{]2021), protein docking (Duhovny et al. aa Shulman-Peleg et i
[Gainza et al.||2020}/Sverrisson et al.|[2021), and driver mutation identification (Lefevre et al.
Li et al|[2020} Jankauskaité et al, [2019).

Most existing works in modeling protein structures directly borrow models designed for other ap-

plications, including 3D-CNNs in computer vision, Transformers
[2017)) from natural language processing, and GNNs (Kipf & Welling}/2016) in data mining. Though
compatible with general objects, these models have overlooked the subtleties in the fine-grained ge-
ometries, which are much more essential in protein structures. For instance, given an amino acid in
the protein structure, as shown in Figure[]] the locations of four backbone atoms (carbon, nitrogen,
and oxygen) determine a local skeleton, and different residues interact with each other through per-
forming specific orientations between their local frames, either of which have important impacts on

the protein structure and its function (Nelson et al.||2008).

Recent attempts in building geometric-aware neural networks mainly focus on baking 3D rigid
transformations into network operations, leading to the area of SO(3)-invariant and equivariant
networks. One representative work is the Vector Neuron Network (VNN) (Deng et al.|2021), which
achieves SO(3)-equivariance on point clouds by generalizing scalar neurons to 3D vectors. An-
other work is the GVP-GNN (Jing et al.|[2021) that similarly vectorizes hidden neurons in GNN and
demonstrates better prediction accuracy on protein design and quality evaluation tasks. However,

Under review as a conference paper at ICLR 2023

(b) ye
NN. 4 \
=> = wt
Input Protein "

Residuei |

GREE => yorepn @ => yrode

Figure 1: Overview (a) Each amino acid has its own rigid backbone with four heavy atoms, and
can be represented by both lists of scalar and vector features. (b) Tasks associated with the protein
3D structure. Graph-level tasks consider the whole protein structures, and Node-level tasks operate
on specific residues.

these two models can only adopt linear combinations of input vectors, which significantly limits
their modeling capability. A simple example is that, given two input vector features v; and ve, the
outputs w1v1 + w2v2 through one linear layer is constrained in the 2D plane spanned by vj, v2
even after applying their scalar-product non-linearities. That is, WNN-based models are limited in
perceiving orientational features, which have been proven crucial for proteins to perform their func-
tions and interact with other partners (e.g. inner-residue torsion angles, inter-residue orientations)

(Nelson etal] 2008) et & Voei 2010) Xu & Berger| (2006) [Alford eta. 2077).

To achieve more sensitive geometric orientation awareness, we propose a Directed Weight (Ww) per-
ceptrons by extending not only the hidden neurons but also the weights from scalars to 3D vectors,
naturally saturating the entire network with 3D structure information in the Euclidean space. Di-
rected weights support a set of geometric-meaningful operations on both the vector neurons (vector-
list features) and the classical (scalar-list) latent features, and perform flexible non-linear integration
of the hybrid scalar-vector features. As protein structures are naturally attributed proximity graphs,
we introduce a new Equivariant Message Passing Paradigm on protein graphs, to connect the Ww-
perceptrons with the graph learning models by using rigid backbone transformations for each amino
acid, which provides a versatile framework for bringing the biological suitability and flexibility of
the GNN architecture.

To summarize, our key contributions include:

¢ We propose a new network unit based on the Directed Weights for capturing fine-grained
geometric relations, especially for the subtle orientational details in proteins.

¢ We construct an Equivariant Message Passing paradigm based on protein graphs.

¢ Our overall framework, the Orientation-Aware Graph Neural Networks, is versatile in
terms of compatibility with existing deep graph learning models, making them biologically
suitable with minimal modifications to existing GNN models.

2 RELATED WORK

Representation learning on protein 3D structure. Early approaches rely on hand-crafted fea-
tures extracted and statistical methods to predict function annotations (Schaap et al.|{2001}/Zhang &
|Zhang}/2010). Deep learning has been found to achieve success then. 3D CNNs are first proposed to
process protein 3D structures by scanning atom-level features relying on multiple 3D voxels. One of
the representative works (Derevyanko et al.||2018) adopts a 3D CNN-based model for assessing the
quality of the predicted structures. 3D CNNs also shed light on other tasks such as interface predic-
tion (Townshend et al.|{2019}/Amidi et al.|/2018). People also extend them to spherical convolutions
(Gainza et al.|/2020}|Sverrisson et al.||2021{|Hermosilla Casajus et al.|/2021), to the Fourier space
(Zhemchuzhnikov et al.|{2022) and the 3D Voronoi Tessellation space (Igashov et al. 2021). Graph
Convolutional Networks (Kipf & Welling] |2016) have also been adopted to capture geometric and
i

biochemical interactions between residues (Ying et al.|/2018 2019} |F
been shown to achieve great performance on function prediction (Li et al.| |2021), protein design

(Strokach et al.|[2020) and binding prediction {Vecchio et al.| . Recently, transformer-based

Under review as a conference paper at ICLR 2023

ther bioinformatics

methods {Vaswani et al.|[2017) have a trend to replace conventional methods i
tasks (Ingraham et al.}/2019 :

Equivariant neural networks. Equivariance is an important property, for generalizing to Tat

representations for achieving rotation 3 in 3D. In comparison to Tensor Filed Network
with complex tensor products, the Vector Neuron Network (VNN) achieves rotation equivariance
in a much simpler way, which generalizes the values of hidden neurons from scalars to 3D vectors

(Deng et al.| 2021). GVP-GNN has also been a OT. for learning protein representations by
featuring geometric factors as vectors (Jing et al.| Message passing in GNNs for vector
eu

representations using equivariant features have also been explored (Schutt et al, (Schiitt et al. {2021} [Luo et al.

. However, to guarantee rotation equivariance, they can only linearly combine 3D vectors, in
essence, limiting their geometric representing capacity. Another line of methods condition filters on

invariant scalar features for maitaining equivariance (Schiitt et al.| 2017} Gasteiger et al. 2019a} Liu
).

3 DIRECTED WEIGHT PERCEPTRON

(a) Directed Linear 7 - (b) Non-Linearity (c) Directed Interaction

sigmoid(|| |) Via (W,)ov"

Scale

ve
output str
'
st ster” |
'
' '
ReLU(-) -th = H
( vow 7 O
Bo ccuram > Vector-Tnsor Mat

Figure 2: Model Details. A 1-layer DWP consists of three following modules. Both the input,
hidden and output are tuples. (a) Directed Linear Module applies multiple geometric operations
to update scalar and vector features in four different ways with normal and directed weights. (b)
Non-Linearity Module employs ReLU and sigmoid functions for scalar and vector features. (c)
Directed Interaction Module updates the features by using one another as updating parameters
after non-linearity in another way.

A protein is modeled as a KNN-graph G = (V,€) where each node u € V corresponds to one
amino acid, characterized by a scalar-vector tuple hy, = (8u, Vu) with s, € RO,V, € ROX,
and the edges are constructed spatially by querying its k-nearest neighbours in the space. The edge
features € = {ei; Jig j» which represent edge connected node i and j, are also multi-channel scalar-
vector tuples. For simplicity, we set the channel numbers for scalar and vector features as the same,
but they can be different. In practice, scalar, vector features are constructed based on a given protein
structure and are kept separate. To make them compatible, the channel numbers of scalars are larger
than the vectors. Details can be found in the Appendix Section B-1]

3.1 DIRECTED WEIGHTS
Classical neural networks only consider scalar-list features of the form s € R©, with each layer
transforming them with a weight matrix W € R© *© and a bias term b € R©:

s’=Ws+b (1)

Although has been proved to be a universal approximator, such a layer has intrinsically no geometric
interpretation in the 3D shape space, and even the simplest 3D rigid transformation on the input can
result in unpredictable changes of network outputs. Recent attempts have lifted neuron features from
Under review as a conference paper at ICLR 2023

lists to vector-lists V € R©*? with a linear operation mapping to V’ € ROx3
V’'=WV (2)

Under this construction, SO(3)-actions in the latent space are simply matrix multiplications, which
establishes a clear 3D Euclidean structure. However, to maintain input-output consistency under
rigid transformations (e.g. rotation and translation), their operations are still limited to classical lin-
ear combinations weighted by W. To define operations that are more adaptive to the geometrically
meaningful features, we introduce the Directed Weights, that we can define any tensor with the last
dimension of 3 as a directed weight matrix, for example, W e€ RO XC%3, seen as a stack of geo-
metric meaningful vectors lying in the 3D Euclidean Space, and can be acted by SE(3) group from
the right.

3.2 W-OPERATORS

With weights and features both equipped with 3D vector representations, we can design a set of

geometric operators L] (e.g. - or x) with learnable W as parameters in neural networks, which can
operate on both scalar-list features s € R© and vector-list features V € RC

Wos, WOV (3)

Geometric vector operations. Let W © RC'XC be a conventional scalar weight matrix, Ww, E

RO'*C*3 and Wz € RO*® be directed weight tensors. Beyond linear combinations, We can define
two operations that leverage geometric information:

Sto(V; Wi) = Wi-V eR (4)
(V;W, We) = W(W2xV) eRO* (5)
Here sj, transforms C’ vector features to C’ scalars using dot-product with directed weights, de-
tailedly, s; = > ik Ww,” * VI, which explicitly measures angles between vectors, and this opera-
tor brings network the ability to accurately sense the orientational features. In V/,,, a vector crosses a
directed weight before being projected onto the hidden space. While the output of plain linear com-
binations between two vectors v; and v2 can only lie in the plane w)v; + w2v2, cross-product in
EquationD]ereates a new direction outside the plane, which is crucial in 3D modelling. For instance,
the side-chain angles of a given residue could be largely determined by its C, and Cg vectors, but
may lie on the direction perpendicular to the space constructed by the two vectors.

v!

crs

Scalar lifting. In addition, a directed weight W € R©*3 can lift scalars to vectors by adopting the
following operation, the ij th entry of sW is s;Wi;

Vials; W) =sw eRx3 6)

This maps each scalar to a particular vector, enabling inverse transformations or information bottle-
necks from R to R?. An intuition example is that, we can map a scalar representing the distance
between two amino acids to a vector pointing from one amino acid to the other, leading to more
biological meaningful representations for the protein fragment.

Linear combinations. In the end, we keep the linear combination operations with scalar weights
for both scalar and vector features:
sin(s;W)=Ws €R- (7)
(V;W)=WV €RO*3 (8)
While these operators enables a more flexible network design, equivariance to rigid transformations
is broken if considering more complex functions beyond linear combinations. We will introduce a
globally equivariant paradigm to tackle this issue in Section [4]

3.3. W-PERCEPTRON UNIT

Now we combine all the W-operators together, assembling them into what we call Directed Weight
Perceptrons (W-perceptrons). A W-perceptron unit is a function mapping from u = (s, V) to
Under review as a conference paper at ICLR 2023

another scalar-vector tuple u’ = (s’, V’), which can be stacked as network layers to form multi-
layer perceptrons.

A single unit comprises three modules in a sequential way: the Directed Linear module, the Non-
Linearity module, and the Directed Interaction module (Figure[2).

Directed linear module. The output hidden representations derived from the set of operations in-
troduced previously are concatenated and projected into another hidden space of scalars and vectors
separately:

s! = WolShou Stal ERC (9)
Vv" = WylVi., Vis Viel EROS (10)

crs?

Here W, € ROx(C'+C) and Wve RO x(C'+C+C’) are scalar weight matrices. In other word,
the five separate updating functions allow transformation from scalar to scalar (Equation[7p, scalar
to vector (Equation|6), vector to scalar (Equation|4), and vector to vector (Equation|8} Equation|5),
boosting the model’s capability to reason in 3D space.

Non-linearity module. We then apply the non-linearity module_to_the hidden_representation
(s’, V"). Specifically, we apply standard ReLU non-linearity (Nair & Hinton] to the scalar
components. For the vector representations, following (Weiler et al.]/2018} ti
[202 1), we compute a sigmoid activation on the L2-norm of each vector and multip
to the vector entries accordingly:

s’ — ReLU(s"), ve e ve - sigmoid(||v?||2) e8D)

where v;' € R® are the vector columns in V" and v?., are their entries.

Directed interaction module. Finally we introduce the Directed Interaction module, integrating the
hidden features s” and V" into the output tuple (s’, V’)

si=s'+Wy-Vv" ER (12)
v= (s*W,) ovr € RO x3 (13)

Here Wy, Wg are directed weight matrices with sizes C’ x C’ x 3 and C’ x 3, respectively, © de-
notes element-wise multiplication for two matrices. This module establishes a connection between
the scalar and vector feature components, facilitating feature blending. Specifically, Equation [12|
dynamically determines how much the output should rely on scalar and vector representations, and
Equation[I3]weights a list of vectors using the scalar features as attention scores.

4 ORIENTATION-AWARE GRAPH NEURAL NETWORKS

To achieve SO(3)-equivariance without the loss of modeling capacity, we introduce an equivariant

message passing paradigm on protein graphs, to easily plug our versatile W-Perceptron into any
existing graph learning framework, which makes network architectures free from equivariant con-
straints (Section (4-1). The integrated models, called Orientation-Aware Graph Neural Networks,
can not only accurately model the essential orientational features but also maintain the rotation
equivariance efficiently. We also design multiple variants in analogy to other graph neural networks
Section

4.1 EQUIVARIANT MESSAGE PASSING ON PROTEINS

A function f : R? > R® is SO(3)-equivariant (rotation-equivariant) if any rotation matrix R €
R°*3 applied to the input vector a leads to the same transformation on the output f (2):

Rf (a) = f(Ra) (14)

Such equivariant property can be achieved on a protein graph with local orientations, which is nat-

urally defined from its biological structure (Ingraham et al.}|2019). Specifically, each amino acid
Under review as a conference paper at ICLR 2023

node u; € V has four backbone heavy atom (C%, C“, N“, O"), defining a local frame O,, as:

ay,=N*-CXeR?, y,=C"'-Ct eR (15)

+
Lu Yu Lu Yu

; x
W[eull2’ [lyullo’ llaeulle ~ [lalla
The local frame O,, is a rotation matrix that maps a 3D vector from the local to the global coor-
dinate system. An equivariant message passing paradigm then emerges through transforming node
features back and forth between adjacent local frames. Formally, give an amino acid wu with hidden
representation h = (s, V), let f, and f, be transformations on the vector feature V from and to the
global coordinate system:

Ou €R*3 (16)

filh,O,) = (8,VO!), fy(h,O,) = (s,VO,) a7)
The message passing update for node u from layer / to layer / + 1 is performed in the following
steps, the [ | notations indicates the concatenation of different hidden representations along the

channel-wise dimension:
1. Transform the neighbourhood vector representations of u into its local coordinate system
Ou.
2. For each adjacent node v, compute the message m!+! on edge (u,v) € € with an multi-
layer w- -perceptron F to the aggregated node and edge feature [h!,, hi,, e!,,,] (Equation|18).
3. Update node feature h!, with another multi-layer W->perceptron H, and transform it back
to the global coordinate system (Equation[19).

This paradigm achieves SO(3)-equivariance in a very general sense with no constraints on F, H.
The formal proof of equivariance is presented in the Appendix Section[A]

mitt = SO FA (i Mh, evel Ou) (18)
veNu
hit? = fy(H(hi,, mi"), Ou) (19)

4.2 VARIANTS OF ORIENTATION-AWARE GRAPH NEURAL NETWORKS

By integrating the W-Perceptrons with equivariant message passing, we propose multiple variants
of entire Orientation-Aware Graph Neural Networks to boost the design space for different tasks.

OA-GCN. This is the one described and implemented in the previous subsection with Equation 18]
and Equation[19]

OA-GIN. We adopt graph isomorphism operator|Xu et al.](2018) with learnable weighing parameter
é for tunable skip connections,

mitt = (1+e)fi(hl,, Ou) + S> Fl filhi, Ou) (20)
veNu
OA-GAT. In comparison to GCN and GIN, it is not trivial to incorporate with the Graph Attention

Network (GAT) (Velickovicé et al.|/2018), as residues interact with each other unevenly in proteins.

In particular, we define separate attentions for the scalar and vector representations

(0, Vi) = filhi,, Ou) (21)

s= YO aist, Viz YO atv (22)
veN,U{u} VEN, U{u}

hit = fy(H((s',V')), Ou) (23)

The attention scores a*, a” are the softmax values over the inner products of all neighboring source-
target pairs defined as the follows:

: exp ((8u, 8v)) ; exp (tr(V," V.))
ae, , a (24)
ewe. U{u} EXP ((Su; Sw)) Se wewu{uy exP (t2(V, Vi)

The product for scalars and vectors are standard inner product and Frobenius product, respectively.
Under review as a conference paper at ICLR 2023

5 EXPERIMENTS

To demonstrate the basic rationale in perceiving angular features of our W-Perceptron, we design
a synthetic task (Section 5.1). We then conduct experiments on multiple benchmarks, including
node-level tasks: Residue Identification (RES) (Section[5.2), Computational Protein Design (CPD)
(Section 5.3), and graph-level tasks: Model Quality Assessment (MQA) (Section |5.4). Ablation
studies are also included (Section[5.5). More details are documented in the Appendix Section [B]

5.1 SYNTHETIC TASK

a). b).

100000 samples

Positive

Negative

0100 200 300 400 500 600 700 800
Epoch

Figure 3: a). The synthetic study. The vector in red is the anchor used to calculate the ground truth
labels. Vectors falling into the cone are positive samples, whereas outside points are negative. b).
The validation Binary Cross Entropy loss of compared methods.

Datasets. We test the performance of different perceptron designs on a synthetic binary classifica-
tion dataset with angular. We sample a random unit vector uv, € R® as the anchor vector. Then
each vector v; in the space is labeled positive if its angle between the anchor vector am; < 7/4,
and negative otherwise. We generate 100,000 random vectors with 50% positive samples and 50%
negative samples. For a fair comparison, we restrict the number of parameters and hyperparameters
of different models to approximately the same.

Results. As shown in Figure|3] in general, our W-Perceptron (DWP) converges faster than other
models and achieves the lowest validation loss. We also notice that the Geometric Vector Perceptron
(GVP) cannot converge given a sufficient number of epochs due to the poor capability of perceiving
angulars by only using linear combinations of vector features. In comparison to GVP, Vector Neural
Network (VNN) performs better based on its non-linear ReLU operation for vector features. Sim-
ple Multi-Layer Perceptron (MLP) can also capture useful information according to its universal
approximation ability. The superior performance demonstrates that our W-Perceptron has a better
ability on perceiving potential geometric features in space.

5.2 RESIDUE IDENTIFICATION

Datasets. Residue Identification (RES) aims to predict the identity of particular masked amino acid
based on its local surrounding structure (Torng & Altman| . We download 1000, 000 (10°)
substructures from the ATOM3D project (Townshend et al.|/2021), which are originally derived from
574 proteins from the PDB (Berman et al.|/2000). The entire dataset is split into training, validation,
and testing datasets with the ratio of 80%, 10%, and 10%. There are no two proteins with similar
structures in the test and non-test datasets based on the CATH 4.2 topology class at the domain level

(Orengo et al|[1997).

Metrics. We use classification accuracy to evaluate model performance.

Baselines. The 3D-CNN encodes the positions of the atoms in a voxelized 3D volume. We also
compare GNN variants such as GCN , GIN and GAT with our OA-GCN, OA-GIN and OA-GAT
models. We also include GVP-GNN as well.

Results As shown in Table|1| GCN, GIN and GAT almost cannot accurately identify the type of
amino acids, suggesting the lack of ability of conventional GNN models to capture 3D geometric
information. 3D-CNN performs better due to the power of the voxelization technique. Our models
Under review as a conference paper at ICLR 2023

outperform other models, suggesting OA-GNNs could help represent protein substructure geome-
tries better.

Table 1: Results of different RES methods on ATOM3D. Table 2: Ablation studies.
Model | 3D-CNN GCN GIN GAT GVP-GNN OA-GCN OA-GIN OA-GAT Model | NoDW No Int No Equi
Ace % | 45.1 82. 91 124 48.2 50.2 50.8 49.2 Acc% | 473 47.7 33.0

5.3 COMPUTATIONAL PROTEIN DESIGN

Datasets. Computational Protein Design (CPD) predicts the native protein sequence of a given

backbone structure. Specifically, we focus on two databases: CATH 4.2 (Ingraham et al.| |2019)
organizes proteins in a hierarchical structure (Orengo et al.| Following prior works, there

are 18,024 protein chains in the training set, 608 in the validation set, and 1, 120 in the test. TS50
dataset is a relatively old benchmark consisting of only 50 protein structures widely
used in biology communities.

Metrics. Native Sequence Recovery Rate is adopted to evaluate the prediction performance, which
compares the predicted sequence with the ground truth sequence at each position and calculates
the proportion of the correctly recovered amino acids ( The perplexity score
linek et al.||1977) evaluates whether a model could assign a elihood to the test sequences

ig)

ferent CPD methods on the Table 4: Results of MQA models on CASP

CATH 4.2. 11 stage 2.
Model Perplexity | Recovery % t Model ‘Average + Global +
Short Single All Short Single All rp. OF 7 pT
St-Transformer 8.54 9.03 6.85 28.3 276 36.4 VoroMQA 0.42 0.41 0.29 0.65 0.69 0.51
St-GCN 831 8.88 6.55 284 28.1 37.3 RWpls 0.17 0.19 0.13 0.06. 0.03 0.01
St-GIN 8.03 8.52 6.15 27.7 284 38.1 SBROD 0.43. 0.41 0.29 0.55 0.57 (0.39
St-GAT 10.86 10.67 9.89 26.2 26.8 35.2 Prog3D —-0.44.-—«0.43.-«0.30— 0.77 (0.80 0.59
GVP-GNN 7107.44 5.29 32.1 32.0 40.2 3DCNN 0.49. (0.39-«(0.27,— (0.64. (0.67 0.48
OA-GCN 542.569 394 396 385 475  Ormate = 0.39 0.37 0.26 0.63, 0.67 0.48
* 5 DimeNet 0.30 0.35 0.28 © 0.61.-—«0.62-—«0.43
OA-GIN 5.84 5.39 3.85 388 40.1 47.8
OA-GAT 502 553 413 375. 303467 GraphQA 0.48 0.40 042 0.75 0.72 0.74
: Pe : : : GVP-GNN 0.58 0.33 046 0.80 0.61 0.81
No DW 652 6.79 5.28 36.7 35.0 42.9 OA-GCN 0.62 037 051 0.84 0.65 0.85
No Int 645 6.36 4.87 37.2 36.3 44.9 OA-GIN 0.63 0.36 052 0.86 0.67 0.88
No Equi 6.21 6.04 4.64 36.9 37-1 45.8 OA-GAT 0.65 0.42 0.53 0.83 (0.69 0.86
Table 5: Results of structure biology methods on the TS50.
Model | Rosetta SPIN ProteinSolver — Wang’s SPIN2  SBROF ProDCoNN _ St-Trans
Recv % | 30.0 30.3 30.8 33.0 33.6 39.2 40.7 42.3
Model | DenseCPD GVP-GNN OA-GCN OA-GAT OA-GIN No DW No Int No Equi
Recv % | 50.7 44.9 53.8 54.5 52.7 46.8 48.7 49.5

Baselines. For the CATH 4.2 dataset, we compare our models with other state-of-the-art models,
including GVP-GNN, Structured Transformer and Structured GNN (Ingraham et al.|[2019).
compare multiple machine learning approaches in the TS50 dataset, including 3D CNN-based meth-

ods (ProDConn (Zhang et al.|/2020), DenseCPD (Qi & Zhang}|2020)), sequential models (Wang’s
model SPIN (Li et al|| ,

Results. As shown in Table|3} our models achieve significant improvement over other methods on
the CATH dataset. This result also suggests that the the short-chain and single-chain subsets are

more challenging, which is consistent with the result in (Ingraham et al.||2019). As shown in Table
at

B} our models also gain superior performance. These results suggest that adopting OA-GNNs is a
more effective way of designing valid protein sequences.

5.4 MODEL QUALITY ASSESSMENT

Datasets. Model quality assessment (MQA) evaluates the quality of predicted protein structures
(Kwon et al.}|2 The goal is to fit a model approximating the numerical metric used to compare

"As there is no canonical training set for TS50, we follow prior methods to use protein sequenc:
than 30% similarity with the TS5O test set from the CATH training set for training (Jing et al

ith less

Under review as a conference paper at ICLR 2023

the predicted 3D structure with the native 3D structure. For this task, We randomly split the targets
in CASP5-CASP10 (Moult et al|] (Mout et al] POT) (2014) and sample 50 decoys for each target for generating the
training and validation sets and use the CASP11 stage 2 proteins as the test set to ensure no similar
structures are involved. In total, there are 508 proteins for training, 56 for validation, and 85 targets
with 150 decoys for the test.

Metrics. We regress the global GDT_TS score (Zemla}/2003), which evaluates the quality of decoys.
We adopt the correlation between the predicted and real GDT_TS values to measure the performance.
We use three statistical correlation metrics: Pearson’s correlation r, Spearman’s p, and Kendall’s 7.
We calculate the correlation for each target and average all the correlations over all the targets. We
also calculate the Global correlations by taking the union of all decoy sets without considering the

targets (Pages et al.|/2019).

Baselines. VoroMQA (Olechnovié & Venclovas| |2017) leverages potential model with protein
contact maps, while RWplus (Zhang & Zhang] |2010) relies on physical enerey terms. SBROD
uses hand-crafted features. Proq3D (Uzi employs a FCNs
for regression. 3DCNN (Derevyanko et al.|/2018) and Ornate { apply 3D CNNS
for extracting meaningful features. Grap! ti A (Baldassare et al] bie Net (Gasteiger et al]
and GVP-GNN are the most similar models as ours which med GNN- ee See on
protein graphs.

Results. As shown in table [4] our models outperform other methods on both average and global
metrics except for Spearman’s correlation p where we achieve the second-best, demonstrating our
models can not only evaluate the quality for the same protein but also works well across different
proteins in a more general way.

5.5 ABLATION STUDIES

In the end, we conduct ablation experiments to study the contribution of different modules of our
model on the tasks of CPD (table|3]|5) and RES (table[2). More specifically, based on OA-GCN, we
replace all the directed weights with the regular linear layer with scalar weights (No DW), or remove
the Interaction Module (No Int), or break the equivariance by canceling coordinate transformations
in the step of message passing (No Equi).

In general, all three components of our model are important since removing each of them results
in a significant performance drop. According to the performance gain, the directed weights are the
most important module for the CPD task, as the orientational features are essential for the protein to
fold to a given structure. For Residue Identity, however, equivariance message passing is the most
important component, indicating the necessity of rotation equivariance in learning the representation
of local protein structures.

6 CONCLUSION

For the first time, we generalize the scalar weights in neural networks to 3D directed vectors for
better perceiving geometric features like orientations and angles, which are essential in learning rep-
resentations from protein 3D structures. Based on the directed weights W, we provide a toolbox
of operators as well as a W-Perceptron Unit encouraging efficient scalar-vector feature interac-
tions. We also enforce global SO(3)-equivariance on a message passing paradigm using the local
orientations naturally defined by the rigid property of each amino acid residue, which can be used
for designing more powerful networks. Our integrated Orientation-Aware Graph Neural Networks
achieve comparably better performances on multiple biological tasks in comparison with existing
state-of-the-art methods.

Limitations and future work. While our method shows better awareness to fine geometric details,
there remains a huge space for non-linearity designs upon geometric representations. In the future,
we will extend our framework to other 3D learning tasks like small molecules or point clouds by
learning local rotation transformations and using directed vector weights.
Under review as a conference paper at ICLR 2023

REFERENCES

Rebecca F Alford, Andrew Leaver-Fay, Jeliazko R Jeliazkov, Matthew J O’ Meara, Frank P DiMaio,
Hahnbeom Park, Maxim V Shapovalov, P Douglas Renfrew, Vikram K Mulligan, Kalli Kappel,
et al. The rosetta all-atom energy function for macromolecular modeling and design. Journal of
chemical theory and computation, 13(6):3031-3048, 2017.

Ethan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church.
Unified rational protein engineering with sequence-based deep representation learning. Nature
methods, 16(12):1315-1322, 2019.

Afshine Amidi, Shervine Amidi, Dimitrios Vlachakis, Vasileios Megalooikonomou, Nikos Para-
gios, and Evangelia I Zacharaki. Enzynet: enzyme classification using 3d convolutional neural
networks on spatial representation. PeerJ, 6:e4750, 2018.

Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural
networks. Advances in neural information processing systems, 32, 2019.

Nina M Antikainen and Stephen F Martin. Altering protein specificity: techniques and applications.
Bioorganic & medicinal chemistry, 13(8):2701-2716, 2005.

Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie
Lee, Jue Wang, Qian Cong, Lisa N Kinch, R Dustin Schaeffer, et al. Accurate prediction of
protein structures and interactions using a three-track neural network. Science, 373(6557):871-
876, 2021.

Federico Baldassarre, David Menéndez Hurtado, Arne Elofsson, and Hossein Azizpour. Graphqa:
protein model quality assessment using graph convolutional networks. Bioinformatics, 37(3):
360-366, 2021.

Simon Batzner, Tess E Smidt, Lixin Sun, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari,
and Boris Kozinsky. Se (3)-equivariant graph neural networks for data-efficient and accurate
interatomic potentials. arXiv preprint arXiv:2101.03164, 2021.

Tristan Bepler and Bonnie Berger. Learning protein sequence embeddings using information from
structure. In International Conference on Learning Representations, 2018.

Helen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig,
Ilya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic acids research, 28(1):
235-242, 2000.

Yue Cao, Payel Das, Vijil Chenthamarakshan, Pin- Yu Chen, Igor Melnyk, and Yang Shen. Fold2seq:
A joint sequence (1d)-fold (3d) embedding-based generative model for protein design. In Inter-
national Conference on Machine Learning, pp. 1261-1271. PMLR, 2021.

Jianlin Cheng, Myong-Ho Choe, Are Elofsson, Kun-Sop Han, Jie Hou, Ali HA Maghrabi, Liam J
McGuffin, David Menéndez-Hurtado, Kliment Olechnovié, Torsten Schwede, et al. Estimation of
model accuracy in casp13. Proteins: Structure, Function, and Bioinformatics, 87(12):1361-1377,
2019.

Taco Cohen and Max Welling. Group equivariant convolutional networks. In Jnternational confer-
ence on machine learning, pp. 2990-2999. PMLR, 2016.

Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and Leonidas J
Guibas. Vector neurons: A general framework for so (3)-equivariant networks. In Proceedings of
the IEEE/CVF International Conference on Computer Vision, pp. 12200-12209, 2021.

Georgy Derevyanko, Sergei Grudinin, Yoshua Bengio, and Guillaume Lamoureux. Deep convolu-
tional networks for quality assessment of protein folds. Bioinformatics, 34(23):4046—4053, 2018.

Frederik Diehl. Edge contraction pooling for graph neural networks. arXiv preprint
arXiv:1905.10990, 2019.

10
Under review as a conference paper at ICLR 2023

Dina Duhovny, Ruth Nussinov, and Haim J Wolfson. Efficient unbound docking of rigid molecules.
In International workshop on algorithms in bioinformatics, pp. 185-200. Springer, 2002.

Stephan Eismann, Raphael JL Townshend, Nathaniel Thomas, Milind Jagota, Bowen Jing, and
Ron O Dror. Hierarchical, rotation-equivariant neural networks to select structural models of
protein complexes. Proteins: Structure, Function, and Bioinformatics, 89(5):493-501, 2021.

Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones,
Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: towards crack-
ing the language of life’s code through self-supervised deep learning and high performance com-
puting. arXiv preprint arXiv:2007.06225, 2020.

Alex M Fout. Protein interface prediction using graph convolutional networks. PhD thesis, Colorado
State University, 2017.

Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-
translation equivariant attention networks. Advances in Neural Information Processing Systems,
33:1970-1981, 2020.

Pablo Gainza, Freyr Sverrisson, Frederico Monti, Emanuele Rodola, D Boscaini, MM Bronstein,
and BE Correia. Deciphering interaction fingerprints from protein molecular surfaces using geo-
metric deep learning. Nature Methods, 17(2):184—192, 2020.

Octavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi S
Jaakkola, and Andreas Krause. Independent se (3)-equivariant models for end-to-end rigid protein
docking. In International Conference on Learning Representations, 2021.

Hongyang Gao and Shuiwang Ji. Graph u-nets. In international conference on machine learning,
pp. 2083-2092. PMLR, 2019.

Johannes Gasteiger, Janek Gro8, and Stephan Giinnemann. Directional message passing for molec-
ular graphs. In International Conference on Learning Representations, 2019a.

Johannes Gasteiger, Janek Gro8, and Stephan Giinnemann. Directional message passing for molec-
ular graphs. In International Conference on Learning Representations, 2019b.

Vladimir Gligorijevi¢, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Beren-
berg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, lan M Fisk, Hera Vlamakis, et al. Structure-
based protein function prediction using graph convolutional networks. Nature communications,
12(1):1-14, 2021.

Pedro Hermosilla, Marco Schafer, Matej Lang, Gloria Fackelmann, Pere-Pau Vazquez, Barbora Ko-
zlikova, Michael Krone, Tobias Ritschel, and Timo Ropinski. Intrinsic-extrinsic convolution and
pooling for learning on 3d protein structures. In International Conference on Learning Represen-
tations, 2020.

Pedro Hermosilla Casajus, Marco Schafer, Matej Lang, Gloria Fackelmann, Pere Pau Vazquez Alco-
cer, Barbora Kozlikova, Michael Krone, Tobias Ritschel, and Timo Ropinski. Intrinsic-extrinsic
convolution and pooling for learning on 3d protein structures. In /nternational Conference on
Learning Representations, ICLR 2021: Vienna, Austria, May 04 2021, pp. 1-16. OpenReview.
net, 2021.

Jie Hou, Badri Adhikari, and Jianlin Cheng. Deepsf: deep convolutional neural network for mapping
protein sequences to folds. Bioinformatics, 34(8):1295—1303, 2018.

Ilia Igashov, Kliment Olechnovié, Maria Kadukova, Ceslovas Venclovas, and Sergei Grudinin.
Vorocnn: deep convolutional neural network built on 3d voronoi tessellation of protein structures.
Bioinformatics, 37(16):2332-2339, 2021.

John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for graph-

based protein design. Advances in neural information processing systems, 32, 2019.

11
Under review as a conference paper at ICLR 2023

Justina Jankauskaité, Brian Jiménez-Garcia, Justas Dapkiinas, Juan Fernandez-Recio, and Iain H
Moal. Skempi 2.0: an updated benchmark of changes in protein-protein binding energy, kinetics
and thermodynamics upon mutation. Bioinformatics, 35(3):462-469, 2019.

Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. Perplexity—a measure of the
difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1):
S$63-S63, 1977.

Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action
recognition. JEEE transactions on pattern analysis and machine intelligence, 35(1):221-231,
2012.

Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror.
Learning from protein structure with geometric vector perceptrons. In International Conference
on Learning Representations, 2020.

Bowen Jing, Stephan Eismann, Pratham N Soni, and Ron O Dror. Equivariant graph neural networks
for 3d macromolecular structure. JCML 2021 CompBio Workshop, 2021.

John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.

Morten Kallberg, Haipeng Wang, Sheng Wang, Jian Peng, Zhiyong Wang, Hui Lu, and Jinbo Xu.
Template-based protein structure modeling using the raptorx web server. Nature protocols, 7(8):
1511-1522, 2012.

Mikhail Karasikov, Guillaume Pagés, and Sergei Grudinin. Smooth orientation-dependent scoring
function for coarse-grained protein quality assessment. Bioinformatics, 35(16):2801—2808, 2019.

Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171-
4186, 2019.

Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv: 1609.02907, 2016.

Jonas KGéhler, Leon Klein, and Frank Noé. Equivariant flows: exact likelihood generative learn-
ing for symmetric densities. In International Conference on Machine Learning, pp. 5361-5370.
PMLR, 2020.

Sohee Kwon, Jonghun Won, Andriy Kryshtafovych, and Chaok Seok. Assessment of protein model
structure accuracy estimation in casp14: Old and new challenges. Proteins: Structure, Function,
and Bioinformatics, 2021.

Fabrice Lefévre, Marie-Héléne Rémy, and Jean-Michel Masson. Alanine-stretch scanning muta-
genesis: a simple and efficient method to probe protein structure and function. Nucleic acids
research, 25(2):447-448, 1997.

Bian Li, Yucheng T. Yang, John A. Capra, and Mark B. Gerstein. Predicting changes in protein ther-
modynamic stability upon point mutation with deep 3d convolutional neural networks. bioRxiv,
2020. doi: 10.1101/2020.02.28.959874.

Shuangli Li, Jingbo Zhou, Tong Xu, Liang Huang, Fan Wang, Haoyi Xiong, Weili Huang, Dejing
Dou, and Hui Xiong. Structure-aware interactive graph neural networks for the prediction of
protein-ligand binding affinity. In Proceedings of the 27th ACM SIGKDD Conference on Knowl-
edge Discovery & Data Mining, pp. 975-985, 2021.

Zhixiu Li, Yuedong Yang, Eshel Faraggi, Jian Zhan, and Yaoqi Zhou. Direct prediction of profiles
of sequences compatible with a protein structure by neural networks with fragment-based local
and energy-based nonlocal profiles. Proteins: Structure, Function, and Bioinformatics, 82(10):
2565-2573, 2014.

12
Under review as a conference paper at ICLR 2023

Yi Liu, Limei Wang, Meng Liu, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message
passing for 3d graph networks. 2021.

Jesper Lundstrém, Leszek Rychlewski, Janusz Bujnicki, and Arne Elofsson. Pcons: A neural-
network-based consensus predictor that improves fold recognition. Protein science, 10(11):2354—
2362, 2001.

Shitong Luo, Jiahan Li, Jiaqi Guan, Yufeng Su, Chaoran Cheng, Jian Peng, and Jianzhu Ma. Equiv-
ariant point cloud analysis via learning orientations for message passing. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18932-18941, 2022.

John Moult, Krzysztof Fidelis, Andriy Kryshtafovych, Torsten Schwede, and Anna Tramontano.
Critical assessment of methods of protein structure prediction (casp)—round x. Proteins: Struc-
ture, Function, and Bioinformatics, 82:1-6, 2014.

Alexey G Murzin, Steven E Brenner, Tim Hubbard, and Cyrus Chothia. Scop: a structural classifi-
cation of proteins database for the investigation of sequences and structures. Journal of molecular
biology, 247(4):536-540, 1995.

Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines.
In Icml, 2010.

David L Nelson, Albert L Lehninger, and Michael M Cox. Lehninger principles of biochemistry.
Macmillan, 2008.

James O’Connell, Zhixiu Li, Jack Hanson, Rhys Heffernan, James Lyons, Kuldip Paliwal, Abdollah
Dehzangi, Yuedong Yang, and Yaoqi Zhou. Spin2: Predicting sequence profiles from protein
structures using deep neural networks. Proteins: Structure, Function, and Bioinformatics, 86(6):
629-633, 2018.

Kliment Olechnovié and Ceslovas Venclovas. Voromqa: Assessment of protein structure quality
using interatomic contact areas. Proteins: Structure, Function, and Bioinformatics, 85(6):1131-
1145, 2017.

Christine A Orengo, Alex D Michie, Susan Jones, David T Jones, Mark B Swindells, and Janet M
Thornton. Cath—a hierarchic classification of protein domain structures. Structure, 5(8):1093-
1109, 1997.

Guillaume Pagés, Benoit Charmettant, and Sergei Grudinin. Protein model quality assessment using
3d oriented convolutional neural networks. Bioinformatics, 35(18):3313-3319, 2019.

Alioscia Petrelli and Luigi Di Stefano. On the repeatability of the local reference frame for partial
shape matching. In 20// International Conference on Computer Vision, pp. 2244-2251. IEEE,
2011.

Yifei Qi and John ZH Zhang. Densecpd: improving the accuracy of neural-network-based compu-
tational protein sequence design with densenet. Journal of chemical information and modeling,
60(3):1245-1252, 2020.

Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel,
and Yun S Song. Evaluating protein transfer learning with tape. Advances in neural information
processing systems, 32:9689, 2019.

Carol A Rohl, Charlie EM Strauss, Kira MS Misura, and David Baker. Protein structure prediction
using rosetta. Methods in enzymology, 383:66-93, 2004.

Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural net-
works. In International conference on machine learning, pp. 9323-9332. PMLR, 2021.

Marcel G Schaap, Feike J Leij, and Martinus Th Van Genuchten. Rosetta: A computer program for

estimating soil hydraulic parameters with hierarchical pedotransfer functions. Journal of hydrol-
ogy, 251(3-4):163-176, 2001.

13
Under review as a conference paper at ICLR 2023

Kristof Schiitt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre
Tkatchenko, and Klaus-Robert Miiller. Schnet: A continuous-filter convolutional neural network
for modeling quantum interactions. Advances in neural information processing systems, 30, 2017.

Kristof Schiitt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction
of tensorial properties and molecular spectra. In International Conference on Machine Learning,
pp. 9377-9388. PMLR, 2021.

Alexandra Shulman-Peleg, Ruth Nussinov, and Haim J Wolfson. Recognition of functional sites in
protein structures. Journal of molecular biology, 339(3):607-633, 2004.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.

Nils Strodthoff, Patrick Wagner, Markus Wenzel, and Wojciech Samek. Udsmprot: universal deep
sequence models for protein classification. Bioinformatics, 36(8):2401—2409, 2020.

Alexey Strokach, David Becerra, Carles Corbi-Verge, Albert Perez-Riba, and Philip M Kim. Fast
and flexible protein design using deep graph neural networks. Cell Systems, 11(4):402-411, 2020.

Freyr Sverrisson, Jean Feydy, Bruno E Correia, and Michael M Bronstein. Fast end-to-end learning
on protein surfaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 15272-15281, 2021.

Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point
clouds. arXiv preprint arXiv: 1802.08219, 2018.

Wen Torng and Russ B Altman. 3d deep convolutional neural networks for amino acid environment
similarity analysis. BMC bioinformatics, 18(1):1-23, 2017.

Raphael Townshend, Rishi Bedi, Patricia Suriana, and Ron Dror. End-to-end learning on 3d protein
structure for interface prediction. Advances in Neural Information Processing Systems, 32:15642—
15651, 2019.

Raphael John Lamarre Townshend, Martin Vogele, Patricia Adriana Suriana, Alexander Derry,
Alexander Powers, Yianni Laloudakis, Sidhika Balachandar, Bowen Jing, Brandon M Ander-
son, Stephan Eismann, et al. Atom3d: Tasks on molecules in three dimensions. In Thirty-fifth
Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021.

Karolis Uziela, David Menendez Hurtado, Nanjiang Shu, Bjorn Wallner, and Arne Elofsson.
Prog3d: improved model quality assessments using deep learning. Bioinformatics, 33(10):1578-
1580, 2017.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.

AD Vecchio, A Deac, P Lid, and P Velic¢kovic. Neural message passing for joint paratope-epitope
prediction. 2021.

Petar Veliékovié, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lid, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.

Donald Voet and Judith G Voet. Biochemistry. John Wiley & Sons, 2010.

Jingxue Wang, Huali Cao, John ZH Zhang, and Yifei Qi. Computational protein design with deep
learning neural networks. Scientific reports, 8(1):1-9, 2018.

Wei Wang, Yan Huang, Yizhou Wang, and Liang Wang. Generalized autoencoder: A neural network
framework for dimensionality reduction. In Proceedings of the IEEE conference on computer
vision and pattern recognition workshops, pp. 490-497, 2014.

14
Under review as a conference paper at ICLR 2023

Xiao Wang, Sean T Flannery, and Daisuke Kihara. Protein docking model evaluation by graph
neural networks. Frontiers in Molecular Biosciences, 8:402, 2021.

Edwin C Webb et al. Enzyme nomenclature 1992. Recommendations of the Nomenclature Commit-
tee of the International Union of Biochemistry and Molecular Biology on the Nomenclature and
Classification of Enzymes. Academic Press, 1992.

Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S Cohen. 3d steerable
cnns: Learning rotationally equivariant features in volumetric data. Advances in Neural Informa-
tion Processing Systems, 31, 2018.

Jinbo Xu and Bonnie Berger. Fast and accurate algorithms for protein side-chain packing. Journal
of the ACM (JACM), 53(4):533-557, 2006.

Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In Jnternational Conference on Learning Representations, 2018.

Jiaqi Yang, Yang Xiao, and Zhiguo Cao. Toward the repeatability and robustness of the local refer-
ence frame for 3d shape matching: An evaluation. [EEE Transactions on Image Processing, 27
(8):3766-3781, 2018.

Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hier-
archical graph representation learning with differentiable pooling. Advances in neural information
processing systems, 31, 2018.

Adam Zemla. Lga: a method for finding 3d similarities in protein structures. Nucleic acids research,
31(13):3370-3374, 2003.

Jian Zhang and Yang Zhang. A novel side-chain orientation dependent potential derived from
random-walk reference state for protein fold selection and structure prediction. PloS one, 5(10):
e15386, 2010.

Yuan Zhang, Yang Chen, Chenran Wang, Chun-Chao Lo, Xiuwen Liu, Wei Wu, and Jinfeng Zhang.
Prodconn: Protein design using a convolutional neural network. Proteins: Structure, Function,
and Bioinformatics, 88(7):819-829, 2020.

Dmitrii Zhemchuzhnikov, Ilia Igashov, and Sergei Grudinin. 6dcnn with roto-translational convolu-
tion filters for volumetric data processing. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 36, pp. 4707-4715, 2022.

15
Under review as a conference paper at ICLR 2023

A PROOF OF ROTATION EQUIVARIANCE

A function f taking a 3D vector 2 € R® as input is rotation equivariance, if applying ant rotation
matrix R € R°*? on @ leads to the same transformations of the output f(a). Formally, f : R? >
R? is rotation equivariance by fulfilling:

R(f(x)) = f(Ra) (25)

For notation consistency, we consider each row of the matrix to be an individual 3D vector and use
right-hand matrix multiplication here. When performing equivariant message passing on protein
graph for node i with local rotation matrix O;, we first transform the vector representations of its
neighbors from global reference to its local reference, that is, for a particular neighbor node j with
vector feature Vj € RC*3, apply our DWNN layers f, and transform the updated features back
to the global reference. We set only one neighbor node wu; for u; for simplicity.The updated vector
representation V; for node i is

Vi = [f(VjOP)]O; (26)
If we apply a global rotation matrix R to all vectors in the global frame, the local rotation matrix O;
will be transformed to O; = O;R, and V; to be V, = Vj R, now the output V, is

V, = [F(V; 0:10; (21)
=[f(VjRR'OT)O.R (28)
=[f(VjON]OR (29)
And if instead, we directly apply the rotation matrix to original output V;, we got
V, = [f(VjO;)]O.R (30)
=ViR 1)

In other words, rotating the input leads to the same transformations to output, so we can preserve
rotation equivariance for vector representations in this way. Note that scalar features remain invariant
when applying global rotation, so our message-passing paradigm with global and local coordinate
transformations is rotation equivariance.

B EXPERIMENT DETAILS

We represent a protein 3D structure as an attributed graph, with each node and edge attached with
scalar and vector features that are geometric-aware. We implement our DWNN in the equivariant
message passing manner, with 3 layer Directed Weight Perceptrons for all tasks.

B.1 PROTEIN FEATURES

In this paper, we use an attributed graph G = (V, €) to represent the protein structure, where each
node corresponds to one particular amino acid in the protein with edges connecting its k-nearest
neighbors. Here we set k = 30. The node features V = {vj, .., uy} and edge features € = {e;;}i4;
are both multi-channel scalar-vector tuples with scalar features like distances and dihedral angles
and vector features like unit vectors representing particular orientations.

A node v; represents the i-th residue in the protein with scalar and vector features describing its
geometric and chemical properties if available. Therefore, a node in this graph may have multi-
channel scalar-vector tuples (s;, V;), $; € R° or R?°, V; € R°*° as its initial features.

* Scalar Feature. The {sin, cos} o {v,w,@}. Here {7,w, o} are dihedral angles computed
from its four backbone atom positions, Ca;_1, Ni, Ca;, Ni+1.

¢ Scalar Feature. A one-hot representation of residue if the identity is available.

¢ Vector Feature. The unit vectors in the directions of Caj41 — Ca; and Caj_1 — Caj.

* Vector Feature. The unit vector in the direction of C'3; Ca; corresponds to the side-chain
directions.

16
Under review as a conference paper at ICLR 2023

The edge e;; connecting the i-th residue and the j-th residue also has multi-channel scalar-vector
tuples as its feature (8;;, Vij), si; € R*4+, Vij € RY?

* Scalar Feature. The encoding of the distance ||Ca; — Ca,|| using 16 Gaussian radial basis
functions with centers spaced between 0 to 20 angstroms.

¢ Scalar Feature. The positional encoding of j — i corresponding the relative position in the
protein sequence.

¢ Scalar Feature: The contact signal describes if the two residues contact in the space,1 if
||Ca; — Ca;|| < 8 and 0 otherwise.

¢ Scalar Feature. The H-bond signal describes if there may be a H-bond between the two
nodes calculated by backbone distance.

* Vector Feature. The unit vector in the direction of Ca; — Ca,.

B.2 MODEL DETAILS

Synthetic Task. We uniformly sample points on the sphere and the ratio of positive and negative
samples is 1:1. We consider the length of each vector as one channel scalar feature € R and the
vector itself as one channel vector feature € R!*?. The VNN and GVP models in this experiment
are set to 3 layers. And our DWP is only | layer, consisting 1 Directed Linear Module, 1 NonLinear
Module, and 1 Directed Interaction Module. We also train a 3-layer MLP by concatenating the
scalar and vector feature as a four-channel scalar feature € R* as input. Specifically, the parameter
counts of VNN, GVP, MLP, DWP is 24,28,24 and 24. We use Binary CrossEntropy Loss for this
two-class classification task. For MLP, the scalar and vector features are concatenated along the
channel, which means the feature of each point is a vector in R*. For GVP and VNN, the input
scalar and vector are represented by separate neural network components.

For all models we trained in the protein-related task, we use 128 scalar channels and 32 vector
channels for each node’s hidden representations, 64 scalar channels, and 16 vector channels for each
edge’s hidden representations. There is 4 message passing updations in implemented models, where
each passing layer consists of 3 stacked DWPs. In total there are about 2.9 million parameters of
our models.

CPD. We train our model in a BERT-style recovery target (Strokach et al.}!2020), more specifically,

we dynamically mask 85% residues of the whole protein and let the model recover them using
structure information from its neighbors using CrossEntropy loss. During testing, we mask all the
residues and recover the whole protein sequence in one forward pass, we also try to iteratively
predict one residue per forwarding pass.

MQA. Because MQA is a regression task, we train our model using MSE loss. When testing, we
compute the different types of correlations between predicted scores and ground truth.

RES. Residue identity requires the model to classify the center residue from a local substructure
of the protein. We construct a subgraph of each local substructure according to Section|B.1} To
guarantee equitable comparison, we retrain all the methods based on our protein graph from scratch.

B.3. TRAINING DETAILS
For the synthetic task, we train each model for 1000 epochs with a learning rate of le — 3 and plot
the training loss for evaluation.

We train our models with learning rate 3e — 4 for CPD and 2e — 4 for MQA, a dropout rate of 10%

2014), and Layernorm paradigm. We train all the models on NVIDIA Tesla V100
or 100 epochs for eac

task.

Our model uses 4-layer message passing, where each message passing consists of 3-layer DWP. The
total model consists of about 2.9 million parameters.

17
Under review as a conference paper at ICLR 2023

B.4 BASELINE DETAILS

We obtain most of our baseline numerical results from other benchmark papers. But because some
models haven’t been evaluated on some of the datasets we used, we reimplement them following the
same model complexity and training hyperparameters as our models.

In the synthetic dataset, we download the source code from (Deng et al.|/2021) and (Jing et al.|
2021), both of which use CrossEntropy loss. The number of parameters of VNN, GVP, MLP, and

DWP is 24, 28, 24, and 24, respectively.

For the RES task Table 1, we implement GCN, GIN, and GAT models, which are trained and
evaluated with the same settings as our models. Following the source code from
[2021] Ting et al.| (2021p, we also retrain 3D CNN and GVP-GNN. To make a fair comparison, we
keep the model complexity (e.g. hidden dimensions, layer numbers) and hyperparameters (epochs,
random seed) of these baselines the same as our models.

For the CPD task in Table 3, we report the result of St-Transformer from (Ingraham et al.||2019),
St-GCN, and GVP-GNN from (Jing et al.| 2020). We modify the source code from Structed-

Trans: ormer(Ingraham et al.||2019) to get the result of St-GAT and St-GIN by replacing the original
attention layer in the encoder and decoder module with Graph Attention Network and Graph Iso-
morphism Network. The hidden dimensions and layer numbers are kept the same as our models.
In Table 5, we adopt most of the results from (Jing et al.|/2020) except running the St-Transformer
model on the TS5O dataset.

For MQA in Table 4, we merge the results of the same task from (Jing et al.|/2020) and Structed-

Transformer (Ingraham et al.||2019). As GVP-GNN only reports three of six of our metrics, so we
reimplement GVP-GNN and DimeNet on our benchmark. Same as the above model, the hidden

dimensions and layer numbers are kept the same as our models.

For our ablation models, we concatenate scalar and vector features as input and replace all the
directed weights with the regular linear layer with scalar weights to get the No DW model. By
removing the Interaction Module lying in the final layer of each perceptron, we obtain the No Int
model. We also replace local frames with unity matrices to break the equivariance in the message
passing part (No equivariance).

In the FOLD and REACT tasks, Kipf et al. and Diehl et al. construct the input graph using the
protein’s contact map and use pair-wise distances of residues as edge features. Baldassarre et al. use
the spatial features including the dihedral angles, surface accessibility, and secondary structure type
of each residue’s node feature. They also use distances, sequence distances, and bond types for edge
features. Hermosilla et al. consider more fine-grained atom-level spatial information, e.g. covalent
radius, van der Waals radius, and atom mass, which leads to better performance. Gligorijevi et al.
refer to different types of contact maps and geometric distances for the input of GNN. All GNN and
CNN baselines make use of 3D information in proteins.

C ADDITIONAL RESULTS

C.1 MULTIPLE RUNS ON RES TASK

We conduct different runs on RES dataset for both baseline and our models. As shown in Table[6]
we report the mean and std results of each method across five different random seeds, our models
are still doing best among other methods and have stable, robust performance with few deviations.

Table 6: Results across five different random seeds of models on RES dataset (Mean + Std).
Model | 3D CNN GCN GIN GAT GVP-GNN | OA-GCN | OA-GIN | OA-GAT |
Acc % | 45.040.4 | 8.240.3 | 9.240.5 | 12.440.2 | 48.4+0.4 50.240.2 | 50.8+0.1 | 49.2+0.3 |

C.2 ITERATIVE PREDICTION ON CPD TASK

We tested the sequential decoding of the prediction from the N-side to the C-side and reported the
native sequence recovery as shown in Table[7]

18
Under review as a conference paper at ICLR 2023

Table 7: Iterative prediction results of our models on both CATH (Short, Single, All) and TS50
dataset.

Model Short ] Single |] All ] TS50
OA-GCN (iter) | 34.1 35.7 | 45.3 | 50.9
OA-GIN (iter) | 34.4 | 37.2 | 45.7 | 52.2
OA-GAT (iter) | 36.0 | 36.5 | 36.5 | 49.6

The results have shown that the sequential decoding approach basically caused a degradation of the
results, but because we introduced the directed weight and geometric operations, our results are still
better than other benchmarks. We have added these results in the supplementary material.

C.3. PROTEIN FUNCTION CLASSIFICATION

Datasets. Fold Classification (FOLD) predicts the structure category of a given protein structure.
We choose the SCOP v1.75 dataset collected by [Murzin et al.|(1995) that organizes structures into
3-layer hierarchical classes. In total, there are 16, 712 proteins covering 7 major structural types with
1, 195 identified folds. We adopt a reduced dataset from|Hou et al.|(2018), and remove homologous
sequences between test and training data sets at Family (Fam), Superfamily (Sup) or Fold levels,
resulting in three different classification tasks.

The Enzyme-Catalyzed Reaction Classification (REACT) study is a very similar task as it requires
to classify the EC number|Webb et al.|(1992) of a catalyzed enzyme based on the protein structures.
Therefore, we put these two tasks ue by-side to evaluate the performance of different methods.
We use the dataset collected by (Hermosilla Casajus et al. 2021), containing 37, 428 proteins from
384 types of Enzyme-Catalyzed classes and we split them into training, validation set, test set and
ensure no sequence or structure overlaps across different sets. In total, there are 29, 215 structures
for training, 2, 562 for validation, and 5, 651 for test.

Metrics. Following standard benchmarks (2019); |Hermosilla Casajus et al.| (2021), we use

the classification accuracy for evaluating the prediction performance.

Baselines. We compare with CNN-based models and GNN-based models which learn the protein
annotations using 3D structures from scratch. For fair comparison, we also do not include LSTM-
based or transformer-based methods, as they all pre-train their models using millions of protein

sequences and only fine-tune their models on 3D syructures COO [Bepler & Berger! Berger| (2018 (2018); [Alley et al.|

Results. As shown in Table|8] our models demonstrate rac compatt prediction performance in both
tasks across different levels of the hierarchy. Classifying the fold and enzyme classes based on the
protein structure is one of the key problems in structural biology and our proposed models can be
used as new tools to conduct function annotations for new proteins.

For FOLD and REACT tasks, all GNN-based baselines use 3D structure information:

+ [Kipf & Welling] (2016) and|Dieh]] (2019) construct the input graph using the protein’s con-

tact map and use pair-wise distances of residues as edge features.

. (2021) use the spatial features including the dihedral angles, surface
accessibility, and secondary structure type of each residue’s node feature. They also use
distances, sequence distances, and bond types for edge features.

* [Hermosilla Casajus et al.| (2021) consider more fine-grained atom-level spatial informa-
tion, e.g. covalent radius, van der Waals radius, and atom mass, which leads to better
performance.

* |Gligorijevié et al] zal 2021) refer to different types of contact maps and geometric distances
or the input o

Despite not using special training techniques, pre-training tasks, and fine-tuning of hyperparameters,
our models demonstrate comparable prediction performance in both tasks across different levels of
the hierarchy. Especially with the introduction of our directed weight with equivariant message
passing, it is a big improvement over the normal GNN.

19
Under review as a conference paper at ICLR 2023

Table 8: Results on Fold and React classification
FOLD

Architecture REACT
Fold Sup Fam

(2018) 1D ResNet 17.0% 31.0% 77.0% 70.9%
Derevyanko et al. (2018 3D CNN 31.6% 45.4% 92.5% 78.8%
Kipf & Welling (2016 GCN 16.8% 21.3% 82.8% 67.3%
[Diehl (2019) GCN 12.9% 16.3% 72.5% 57.9%

i ; GCN 45.0% 69.7% 98.9% 87.2%
GCN 23.7% 32.5% 844% 60.8%

LSTM+GCN- 15.3% 20.6% +=73.2% = 63.3%

Our method OA-GCN 31.2% 39.8% 848% 76.0%

Our method OA-GIN 32.8% 38.3% 85.2% 76.7%

Our method OA-GAT 29.9% 36.6% 79.0% 77.2%

D COMPARISONS TO PRIOR WORKS

D.1 COMPARISON TO ANGULAR-EXPLICIT MODELS

There are several works that explicitly model angles/torsion angles in GNNs. DimeNet(Gasteiger}
fet al.| (2019b) uses physical-informed RBF and SBF functions to represent distances and angular
information. SphereNet [Liu et al.| (2021) further employs a 3D meaningful spherical coordinate
system to represent molecular torsional angles. However, these methods rely on invariant scalar
input features for remaining equivariance, and during convolution, these scalar features (distances,
angles) are just used for updating hidden representations and stay the same in the next layer updating.

On the technical level, our model also has several differences from other methods when compared
with existing studies to consider backbone torsion angles (DimeNet, SpehreNet, etc.), as follows:

1. RBF-embedded distances and Trigonometric-embedded angles are adopted in the input fea-
ture engineering part (see supplementary material). To better represent the 3D geometries,
we also introduce several normal vectors as input node features and edge characteristics.
The scalar representations are invariant and vectors are equivariant. But these methods
don’t consider vector representations and only focus on invariant scalar features.

2. We capture fine-grained geometries with the help of vector representations and directed
weight perceptrons. And representations are updated layer-by-layer. Our directed weights
are learnable to be expandable for complex reasoning capabilities. But these methods only
embed angles and distances into edge features during message updating and lack geometry
meaningful operations. They also don’t have directed weights, which are key contributions
to our work.

3. We maintain equivariance with the help of local coordinate transformations on vector rep-
resentations equipped with rigid residue positions. But these methods don’t consider equiv-
ariant vectors, they only condition on invariant scalars. SphereNet does use local spherical
systems, but they are used for calculating relative locations of atoms, seen as a feature
engineering part.

4. We aim to design a new type of vector neurons for geometric learning and a general mes-
sage passing paradigm for 3D graphs. We have shown our versatility in any existing graph
learning frameworks. However, these methods are designed specifically for molecular
graphs and are hard to extend for other 3D point sets.

D.2. COMPARISON TO OTHER EGNN WORKS

Local-global frame transformation is an important technique for 3D object learning

2011 2018 2022) and is inevitable in the discussions of 3D

20
Under review as a conference paper at ICLR 2023

equivariance. Our proposed framework focuses on its computation in the latent space together with
our vector-based feature representations, while many existing works only apply rotations to the
primal space of explicit geometric/physical properties (e.g. simple cartesian coordinates). It also
provides better flexibility over message-passing operators by eliminating the requirements for spe-
cific feature-update arithmetics — this also serves as an adaptor for plugging our designed feature
representations into many existing GNN models while guaranteeing equivariance.

Among the EGNNs, we could not agree that our model is a simplified model from others. Specif-
ically, some of these EGNNs could be considered a special case of our framework. For instance,
E(n) GNN combines node coordinates weighted by hidden representations,
which could be recognized as linearly updating of simple vector features (coordinate) and unit ma-
trix transformations (in analogy to our back and forth transformations).

Additionally, to the best of our knowledge, there is no previous work that integrates vector-based
features and directed weighted neural networks into local-global transformation frameworks. We ap-
ply transformations on hidden scalar-vector features back and forth for better representations under
equivariance, which turns out to show decent performances in the protein learning tasks.

D.3. COMPARISON TO OTHER BACKBONE-ORIENTATION MODELS

Using sin,cos...,,w as features are popular techniques in protein learning tasks
[Ganea et.al}2021), but our models can have a more powerful ability to
make use of these features. The meaning of Orientation-Aware GNNs is not to use torsional angles
as the node and edge features, but for reasoning on them. Our real contribution is to design a new
type of neural network, which has good capability in sensing these geometric features (e.g. angles,
orientations) as well as capturing complex interactions. The synthetic experiment gives an intuitive
example of our claims, where our designed directed weight perceptrons have more powerful in sens-
ing and handling angular-related characteristics. Here we share an even simpler example: Assume
the input contains two 3D vectors v; and v2 described by their coordinates [a1, yi, 21, £2, ye, 22],
the problem we solved is that how should we let the model know the angle between v, and v2. The
most straightforward intuition is that if the weights are directed vectors, then we could count on the
inner products between weights and input vectors to capture the angle information. Furthermore,
if one wants to retrieve more complex geometric objects, e.g. normal vectors, dihedral angles, or
parallel surfaces based on input coordinates, our employed cross-product operation can easily solve
that. As we use scalar-vector tuple hidden representations for nodes and edges, and directed weights
with geometric-meaningful operators are learnable, our models could perform more complex geo-
metric reasoning ability and modeling capability beyond angles, especially approximating nonlinear
functions on vector representations.

The most straightforward intuition is that if the weights are directed vectors, then we could count on
the inner products between weights and input vectors to capture the angle information. Furthermore,
if one wants to retrieve more complex geometric objects, e.g. normal vectors, dihedral angles, or
parallel surfaces based on input coordinates, our employed cross-product operation can easily solve
that. As we use scalar-vector tuple hidden representations for nodes and edges, and directed weights
with geometric-meaningful operators are learnable, our models could perform more complex geo-
metric reasoning ability and modeling capability beyond angles, especially approximating nonlinear
functions on vector representations.

The motivation and function of residue orientations are also different from prior works. Instead of
using the angles (orientations) as input features, residue backbone orientations define the coordinate
frame of the whole system and are used in the equivariant message passing part for coordinate trans-
formation. Rigid orientations play the role of bridging local and global coordinates for equivariance
in our message passing part.

21
Under review as a conference paper at ICLR 2023

E EXPLANATIONS TO SOME MODULES

J
E.1 ABOUT THE s',,,

For the s/,,,features, it can be interpreted as the summation of the channel-wise dot product be-
tween the input features (channels of vectors) and the directed weights (a matrix of vectors, so a
3-dimensional tensor). Let us consider the i-th number in the output feature, it can be formulated as:

= Wikysk — (wi) v4
jk

j

Notice that both are 3D vectors, **the dot product can essentially reflect the cosine value between
them after normalization**. Essentially, our method utilizes the dot product operation between the
directed weights and the input coordinate features, and more importantly, the weight vectors are
learnable. We also recommend you pay attention to other operations we introduced for reasoning
in complex geometric relations and encouraging efficient scalar-vector feature interactions. For
instance, the cross-product operation can create new orthogonal orientations; the interaction modules
can facilitate feature blending.

E.2. ABOUT THE SCALR-LIFTING PART

The initial motivation of scalar lifting is to enable scalar-vector interactions, which are not well
addressed in previous works (for instance, VNN (Deng et al.|/2021) and GVP Ging et al.}/2020) can
only perform vector-to-scalar mappings with a very limited choice of operations). Unprojecting from
lower dimensions to higher dimensions is indeed non-bijective on the test data themselves, but it is
a widely adopted approach like the diluted convolution or the transposed convolution that succeeds
in the projection onto higher dimensional features. Mapping from high dimensions to lower space
and then projecting back is also a popular technique in FCNs
fetal 2021}, Generative Models (Wang et al.||2014), and Manifold Learning, which can be regarded
as feature compression and information bottleneck. Therefore, though not unique, the learnable
directed weights endow the model with the expressiveness to recover useful information hopefully
after training on many data. More intuitively, the scalar lifting operator generates fixed directional
vectors with different lengths during inference, which can potentially provide more information
during the next round of feature processing and facilitate information flow between scalar and vector
features.

The scalar lifting module serves to pass scalar information to the vector representation in our model.
Though we give a geometric interpretation of the intuition, our directed weight is learnable, the
operation of the model is complex and its final geometric meaning is non-linear, which is difficult to
define simply. The function of the mapping is learned according to the needs of the downstream task
and is not limited to reconstruction or compression. From the experimental results, the network we
designed does achieve excellent performance, demonstrating our strong geometric perception and
representation capabilities.

22
