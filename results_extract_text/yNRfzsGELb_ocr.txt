Under review as a conference paper at ICLR 2023

REMOVING STRUCTURED NOISE WITH
DIFFUSION MODELS

Anonymous authors
Paper under double-blind review

ABSTRACT

Solving ill-posed inverse problems requires careful formulation of prior beliefs
over the signals of interest and an accurate description of their manifestation into
noisy measurements. Handcrafted signal priors based on e.g. sparsity are increas-
ingly replaced by data-driven deep generative models, and several groups have
recently shown that state-of-the-art score-based diffusion models yield particu-
larly strong performance and flexibility. In this paper, we show that the powerful
paradigm of posterior sampling with diffusion models can be extended to include
rich, structured, noise models. To that end, we propose a joint conditional reverse
diffusion process with learned scores for the noise and signal-generating distribu-
tion. We demonstrate strong performance gains across various inverse problems
with structured noise, outperforming competitive baselines that use normalizing
flows and adversarial networks. This opens up new opportunities and relevant
practical applications of diffusion modeling for inverse problems in the context of
non-Gaussian measurements.

1 INTRODUCTION

Many signal and image processing problems, such as denoising, compressed sensing, or phase re-
trieval, can be formulated as inverse problems that aim to recover unknown signals from (noisy)
observations. These ill-posed problems are, by definition, subject to many solutions under the given
measurement model. Therefore, prior knowledge is required for a meaningful and physically plau-
sible recovery of the original signal. Bayesian inference and maximum a posteriori (MAP) solutions
incorporate both signal priors and observation likelihood models. Choosing an appropriate statistical
prior is not trivial and is often dependent on both the application as well as the recovery task.

Before deep learning, sparsity in some transform domain has been the go-to prior in compressed
sensing (CS) methods (Eldar & Kutyniok]|2012), such as iterative thresholding
or wavelet decomposition (Mallat]|1999). At present, deep generative modeling has estab-
lished itself as a strong mechanism for learning such priors for inverse problem-solving. Both gen-
erative adversarial networks (GANs) and normalizing flows (NFs)
have been applied as natural signal priors for inverse problems in image
recovery. These data-driven methods are more powerful compared to classical methods, as they can
accurately learn the natural signal manifold and do not rely on assumptions such as signal sparsity
or hand-crafted basis functions. Recently, diffusion models have shown impressive results for both
conditional and unconditional image generation and can be easily fitted to a target data distribution
using score matching (Vincent] |2011} |Song et al. 2020). These deep generative models learn the
score of the data manifold and produce samples by reverting a diffusion process, guiding noise sam-
ples towards the target distribution. Diffusion models have achieved state-of-the-art performance in
many downstream tasks and applications, ranging from state-of-the-art text-to-image models such as

DALL-E 2 (Ramesh et al.|/2022) to medical imaging (Song et al.| 2021b} (Jalal et al} 202 1a} Chung|

(2022). Furthermore, understanding of diffusion models is rapidly improving and progress

in the field is extremely fast-paced (Chung et al.| 2022a| Bansal et al.| 2022} Daras et al.| 2022a}
Karras et al. 2022} {Luo} 2022). The iterative nature of the sampling procedure used by diffusion
models renders inference slow compared to GANs and VAEs. However, many recent efforts have
shown ways to significantly improve the sampling speed by accelerating the diffusion process. In-

spired by momentum methods in sampling, |Daras et al.|(2022b) introduces a momentum sampler for
diffusion models, which leads to increased sample quality with fewer function evaluations.

Under review as a conference paper at ICLR 2023

offers a new sampling strategy, namely Come-Closer-Diffuse-Faster (CCDF), which
leverages the conditional quality of inverse problems. The reverse diffusion can be initialized from
the observation instead of a sample from the base distribution, which leads to faster convergence
for conditional sampling. proposes a progressive distillation method that
augments the training of the diffusion models with a student-teacher model setup. In doing this,
they were able to drastically reduce the number of sampling steps. Lastly, many methods aim to
execute the diffusion process in a reduced space to accelerate the diffusion process. While

(2022) restricts diffusion through projections onto subspaces, |Vahdat et al. (2021) and |Rombach|

t al.|(2022) run the diffusion in the latent space.

Despite this promise, current score-based diffusion methods for inverse problems are limited to mea-
surement models with unstructured noise. In many image processing tasks, corruptions are however
highly structured and spatially correlated. Relevant examples include interference, speckle, or haze.
Nevertheless, current conditional diffusion models naively assume that the noise follows some basic
tractable distribution (e.g. Gaussian or Poisson). Beyond the realm of diffusion models, |Whang et al.]

extended normalizing flow (NF)-based inference to structured noise applications. However,
compared to diffusion models, NFs require specialized network architectures, which are computa-
tionally and memory expensive.

Given the promising outlook of diffusion models, we propose to learn score models for both the noise
and the desired signal and perform joint inference of both quantities, coupled via the observation
model. The resulting sampling scheme enables solving a wide variety of inverse problems with
structured noise.

The main contributions of this work are as follows:
¢ We propose a novel joint conditional posterior sampling method to efficiently remove struc-

tured noise using diffusion models. Our formulation is compatible with many existing
iterative sampling methods for score-based generative models.

¢ We show strong performance gains across various challenging inverse problems involving
structured noise compared to competitive state-of-the-art methods based on NFs and GANs.

¢ We demonstrate improved robustness on out-of-distribution signals compared to baselines.

2 PROBLEM STATEMENT

Many image reconstruction tasks can be formulated as an inverse problem with the basic form
y =Ax+n, dd)

where y € R” is the noisy observation, x € R¢@ the desired signal or image, and n € R” the
additive noise. The linear forward operator A € R™*¢ captures the deterministic transformation of
x. Maximum a posteriori (MAP) inference is typically used to find an optimal solution Xyap that
maximizes posterior density px |y (x|y):

Xmap = arg max log px\y (x|y) (2)
x

= arg max [log py|x(y|x) + log px(x)], (3)
x

where py|x (y|x) is the likelihood according to the measurement model and log px (x) the signal
prior.

Assumptions on the stochastic corruption process n are of key importance too, in particular for ap-
plications for which this process is highly structured. However, most methods assume i.i.d. Gaussian
distributed noise, such that the forward model becomes py} x (y|x) ~ M(Ax, 73,1). This naturally
leads to the following simplified problem:

a . 1
Smap = arg min —||y — Ax||3 — log px (x). (4)
x 20N

However, as mentioned, this naive assumption can be very restrictive as many noise processes are
much more structured and complex. A myriad of problems can be addressed under the formulation
Under review as a conference paper at ICLR 2023

of equation [I] given the freedom of choice for the noise source n. Therefore, in this work, our
aim is to solve a more broad class of inverse problems defined by any arbitrary noise distribution
n ~ py(n) 4 N and signal prior x ~ px(x), resulting in the following, more general, MAP

estimator proposed by|Whang et al.|(2021):
Xap = arg max log py (y — Ax) — log px (x). (5)
x

In this paper, we propose to solve this class of problems using flexible diffusion models. Further-
more, diffusion models naturally enable posterior sampling, allowing us to take advantage of the

2.1 RELATED WORK
2.1.1 NORMALIZING FLOWS

[Whang et al.| (2021) propose to use normalizing flows (NFs) to model both the data and the noise

distributions. Normalizing flows are a special class of likelihood-based generative models that make
use of an invertible mapping G : R¢ + R¢ to transform samples from a base distribution pz (z) into
a more complex multimodal distribution x = G(z) ~ px (x). The invertible nature of the mapping
G allows for exact density evaluation through the change of variables formula:

log px (x) = log pz() + log | det Jg-1(x)|, (6)
where J is the Jacobian that accounts for the change in volume between densities. Since exact
likelihood computation is possible through the flow direction G~', the parameters of the generator
network can be optimized to maximize likelihood of the training data. Subsequently, the inverse
task is solved using the MAP estimation in equation[5}

% = arg max {log poy (y — Ax) + log pcx (x)}. @)
x

where G'y and G'x are generative flow models for the noise and data respectively. Analog to that,
the solution can be solved in the latent space rather than the image space as follows:

Z= arg max {log pay (y — A(Gx(z))) + Alog pax (Gx (z))}- (8)

Note that in equation|8]a smoothing parameter is added to weigh the prior and likelihood terms,
as was also done in|Whang et al.|(2021).

2.1.2 GENERATIVE ADVERSARIAL NETWORKS

Generative adversarial networks (GANs) are implicit generative models that can learn the data man-
ifold in an adversarial manner (Goodfellow et al.| 2020). The generative model is trained with an
auxiliary discriminator network that evaluates the generator’s performance in a minimax game. The
generator G(z) : R’ — R¢ maps latent vectors z € R! ~ N(0,1) to the data distribution of interest.

The structure of the generative model can also be used in inverse problem solving 2017).
The objective can be derived from equation B]and is given by:

# = arg min {|ly — AGx(z)|| + Allz|]3}, (9)

where \ weights the importance of the prior with the measurement error. The 2 regularization
term on the latent variable is proportional to negative log-likelihood under the prior defined by Gx,
where the subscript denotes the density that the generator is approximating. While this method
does not explicitly model the noise, it remains an interesting comparison, as the generator cannot
reproduce the noise found in the measurement and can only recover signals that are in the range of
the generator. Therefore, due to the limited support of the learned distribution, GANs can inherently
remove structured noise. However, the representation error (i.e. observation lies far from the range

of the generator (Bora et al.| |2017)) imposed by the structured noise comes at the cost of recovery
quality.
Under review as a conference paper at ICLR 2023

2.2 BACKGROUND ON SCORE-BASED DIFFUSION MODELS

One class of deep generative models is known as diffusion models. These generative models have
been introduced independently as score-based models 035 & Ermon| 2019} |2020) and denoising

diffusion probabilistic modeling (DDPM) (Ho et al.| In this work, we will consider the
formulation introduced in (2020), which unifies both perspectives on diffusion models

by expressing diffusion as a continuous process through stochastic differential equations (SDE).
Diffusion models produce samples by reversing a corruption process. In essence these models are
networks trained to denoise its input. Through iteration of this process, samples can be drawn from
a learned data distribution, starting from random noise.

The diffusion process of the data {x: € RF 10 1]

Gaussian perturbations of increasing magnitude indexed by time t € [0, 1]. Starting from the data
distribution at t = 0, clean images are defined by xp ~ p(xo) = p(x). Forward diffusion can be
described using an SDE as follows:

dx, = f(t)x,dt + g(t)dw, (10)

where w € R? is a standard Wiener process, f(t) : [0,1] — R and g(t) : [0,1] > R are the
drift and diffusion coefficients, respectively. Moreover, these coefficients are chosen so that the
resulting distribution p;(x) at the end of the perturbation process approximates a predefined base
distribution p;(x) + (x). Furthermore, the transition kernel of the diffusion process is defined as
a(xt[x) ~ N(x: |a(t)x, 6?(t)1), where a(t) and 8(t) can be analytically derived from the SDE.

Naturally, we are interested in reversing the diffusion process, so that we can sample from xp ~
po(xo). The reverse diffusion process is also a diffusion process given by the reverse-time SDE

(Anderson) 1982] Song eal2020}
dx, = [f(t)x: — g(t)? Vc, log p(x) ] dt + g(t)dw, (1)
K-_S

score

is characterized by a continuous sequence of

where w; is the standard Wiener process in the reverse direction. The gradient of the log-likelihood
of the data with respect to itself, a.k.a. the score function, arises from the reverse-time SDE. The
score function is a gradient field pointing back to the data manifold and can intuitively be used to
guide a random sample from the base distribution 7(x) to the desired data distribution. Given a
dataset ¥ = {x(),x@),...,x(/%)} ~ p(x), scores can be estimated by training a neural network
So(Xz,t) parameterized by weights 0, with score-matching techniques such as the denoising score

matching (DSM) objective (Vincent) [201 1p:
0° = argminE,~v[0,1) {Ep r)~n(sataebe [lls0(%e-4) ~ Vix log aCebs)|2]}- 12)

Given a sufficiently large dataset V and model capacity, DSM ensures that the score network con-
verges to 59(x;,t) ~ Vx, log p(x:). After training the time-dependent score model so, it can be
used to calculate the reverse-time diffusion process and solve the trajectory using numerical sam-
plers such as the Euler-Maruyama algorithm. Alternatively, more sophisticated samplers, such as

ALD (Song & Ermon] 2019), probability flow ODE (Song et al. (2020), and Predictor-Corrector
Song et al.

sampler ( 20), can be used to further improve sample quality.

These iterative sampling algorithms discretize the continuous time SDE into a sequence of time
steps {0 = to,t1,...,t7 = 1}, where a noisy sample X;, is denoised to produce a sample for the
next time step X,,_,. The resulting samples {X,,}7_9 constitute an approximation of the actual
diffusion process {X+},<[9,1)-

3. METHOD

3.1 CONDITIONAL POSTERIOR SAMPLING UNDER STRUCTURED NOISE

We are interested in posterior sampling under structured noise. We recast this as a joint optimization
problem with respect to the signal x and noise n given by:

(x,n) ~ px,w(x, nly) x py|x.n (yx. nm) > px (x) - py (n). (13)
Under review as a conference paper at ICLR 2023

Solving inverse problems using diffusion models requires conditioning of the diffusion process on
the observation y, such that we can sample from the posterior p xy (x, nly). Therefore, we construct
a joint conditional diffusion process {x,, nElY }rejo,1)° in turn producing a joint conditional reverse-
time SDE:

(x, ng) = [f(t)(x), 1) — g(t)? Vicz,n, log p(x1, mely)] dt + g(t)dwy. (4)

We would like to factorize the posterior using our learned unconditional score model and tractable
measurement model, given the joint formulation. Consequently, we construct two separate dif-
fusion processes, defined by separate score models but entangled through the measurement model
Py|x,n(y|x, 1). In addition to the original score model s9 (x, t), we introduce a second score model
sy(y,t) ~ Vn, log px (m;), parameterized by weights ¢, to model the expressive noise component
n. These two score networks can be trained independently on datasets for x and n, respectively,
using the objective in equation [12] The gradients of the posterior with respect to x and n are now
given by:

Vx, log p(xt, nly) ~ Vx, log p(x) + Vx, log p(¥t|Xt, ns)
& 89+ (Xt, t) + Vx, log p(¥rlxe, ne), (15)

Vim, log p(x, nly) ~ Vn, log p(nz) + Vn, log p(¥1/Xe, Me)
Sox (me, t) + Vn, log p(¥tlx:, ns), (16)

2

where y; is a sample from p(y;|y), and {Yt re(0,1] is an additional stochastic process that essentially
corrupts the observation along the SDE trajectory together with x,. As p(yz|y) is tractable, we can
easily compute ¥; = a(t)y + 3(t)Az, using the reparameterization trick with z € R? ~ N(0,1),
see{Song et al.](2021b). Subsequently, the approximation in equation [15] and equation [16] can be
substituted for the conditional score in equation[14] resulting in two entangled diffusion processes:

(17)

{ dx, = [f(t)x1 — g(t)? {s0- (xt, t) + Vix, log p(WelxXe, me) } Jat + g(t)dwe,e
dn, [f(t — g(t)? {so (me, t) + Vn, log p(¥elxe, m1) } Jat + gQ)dww,t

which allows us to perform posterior sampling for both the signal, such that x = xo ~ px|y(xoly),
as well as the structured noise, such thatn = ng ~ pyjy(noly).

To solve the approximated joint conditional reverse-time SDE, we resort to the aforementioned
iterative scheme in Section however, now incorporating the observation via a data-consistency
step. This is done by taking gradient steps that minimize the 2 norm between the true observation
and its model prediction given current estimates of x and n. Ultimately, this results in solutions
that are consistent with the observation y and have high likelihood under both prior models. The
data-consistency update steps for both x and n are derived as follows:

Xp-At = Xt — Vx, log p(¥e|X, ny) yar = Hy — Vn, log p(¥t|Xe, ny)
= & — Vax ||%e — (AK + 8)|13 = fy — Va, |[9e — (AK + fv)I5
=% —AAT(AR,— Fr + Hu), (18) = ny — (AX — ye + iu), (19)

where the time difference between two steps At = 1/T and X and yu are weighting coefficients for
the signal and noise gradient steps, respectively. An example of the complete sampling algorithm is
shown in Algorithm[I] which adapts the Euler-Maruyama sampler to jointly find
the optimal data sample and the optimal noise sample while taking into account the measurement
model in line[7|and[8|using the outcome of equation|18]and equation|19} respectively. Although we
show the Euler-Maruyama method, our addition is applicable to a large family of iterative sampling
methods for score-based generative models.
Under review as a conference paper at ICLR 2023

Algorithm 1: Joint conditional posterior sampling with Euler-Maruyama method

Require: 7’, sg, 54,A, 1, y Does

1X ~ 7(x), fi ~ a(n), At HF 10 Kiar & Xe — f(t)x,At
2 u Ri_at © Ri_at + g(t)? 85 (RK, t)At
3 fori =T —1to0do 2 z~N(0,1)
4 te ot B Xt_at © Xia + g(t)VAtz
s | 9 ~ Do.(yely) 4
6 15 yar < hy — f (t)mpAt

// Data consistency steps 16 Apart + Dy—ar + g(t)?s% (te, t)At
7 Ri-ar — & — AAT(AR: — He + Hy) 7 z~N(0,1)
8 | eae < fy — p(AR, — Ye + Hh) w | ear © Hy—ar + g(t)VAtz
9 vee return: Xo

1 end

3.2 TRAINING AND INFERENCE SETUP

For training the score models, we use the NCSNv2 architecture as introduced in [Song & Ermon|
in combination with the Adam optimizer and a learning rate of 5e—4 until convergence. For
simplicity, no exponential moving average (EMA) filter on the network weights is applied. Given
two separate datasets, one for the data and one for the structured noise, two separate score models
can be trained independently. This allows for easy adaptation of our method, since many existing
trained score models can be reused. Only during inference, the two priors are combined through
the proposed sampling procedure as described in Algorithm[]] using the adapted Euler-Maruyama

sampler. We use the variance preserving (VP) SDE (9 = 0.1, 8; = 7.0) (Song et al.|/2020) to

define the diffusion trajectory. During each experiment, we run the sampler for 7’ = 600 iterations.

4 EXPERIMENTS

All models are trained on the CelebA dataset and the MNIST dataset with 10000
and 27000 training samples, respectively. We downsize the images to 64 x 64 pixels. Due to
computational constraints, we test on a randomly selected subset of 100 images. We use both the
peak signal-to noise ratio (PSNR) and structural similarity index (SSIM) to evaluate our results.

4.1 BASELINE METHODS

The closest to our work is the flow-based noise model proposed by|Whang et al.|(2021), discussed in
Section which will serve as our main baseline. To boost the performance of this baseline and
to make it more competitive we moreover replace the originally-used RealNVP (Dinh et al.|/2016)

with the Glow architecture (Kingma & Dhariwall 2018). Glow is a widely used flow model highly

inspired by RealNVP, with the addition of I x I convolutions before each coupling layer. We use
the exact implementation found in [Asim et al.](2020), with a flow depth of K = 18, and number
of levels L = 4, which has been optimized for the same CelebA dataset used in this work and thus
should provide a fair comparison with the proposed method.

Secondly, GANS as discussed in Section[2.T-2]are used as a comparison. We train a DCGAN
{ford et al.| {2015), with a generator latent input dimension of 1 = 100. The generator architecture
consists of 4 strided 2D transposed convolutional layers, having 4 x 4 kernels yielding feature maps
of 512, 256, 128 and 64. Each convolutional layer is followed by a batch normalization layer and
ReLU activation.

Lastly, depending on the reconstruction task, classical non-data-driven methods are used as a com-
parison. For denoising experiments, we use the block-matching and 3D filtering algorithm (BM3D)
(Dabov et al. /2006), and in compressed sensing experiments, LASSO with wavelet basis

1996).
Under review as a conference paper at ICLR 2023

Tis? 033 175 049 22.91 O83 725 087 ] 19 1045 0.27 13.07 0.22
41 30

°

a
SSIM
PSNR

© 20 = 20 a
g a
- A Joa Joa
IS IS}
5 0.2 a =] 5 0.2
105 10 .
i i i T 0.0 T T T T 0.0
S = s S Ss = s S
= ic Y Ss = ic Y Ss
$ $
(a) CelebA (b) Out-of-distribution data

Figure 1: Quantitative results using PSNR (green) and SSIM (blue) for the removing MNIST digits
experiment on 64 x 64 images of the (a) CelebA and (b) out-of-distribution datasets.

Except for the flow-based method of|Whang et al.|(2021), none of these methods explicitly model

the noise distribution. Still, they are a valuable baseline, as they demonstrate the effectiveness of
incorporating a learned structured noise prior rather than relying on simple noise priors.

Automatic hyperparameter tuning for optimal inference was performed for all baseline methods on
a small validation set of only 5 images. For both GAN and flow-based methods, we anneal the step
size during inference based on stagnation of the objective.

4.2 RESULTS

4.2.1 REMOVING MNIST DIGITS

For comparison with (2021), we recreate an experiment introduced in their work,
where MNIST digits are added to CelebA faces. Moreover, the experiment is easily reproducible
as both CelebA and MNIST datasets are publicly available. The corruption process is defined by
y = 0.5- Xceteba + 0.5 - nist. In this experiment, the score network s¥ is trained on the MNIST
dataset. Fig.|la]shows a quantitative comparison of our method with all baselines. Furthermore, a
random selection of test samples is shown in Fig. 2]for qualitative analysis. Both our method and the
flow-based method are able to recover the data, and remove most of the structured noise. However,
more details are preserved using the diffusion method. In contrast, the flow-based method cannot
completely remove the digits in some cases and is unable to reconstruct some subtle features present
in the original images. Furthermore, we observe that for the flow-based method, initialization from
the measurement is necessary to reproduce the results in{Whang et al.|(2021) since random initial-
ization does not converge. The GAN method is also able to remove the digits, but cannot accurately
reconstruct the faces as it is unable to project the observation onto the range of the generator. Simi-
larly, the BM3D denoiser fails to recover the underlying signal, confirming the importance of prior
knowledge of the noise in this experiment. The metrics in Fig. [Ta] support these observations. See
Table[I]for the extended results.

Additionally, we expose the methods in a similar experiment to out-of-distribution (OoD) data. The
images from this dataset not found in the CelebA dataset, which is the data used for training the
models. In fact, the out-of-distribution data is generated using the stable-diffusion text-to-image
model|Rombach et al.](2022). We use the exact same hyperparameters as during the experiment on
the CelebA dataset. Quantitative and qualitative results are shown in Fig.[Ibland Fig.} respectively.
Similarly to the findings of/Whang et al.|(2021);/Asim et al. 2020), the flow-based method is robust
to OoD data, due to their inherent invertibility. We empirically show that the diffusion method is also
resistant to OoD data in inverse tasks with complex noise structures and even outperforms the flow-
based methos. Unsurprisingly, the GAN method performs even more poorly when subjected to OoD
data. More experiments, covering different inverse problem settings can be found in Appendix |A]

Under review as a conference paper at ICLR 2023

Ground
Truth

Noisy
DIFFUSION Input

FLOW

GAN

A
CG

Figure 2: Results for our diffusion-based method compared ° ine baselines; FLOW (Whang et al |

2021 ), GAN (Bora et al.| 2017), and BM3D (Dabov et al.| on the removing poet said

experiment on 64 x 64 images of the CelebA dataset.

is
28
18
O

BM3D

Goooge
‘bbb

Ground
Truth

Noisy
Input

DIFFUSION

ete ee

ape

GAN

BM3D

SS

Ss

Figure 3: Results for our diffusion-based method on the removing MNIST digits experiment on an
out of distribution dataset, generated using stable diffusion (Rombach et al.||2022).
Under review as a conference paper at ICLR 2023

4.2.2 PERFORMANCE

To highlight the difference in inference time between our method and the baselines, benchmarks
are performed on a single 12GBytes NVIDIA GeForce RTX 3080 Ti, see Table[]in Appendix [B.2}
Although this is not an extensive benchmark, a quick comparison of inference times reveals a 50x
difference in speed between ours and the flow-based method. All the deep generative models need
approximately an equal amount of iterations (T ~ 600) to converge. However, for the same model-
ing capacity, the flow model requires a substantial higher amount of trainable parameters compared
to the diffusion method. This is mainly due to the restrictive requirements imposed on the architec-
ture to ensure tractable likelihood computation. It should be noted that no improvements to speed

up the diffusion process, such as CCDF (Chung et al.||2022b) are applied for the diffusion method,
giving room for even more improvement in future work.

5 DISCUSSION AND CONCLUSIONS

In this work, we present a framework for removing structured noise using diffusion models. Our
work provides an efficient addition to existing score-based conditional sampling methods incor-
porating knowledge of the noise distribution. We demonstrate our method on natural and out-of-
distribution data and achieve increased performance over the state-of-the-art and established con-
ventional methods for complex inverse tasks. Additionally, the diffusion based method is substan-
tially easier to train using the score matching objective compared to other deep generative methods
and furthermore allows for posterior sampling.

While our method is considerably faster and better in removing structured noise compared to the
flow-based method Te doen it is not ready (yet) for real-time inference and still slow
compared to GANs (Bora et al.| 2017) and classical methods. Luckily, research into accelerating
the diffusion process are well on their way. In addition, although a simple sampling algorithm
was adapted in this work, many more sampling algorithms for score-based diffusion models exist,
each of which introduces a new set of hyperparameters. For example, the predictor-corrector (PC)
sampler has been shown to improve sample quality (2020). Future work should explore
this wide increase in design space to understand limitations and possibilities of more sophisticated
sampling schemes in combination with the proposed joint diffusion method. Furthermore, the range
of problems to which we can apply the proposed method, can be expanded into non-linear likelihood
models and extend beyond the additive noise models.

Lastly, the connection between diffusion models and continuous normalizing flows through the neu-
ral ODE formulation is not investigated, but greatly of interest given the com-
parison with the flow-based method in this work.

6 REPRODUCIBILITY STATEMENT

All code used to train and evaluate the models as presented in this paper can be found at |https:]

//anonymous . 4open. science/r/iclr2023-joint-diffusion| Essentially, the codebase in
ttps://github . com/yang- song/score_sde_pytorch] of [Song et al.] (2020) is used to train

the score-based diffusion networks, for both data and structured noise, independently. To implement
the proposed inference scheme, the lines in Algorithm[I]should be adapted to create a sampler that
includes both trained diffusion models. Details regarding the training and inference settings used to
reproduce the results in this work can be found in SectionB2]

REFERENCES

Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Ap-
plications, 12(3):313-326, 1982. Al

Muhammad Asim, Max Daniels, Oscar Leong, Ali Ahmed, and Paul Hand. Invertible generative
models for inverse problems: mitigating representation error and dataset bias. In /nternational
Conference on Machine Learning, pp. 399-409. PMLR, 2020. Hey
Under review as a conference paper at ICLR 2023

Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Gold-
blum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms
without noise. arXiv preprint arXiv:2208.09392, 2022. fy]

Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2(1):183-202, 2009.

Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using genera-
tive models. In International Conference on Machine Learning, pp. 537-546. PMLR, 2017.

Hyungjin Chung and Jong Chul Ye. Score-based diffusion models for accelerated mri. Medical
Image Analysis, pp. 102479, 2022.

Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for
inverse problems using manifold constraints. arXiv preprint arXiv:2206.00941, 2022a.

Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating con-
ditional diffusion models for inverse problems through stochastic contraction. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12413-12422,

2022b.

Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising with
block-matching and 3d filtering. In Image processing: algorithms and systems, neural networks,
and machine learning, volume 6064, pp. 354-365. SPIE, 2006. (6)[8} [15]

Giannis Daras, Yuval Dagan, Alex Dimakis, and Constantinos Daskalakis. Score-guided interme-
diate level optimization: Fast langevin mixing for inverse problems. In Jnternational Conference
on Machine Learning, pp. 4722-4753. PMLR, 2022a.

Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G. Dimakis, and Peyman Milanfar.
Soft diffusion: Score matching for general corruptions, 2022b.

Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv: 1605.08803, 2016. 6]

Yonina C Eldar and Gitta Kutyniok. Compressed sensing: theory and applications. Cambridge
university press, 2012. [I]

Jan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the
ACM, 63(11):139-144, 2020. 3]

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
Neural Information Processing Systems, 33:6840-685 1, 2020.

Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir. Robust
compressed sensing mri with deep generative priors. Advances in Neural Information Processing
Systems, 34:14938-14954, 202 1a.

Ajil Jalal, Sushrut Karmalkar, Alex Dimakis, and Eric Price. Instance-optimal compressed sens-
ing via posterior sampling. In International Conference on Machine Learning, pp. 4709-4720.
PMLR, 2021b.

Bowen Jing, Gabriele Corso, Renato Berlinghieri, and Tommi Jaakkola. Subspace diffusion gener-
ative models. arXiv preprint arXiv:2205.01490, 2022. 2

Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-
based generative models. arXiv preprint arXiv:2206.00364, 2022.

Bahjat Kawar, Gregory Vaksman, and Michael Elad. Snips: Solving noisy inverse problems stochas-
tically. Advances in Neural Information Processing Systems, 34:21757-21769, 2021. B]

Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
Advances in neural information processing systems, 31, 2018. 6

10
Under review as a conference paper at ICLR 2023

Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015. [6]

Calvin Luo. Understanding diffusion models: A unified perspective. arXiv preprint
arXiv:2208.11970, 2022.

Stéphane Mallat. A wavelet tour of signal processing. Elsevier, 1999.

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv: 1511.06434, 2015. 6]

Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. {i

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjérn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pp. 10684-10695, 2022.

Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In
International Conference on Learning Representations, 2021. 2]

Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in Neural Information Processing Systems, 32, 2019. A]

Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
Advances in neural information processing systems, 33:12438—12448, 2020. Alle)

Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In Interna-
tional Conference on Learning Representations, 2020. Al} (6) 9]

Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-
based diffusion models. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 1415-1428.
Curran Associates, Inc., 2021a.

Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging
with score-based generative models. In International Conference on Learning Representations,

2021b. [5]

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267—288, 1996. {6}[15]

Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space.
Advances in Neural Information Processing Systems, 34:11287—11302, 2021. iA

Pascal Vincent. A connection between score matching and denoising autoencoders. Neural compu-
tation, 23(7):1661-1674, 2011.

Xinyi Wei, Hans van Gorp, Lizeth Gonzalez-Carabarin, Daniel Freedman, Yonina C Eldar, and
Ruud JG van Sloun. Deep unfolding with normalizing flow priors for inverse problems. [EEE
Transactions on Signal Processing, 70:2962-2971, 2022.

Jay Whang, Qi Lei, and Alex Dimakis. Solving inverse problems with a flow-based noise model. In
International Conference on Machine Learning, pp. 11146-11157. PMLR, 2021. ABET

11
Under review as a conference paper at ICLR 2023

x0 15) 053296 078 55D 01 4G 30 First oar 1b) 016 one OT 9 0M] 49
.
jos 25 fo.
25 e e B 0.8
© : 108 5 % oof H 406 5
joa joa
15 : I5f .
Jo | a to2
10 107 :
T T T 0.0 T T T T 0.0
> eS = 9 > eS =
Y S) S S ‘a $ S)
° © & v ° © &
$ $
9 9
(a) CelebA (b) Out-of-distribution data

Figure 4: Quantitative results using PSNR (green) and SSIM (blue) for the compressed sensing with
sinusoidal noise experiment on 64 x 64 images of the (a) CelebA and (b) out-of-distribution datasets.

A ADDITIONAL EXPERIMENTS

The following section explores additional inverse problems with compressed sensing and structured
noise. The goal is to show the performance of the proposed method in a variety of settings.

A.1 STRUCTURED NOISE WITH COMPRESSED SENSING

The corruption process is defined by y = Ax + Ngine With a random Gaussian measurement matrix
Qnk

A € R™*4 and a noise with sinusoidal variance o;, x exp(sin(22*)) for each pixel k. The
subsampling factor is defined by the size of the measurement matrix d/m. In this experiment, the
score network sy is trained on a dataset generated with sinusoidal noise samples Ngine. In Fig. [5]the
results of the compressed sensing experiment and the comparison with the baselines are shown for
an average standard deviation of oy = 0.2 and subsampling of factor d/m = 2. Given the same
hyperparameter settings, we repeat the experiment on the out-of-distribution (OoD) dataset, shown
in Fig. [6] Similar to the results found in Section [4.2.1] the diffusion method is more robust to the
shift in distribution and is able to deliver high quality recovery under the structured noise setting.
In contrast, the flow-based method under-performs when subjected to the OoD data. Quantitative

results on both CelebA and OoD are found in Fig. [4]as well as Table[I]and Table [2 respectively.

A.2 REMOVING SINUSOIDAL NOISE

The corruption process is defined by y = x + Mgine Where the noise variance a, o< exp(sin(2#))
follows a sinusoidal pattern along each row of the image k. In this experiment, the score network sy
is trained on a dataset generated with 1D sinusoidal noise samples Nine. See Fig.[8}for a comparison
of our method to the flow-based method for varying noise variances. Both methods perform quite
well, with the diffusion method having a slight edge. A visual comparison in Fig.[7| however, reveals

that the diffusion method preserves more detail in general.

12
Under review as a conference paper at ICLR 2023

Ground
GAN FLOW DIFFUSION Truth

LASSO

Figure 5: Comparison of results from our diffusion method compared to the baselines on the com-
pressed sensing with sinusoidal noise experiment with d/m = 2, on = 0.2 on 64 x 64 images of
the CelebA dataset.

GAN FLOW DIFFUSION

LASSO

Figure 6: Results for our diffusion-based method on the compressed sensing with sinusoidal noise
experiment on an out-of-distribution dataset, generated using stable diffusion (Rombach et al.

2022}.
Under review as a conference paper at ICLR 2023

Ground
Truth

Noisy
GAN FLOW _ DIFFUSION Input

BM3D

IDininio

Figure 7: Comparison of results from our diffusion method compared to the baselines on the remov-
ing sinusoidal noise experiment with ov = 0.2 on 64 x 64 images of the CelebA dataset.

— FLOW
' DIFFUSION ]

PSNR

18 -

16 -

4p

T
0.1 0.2 0.3
Avg. Noise Std. ov

Figure 8: Comparison of PSNR values for varying sinusoidal noise variances. Shaded areas repre-
sent the standard deviation on the metric.

14
Under review as a conference paper at ICLR 2023

B EXTENDED RESULTS
B.1 METRICS

Table 1: Results for the experiments and different methods on the CelebA dataset.

*Ours, {Whang et al.|(2021}, ‘Bora et al.|(2017}, {Dabov et al.|(2006}, ‘{Tibshiranil(1996).

MNIST CS + sine noise
PSNR SSIM PSNR SSIM
*DIFFUSION 27.2641.925 0.865+40.039 25.5141.040 0.823 + 0.044
FLOW 22.904 1.214 0.827+0.051 24.9642.292 0.779 + 0.082
'GAN 17.50 + 1.404 0.486+0.099 18.904 1.343 0.529 + 0.084
5BM3D 11.56+ 1.879 0.326+0.059_ - -
ILASSO - - 12.934 1.819 0.284 + 0.037

Table 2: Results for the experiments and different methods on the out-of-distribution (OoD) dataset.
*Ours, Whang et al./(2021), ‘Bora et al. (2017), ‘Dabov et al. (2006), {Tibshirani 1996).

MNIST CS + sine noise
PSNR SSIM PSNR SSIM
*DIFFUSION 22.8744.581 0.84240.110 22.904 1.568 0.823 + 0.082
FLOW 19.98 + 1.946 0.824+0.081 19.85+44.840 0.608 + 0.176
'GAN 13.06 + 1.788 0.218+0.088 12.39+41.693 0.159 + 0.070
5BM3D 10.44+ 1.446 0.274+0.069_ - -
ILASSO - - 11.62 41.473 0.336 + 0.057

B.2 COMPUTATIONAL PERFORMANCE

Table 3: Whang eral 0021), (Bora eta ort et a 2017 for all Dabov eta

Number of trainable parameters Inference time / image [ms]

*DIFFUSION 8.9M 1292 [2.153 /it}
‘FLOW 25.8M 61853 [103.1 / it]
'GAN 3.9M 59 [0.059 / it]
*BM3D - 28.5

15
Under review as a conference paper at ICLR 2023

C PSEUDO-CODE

In this section, we provide pseudo-code for the proposed joint conditional diffusion sampler with
the Euler-Maruyama sampling algorithm as basis. Furthermore, we use the SDE formulation for the
diffusion process which is denoted as an sde object with drift, diffusion and marginal_prob
methods. The latter computes the mean and standard deviation of the diffusion transition ker-
nel at a certain time t. Lastly, there are two trained score networks (NCSNv2) score_data and
score_noise for the data and structured noise respectively.

def joint_cond_diffusion_sampler(y, lambda_coeff, mu_coeff, num_steps)
dt = 1/num_steps
x random.normal (y. shape)
n random.normal (y. shape)

for t in linspace(1, 0, num_steps)
# corrupt observation along the diffusion process
mean, std = sde.marginal_prob(y, t)
y_hat = mean + std * random.normal (y. shape)

# data consistency step for x (data)
x = x - lambda_coeff * A.T @ (A @ x - y_hat + n)

# data consistency step for n (noise)
n = n - mu_coeff * (n - y_hat + A @ x)

# reverse diffusion step for x (data)
z = random.normal (x. shape)

x_hat = x - sde.drift(t) * x * dt
x_hat = x_hat + sde.diffusion(t)**2 * score_data(x, t) * dt
x_hat = x_hat + sde.diffusion(t) * sqrt(dt) * z

xX = x_hat

# reverse diffusion step for n (noise)
z = random.normal (n. shape)

n_hat = n - sde.drift(t) * n * dt
n_hat = n_hat + sde.diffusion(t)**2 * score_noise(n, t) * dt
n_hat = n_hat + sde.diffusion(t) * sqrt(dt) * z

n = n_hat
# return the denoised sample xly
return x

16
