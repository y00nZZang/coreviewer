Published as a conference paper at ICLR 2023

HYPERDEEPONET: LEARNING OPERATOR WITH COM-
PLEX TARGET FUNCTION SPACE USING THE LIMITED
RESOURCES VIA HYPERNETWORK

Jae Yong Lee* Sung Woong Cho*& Hyung Ju Hwang ¢

Center for Artificial Intelligence and Natural Sciences Department of Mathematics

Korea Institute for Advanced Study Pohang University of Science and Technology

Seoul, 02455, Republic of Korea Pohang, 37673, Republic of Korea

{jaeyong}@kias -re.kr {swcho95kr, hjhwang}@postech -ac.kr
ABSTRACT

Fast and accurate predictions for complex physical dynamics are a significant chal-
lenge across various applications. Real-time prediction on resource-constrained
hardware is even more crucial in real-world problems. The deep operator network
(DeepONet) has recently been proposed as a framework for learning nonlinear
mappings between function spaces. However, the DeepONet requires many pa-
rameters and has a high computational cost when learning operators, particularly
those with complex (discontinuous or non-smooth) target functions. This study
proposes HyperDeepONet, which uses the expressive power of the hypernetwork
to enable the learning of a complex operator with a smaller set of parameters.
The DeepONet and its variant models can be thought of as a method of inject-
ing the input function information into the target function. From this perspec-
tive, these models can be viewed as a particular case of HyperDeepONet. We
analyze the complexity of DeepONet and conclude that HyperDeepONet needs
relatively lower complexity to obtain the desired accuracy for operator learning.
HyperDeepONet successfully learned various operators with fewer computational
resources compared to other benchmarks.

1 INTRODUCTION

Operator learning for mapping between infinite-dimensional function spaces is a challenging prob-
lem. It has been used in many applications, such as climate prediction and fluid
dynamics (Guo et al.|/2016). The computational efficiency of learning the mapping is still impor-
tant in real-world problems. The target function of the operator can be discontinuous or sharp for
complicated dynamic systems. In this case, balancing model complexity and cost for computational
time is a core problem for the real-time prediction on resource-constrained hardware

2020} {Murshed et al.]{2021).

Many machine learning methods and deep learning-based architectures have been successfully de-
veloped to learn a nonlinear mapping from an infinite-dimensional Banach space to another. They
focus on learning the solution operator of some partial differential equations (PDEs), e.g., the initial
or boundary condition of PDE to the corresponding solution. [Anandkumar et al.|(2019) proposed an
iterative neural operator scheme to learn the solution operator of PDEs.

Simultaneously, (2019|/2021) proposed a deep operator network (DeepONet) architecture
based on the universal operator approximation theorem of (1995). The DeepONet

consists of two networks: branch net taking an input function at fixed finite locations, and trunk net
taking a query location of the output function domain. Each network provides the p outputs. The
two p-outputs are combined as a linear combination (inner-product) to approximate the underlying
operator, where the branch net produces the coefficients (p-coefficients) and the trunk net produces
the basis functions (p-basis) of the target function.

*These authors contributed equally.
t corresponding author
Published as a conference paper at ICLR 2023

While variant models of DeepONet have been developed to improve the vanilla DeepONet, they still
have difficulty approximating the operator for a complicated target function with limited computa-
tional resources. |Lanthaler et al.|(2022) and{Kovachki et al.|(20216) pointed out the limitation of lin-
ear approximation in DeepONet. Some operators have a slow spectral decay rate of the Kolmogorov
n-width, which defines the error of the best possible linear approximation using an n—dimensional
space. A large n is required to learn such operators accurately, which implies that the DeepONet
requires a large number of basis p and network parameters for them.

[Hadorn] (2022) investigated the behavior of DeepONet, to find what makes it challenging to detect
the sharp features in the target function when the number of basis p is small. They proposed a
Shift-DeepONet by adding two neural networks to shift and scale the input function.
also analyzed the limitation of DeepONet via singular value decomposition (SVD)
and proposed a flexible DeepONet (flexDeepONet), adding a pre-net and an additional output in
the branch net. Recently, to overcome the limitation of the linear approximation,
proposed a nonlinear manifold decoder (NOMAD) framework by using a neural network

at takes the output of the branch net as the input along with the query location. Even though these
methods reduce the number of basis functions, the total number of parameters in the model cannot
be decreased. The trunk net still requires many parameters to learn the complex operators, especially
with the complicated (discontinuous or non-smooth) target functions.

In this study, we propose a new architecture, HyperDeepONet, which enables operator learning,
and involves a complex target function space even with limited resources. The HyperDeepONet
uses a hypernetwork, as proposed by [Ha et al.| (2017), which produces parameters for the target
network. pointed out that the final inner product in DeepONet may be inefficient
if the information of the input function fails to propagate through a branch net. The hypernetwork in
HyperDeepONet transmits the information of the input function to each target network’s parameters.
Furthermore, the expressivity of the hypernetwork reduces the neural network complexity by sharing

the parameters (Galanti & Wolf||/2020). Our main contributions are as follows.

¢ We propose a novel HyperDeepONet using a hypernetwork to overcome the limitations of
DeepONet and learn the operators with a complicated target function space. The DeepONet
and its variant models are analyzed primarily in terms of expressing the target function
as a neural network (Figure {4). These models can be simplified versions of our general
HyperDeepONet model (Figuref5}.

¢ We analyze the complexity of DeepONet (Theorem|2} and prove that the complexity of the
HyperDeepONet is lower than that of the DeepONet. We have identified that the Deep-
ONet should employ a large number of basis to obtain the desired accuracy, so it requires
numerous parameters. For variants of DeepONet combined with nonlinear reconstructors,
we also present a lower bound for the number of parameters in the target network.

¢ The experiments show that the HyperDeepONet facilitates learning an operator with a small
number of parameters in the target network even when the target function space is compli-
cated with discontinuity and sharpness, which the DeepONet and its variants suffer from.
The HyperDeepONet learns the operator more accurately even when the total number of
parameters in the overall model is the same.

2 RELATED WORK

Many machine learning methods and deep learning-based architectures have been successfully de-
veloped to solve PDEs with several advantages. One research direction is to use the neural network
directly to represent the solution of PDE (E & Yul 2018} Sirignano & Spiliopoulos| 2018). The
physics-informed neural network (PINN), introduced by|Raissi et al. (2019), minimized the residual
of PDEs by using automatic differentiation instead of numerical approximations.

There is another approach to solve PDEs, called operator learning. Operator learning aims to learn
a nonlinear mapping from an infinite-dimensional Banach space to another. Many studies utilize the
convolutional neural network to parameterize the solution operator of PDEs in various applications

(Guo etal P01g Bhatnagar et al.}|2019 2021 2019 2021). The
neural operator (Kovachki et al.|/2021b} e noni

) is proposed to approximate the nonlinear operator inspired
by Green’s function. (2021) extend the neural operator structure to the Fourier Neural

Published as a conference paper at ICLR 2023

Operator (FNO) to approximate the integral operator effectively using the fast Fourier transform

(FFT).|Kovachki et al.|(202 1a) proved the universality of FNO and identified the size of the network.

The DeepONet (Lu et al.|/2019{|2021) has also been proposed as another framework for operator
learning. The DeepONet has significantly been applied to various problems, such as bubble growth

dynamics (Lin et al.]|2021), hypersonic flows 2021), and fluid flow 2021).

(2022) provided the universal ap- Output
proximation property of DeepONet. [Wang et al. MES 05.2172)
(2021) proposed physics-informed DeepONet_by “ *
adding a residual of PDE as a loss function, and|Ryck' Input re Ly.

(2022) demonstrated the generic bounds REE O, x12) 0 whiting

on the approximation error for it. |Prasthofer et al.
considered the case where the discretization
grid of the input function in DeepONet changes
by employing the coordinate encoder. |Lu et al.
(2022) compared the FNO with DeepONet in differ-
ent benchmarks to demonstrate the relative perfor-
mance. FNO can only infer the output function of
an operator as the input function in the same grid as
it needs to discretize the output function to use Fast
Fourier Transform(FFT). In contrast, the DeepONet
can predict from any location.

(2017) first proposed hypernetwork, a network that creates a weight of the primary net-
work. Because the hypernetwork can achieve weight sharing and model compression, it requires a

relatively small number of parameters even as the dataset grows. (2020) proved that
a hypernetwork provides higher expressivity with low-complexity target networks.
(2020) and|Klocek et al.|(2019) employed this approach to restoring images with insufficient pixel
observations or resolutions. investigated the relationship be-
tween the coefficients of PDEs and the corresponding solutions. They combined the hypernetwork
with the PINN’s residual loss. For time-dependent PDE, [Pan et al.| (2022) designated the time ¢ as
the input of the hypernetwork so that the target network indicates the solution at t.
(2020) devised a chunk embedding method that partitions the parameters of the target network; this
is because the output dimension of the hypernetwork can be large.

os

Figure 1: Example of operator learning: the
input function and the output function for the
solution operator of shallow water equation

3 OPERATOR LEARNING

3.1 PROBLEM SETTING

The goal of operator learning is to learn a mapping from infinite-dimensional function space to the
others using a finite pair of functions. Let G : U — S be a nonlinear operator, where U/ and S are
compact subsets of infinite-dimensional function spaces U C C(¥;R™) and S C C(Y; R*) with
compact domains ¥ C R® and Y C R®. For simplicity, we focus on the case d,, = d, = 1, and all
the results could be extended to a more general case for arbitrary d,, and d,. Suppose we have obser-
vations {u;,G(ui)}_, where u; € U and G(u;) € S. We aim to find an approximation Gy : U > S
with parameter @ using the N observations so that Gg + G. For example, in a dam break scenario, it
is an important to predict the fluid flow over time according to a random initial height of the fluid.
To this end, we want to find an operator Gg, which takes an initial fluid G

height as an input function and produces the fluid height over time at u —— $§

any location as the output function (Figure[I). | |

As explained in {Lanthaler et al.| (2022), the approximator Gg can be ; R
decomposed into the three components (Figure|2) as H i
R™ -R?

Go = RoAo€. (1) A
First, the encoder € takes an input function u from U/ to generate the __ .
finite-dimensional encoded data in R”. Then, the approximator A isan Figure 2: Diagram for the
operator approximator from the encoded data in finite dimension space ‘ree components for op-
R™ to the other finite-dimensional space R?. Finally, the reconstructor ¢tator learning.
R reconstructs the output function s(y) = G(u)(y) with y € ) using the approximated data in R?.
Published as a conference paper at ICLR 2023

3.2 DEEPONET AND ITS LIMITATION

DeepONet can be analyzed using the above three decompositions. Assume that all the input func-
tions u are evaluated at fixed locations {x;}'" C 4; they are called ”sensor points.” DeepONet
uses an encoder as the pointwise projection E(u) = (u(21), u(x2),...,u(am)) of the continuous
function u, the so-called ’sensor values” of the input function wu. An intuitive idea is to employ a
neural network that simply concatenates these m sensor values and a query point y as an input to
approximate the target function G(u)(y). DeepONet, in contrast, handles m sensor values and a
query point y separately into two subnetworks based on the universal approximation theorem for the
operator (Lu et al.|/2021). See Appendix|B]for more details. use the fully connected
neural network for the approximator A : R™” — R?. They referred to the composition of these two
maps as branch net

B:U +R”, B(u) = Ao E(u) (2)
for any u € U. The role of branch net can be interpreted as learning the coefficient of the target
function G(u)(y). They use one additional neural network, called trunk net 7 as shown below.

7: YR? r(y) = {re(y) Ho GB)

for any y € Y. The role of trunk net can be interpreted as learning an affine space V that can
efficiently approximate output function space C(Y; R*:). The functions 7 (y), ....Tp(y) become the
p-basis of vector space associated with V and 79(y) becomes a point of V. By using the trunk net
7, the 7T-induced reconstructor FR is defined as

P
Ry: RP > C(V;R™), Rr(B)(y) = Toy) + D> Bere (y) (4)
k=1
where 3 = ((1, 82, ..., 8p) € R?. In DeepONet, 79(y) is restricted to be a constant 7) € R that is
contained in a reconstructor . The architecture of DeepONet is described in Figure/4](b).

Here, the 7-induced reconstructor R is the linear approximation of the output function space. Be-
cause the linear approximation R cannot consider the elements in its orthogonal complement, a

priori limitation on the best error of DeepONet is explained in|Lanthaler et al.](2022) as

1

(/ [seam -R oAeelu)(y)Paudy(u)) > j= 5)

where A; > Az > ... are the eigenvalues of the covariance operator T'g,,,, of the push-forward
measure Gy,,. Several studies point out that the slow decay rate of the lower bound leads to
inaccurate approximation operator learning using DeepONet (Kovachki et al.| 202 1b} Hadorn|

2022} |Lanthaler et al. 2022). For example, the solution operator of the advection PDEs (Sei-|
dman et al.| [2022] [Venturi & Casey] |2023) and of the Burgers’ equation 2022) are

difficult to approximate when we are using the DeepONet with the small number of basis p.

3.3. VARIANT MODELS OF DEEPONET How to put information of uweEU

Several variants of DeepONet have been developed to
overcome its limitation. All these models can be viewed
from the perspective of parametrizing the target function
as a neural network. When we think of the target net- |
work that receives y as an input and generates an output |
Go(u)(y), the DeepONet and its variant model can be dis-
tinguished by how information from the input function u
is injected into this target network Gg(u), as described
in Figure [3] From this perspective, the trunk net in the
DeepONet can be considered as the target network ex-
cept for the final output, as shown in Figure /4}(a). The
output of the branch net gives the weight between the last hidden layer and the final output.

[Hadorn] (2022) proposed Shift-DeepONet. The main idea is that a scale net and a shift net are used
to shift and scale the input query position y. Therefore, it can be considered that the information of

e
LO
Sof

Target network

Figure 3: The perspective target network
parametrization for operator learning.
Published as a conference paper at ICLR 2023

u(x)
oo
iH, bias to | Prey

Be,

heuoo @ Gow)

PRPS eS 8 Feet
‘Trunk net y Trunk net "* Trunk net “? - _
‘Target network Target network ‘Target network Target network
(a) DeepONet (b) Shift-DeepONet (c) FlexDeepONet (d) NOMAD

Figure 4: DeepONet and its variant models for operator learning.

input function uw generates the weights and bias between the input layer and the first hidden layer, as
explained in Figure[4](b).

Venturi & Casey} (2023) proposed FlexDeepONet, explained in Figure [4] (c). They used the addi-

tional network, pre-net, to give the bias between the input layer and the first hidden layer. Addition-
ally, the output of the branch net also admits the additional output 7) to provide more information
on input function wu at the last inner product layer.

NOMAD is recently developed by (2022) to overcome the limitation of DeepONet.
nifold using a n

They devise a nonlinear output mai eural network that takes the output of branch net
{6;}?_, and the query location y. As explained in Figure|4] (d), the target network receives infor-
mation about the function u as an additional input, similar to other conventional neural embedding

methods (Park et al} 2019} |Chen & Zhang’ 2019} Mescheder et al.|/2019).

These methods provide information on the input function u to only a part of the target network. It is
a natural idea to use a hypernetwork to share the information of input function wu to all parameters
of the target network. We propose a general model HyperDeepONet (Figure[5), which contains the
vanilla DeepONet, FlexDeepONet, and Shift-DeepONet, as a special case of the HyperDeepONet.

4 PROPOSED MODEL: HYPERDEEPONET

4.1 ARCHITECTURE OF HYPERDEEPONET

The HyperDeepONet structure is described in Figure [5]
The encoder € and the approximator A are used, simi-
lar to the vanilla DeepONet. The proposed structure re-
places the branch net with the hypernetwork. The hyper-
network generate all parameters of the target network.
More precisely, we define the hypernetwork h as

ho :U > R?, he(u) := Ao E(u) (6)

for any u € U. Then, h(u) = O € R? isa network |

parameter of the target network, which is used in recon- > Target network
structor for the HyperDeepONet. We define the recon-

structor R as

Figure 5: The proposed HyperDeep-
R: R? > C(Y;R*), R(O)(y) = NN(y;®) (7) ONet structure

where © = [W,] € R”, and NN denotes the target network. Two fully connected neural networks
are employed for the hypernetwork and target network.

Therefore, the main idea is to use the hypernetwork, which takes an input function u and produces
the weights of the target network. It can be thought of as a weight generator for the target network.
The hypernetwork determines the all parameters of the target network containing the weights be-
tween the final hidden layer and the output layer. It implies that the structure of HyperDeepONet
contains the entire structure of DeepONet. As shown in Figure[4](b) and (c), Shift-DeepONet and
FlexDeepONet can also be viewed as special cases of the HyperDeepONet, where the output of the
hypernetwork determines the weights or biases of some layers of the target network. The outputs of
Published as a conference paper at ICLR 2023

the hypernetwork determine the biases for the first hidden layer in the target network for NOMAD
in Figure[4](d).

4.2 COMPARISON ON COMPLEXITY OF DEEPONET AND HYPERDEEPONET

In this section, we would like to clarify the complexity of the DeepONet required for the approxi-

mation A and reconstruction R based on the theory in/Galanti & Wolf|(2020). Furthermore, we will

show that the HyperDeepONet entails a relatively lower complexity than the DeepONet using the

results on the upper bound for the complexity of hypernetwork (Galanti & Wolf] |2020).

4.2.1 NOTATIONS AND DEFINITIONS

Suppose that the pointwise projection values (sensor values) of the input function wu is given as
E(u) = (u(x), u(xz),...,U(%m)). For simplicity, we consider the case Y = [—1, 1]*" and E(u) €
[-1,1]”. For the composition R o A: R”™ — C(¥;R), we focus on approximating the mapping
O : R™+*4y _s R, which is defined as follows:

O(E(u),y) = (Ro A(E(u)))(y), for y € [-1, 1], €(u) € [-1, 1]. (8)

The supremum norm ||h||o. is defined as maxycy ||h(y)|]. Now, we introduce the Sobolev space
W,.n, Which is a subset of C"([—1, 1]";R). For r,n € N,

Wan = {ns(-tnP oR Walle = Walco + [D*n. <1},

1s|k|<r

where D¥h denotes the partial derivative of h with respect to multi-index k € {NU {0}}"". We
assume that the mapping O lies in the Sobolev space Wy,m-+dy+

For the nonlinear activation a, the class of neural network F represents the fully connected neural
network with depth k and corresponding width (hy =n, ho,--- ,he41), where W* € R® x Rev
and b; € R’+1 denote the weights and bias of the i-th layer respectively.

F:={f:[-11]" > Ri f(y: [W. b]) = W* -o(W*t-.-o(Wt-y +b!) +081) + OF}

Some activation functions facilitate an approximation for the Sobolev space and curtail the com-
plexity. We will refer to these functions as universal activation functions. The formal definition
can be found below, where the distance between the class of neural network F and the Sobolev
space W,,,, is defined by d(F;W,,n) ‘= SUPyeywy,,,, nfrex lf — Pleo. Most well-known activa-
tion functions are universal activations that are infinitely differentiable and non-polynomial in any
interval (Mhaskar|{1996). Furthermore, |Hanin & Sellke|(2017) state that the ReLU activation is also
universal.

Definition 1. (Galanti & Wolf 2020) (Universal activation). The activation function o is called
universal if there exists a class of neural network F with activation function o such that the number
of parameters of F is O(e—"/") with d(F; Win) < € for allr,n EN.

We now introduce the theorem, which offers a guideline on the neural network architecture for
operator learning. It suggests that if the entire architecture can be replaced with a fully connected
neural network, large complexity should be required for approximating the target function. It also
verifies that the lower bound for a universal activation function is a sharp bound on the number of
parameters. First, we give an assumption to obtain the theorem.

Assumption 1. Suppose that F and W,.,, represent the class of neural network and the target func-
tion space to approximate, respectively. Let F' be a neural network class representing a structure in
which one neuron is added rather than F. Then, the followings holds for ally € Wyn not contained
in F.

inf || f —w > inf || f —wWllo.

inf If lloo jer lf — Pleo

For r = 0,|Galanti & Wolf|(2020) remark that the assumption is valid for 2-layered neural networks

with respect to the L* norm when an activation function @ is either a hyperbolic tangent or sigmoid
function. With Assumption|!} the following theorem holds, which is a fundamental approach to
identifying the complexity of DeepONet and its variants. Note that a real-valued function g € L1(R)
is called a bounded variation if its total variation supy<c1(R),\)4|).<1 Jz 9(@)¢ (a)dz is finite.
Published as a conference paper at ICLR 2023

Theorem 1. (Galanti & Wolf,|2020). Suppose that F is a class of neural networks with a piecewise

C1(R) activation function o : TR — R of which derivative o' is bounded variation. If any non-
constant  € W,.,, does not belong to F, then d(F; W,.»,) < € implies the number of parameters in

F should be Xe-"/").

4.2.2 LOWER BOUND FOR THE COMPLEXITY OF THE DEEPONET

Now, we provide the minimum number of parameters in DeepONet. The following theorem presents
a criterion on the DeepONet’s complexity to get the desired error. It states that the number of
required parameters increases when the target functions are irregular, corresponding to a small r.
F deeponet (B, T) denotes the class of function in DeepONet, induced by the class of branch net B
and the class of trunk net T.

Theorem 2. (Complexity of DeepONet) Let o : R — R be a universal activation function in C’(R)
such that o and o' are bounded. Suppose that the class of branch net B has a bounded Sobolev
norm (i.e., |||? <l,,V8 € B). Suppose any non-constant  € Wy. does not belong to any class
of neural network. In that case, the number of parameters in the class of trunk net T is Q(e~¢v/")

when d(Fpeeponet(B, T); Wray +m) < &

The core of this proof is showing that the inner product between the branch net and the trunk net
could be replaced with a neural network that has a low complexity (Lemmalfp. Therefore, the entire
structure of DeepONet could be replaced with a neural network that receives [€(u),y] € Rév+™
as input. It gives the lower bound for the number of parameters in DeepONet based on Theorem[]]
The proof can be found in Appendix|C.1]

The analogous results holds for variant models of DeepONet. Models such as Shift-DeepONet and
flexDeepONet could achieve the desired accuracy with a small number of basis. Still, there was a
trade-off in which the first hidden layer of the target network required numerous units. There was
no restriction on the dimension of the last hidden layer in the target network for NOMAD, which
uses a fully nonlinear reconstruction. However, the first hidden layer of the target network also had
to be wide enough, increasing the number of parameters. Details can be found in Appendix|C.2]

For the proposed HyperDeepONet, the sensor values E(w) determine the weight and bias of all
other layers as well as the weight of the last layer of the target network. Due to the nonlinear
activation functions between linear matrix multiplication, it is difficult to replace wot 20 ONet

with a single neural network that receives [E(u), y] € R¢v+™ as input. |Galanti & Wolf|(2020) state
that there exists a hypernetwork structure (HyperDeepONet) such that the number of parameters
in the target network is O(e~“"/"). It implies that the HyperDeepONet reduces the complexity
compared to all the variants of DeepONet.

5 EXPERIMENTS

In this section, we verify the effectiveness of the proposed model HyperDeepONet to learn the
operators with a complicated target function space. To be more specific, we focus on operator
learning problems in which the space of output function space is complicated. Each input function
u; generates multiple triplet data points (u;,y,G(u)(y)) for different values of y. Except for the
shallow water problem, which uses 100 training function pairs and 20 test pairs, we use 1,000
training input-output function pairs and 200 test pairs for all experiments.

For the toy example, we first consider the identity operator G : u; > u;. The Chebyshev polyno-
mial is used as the input (=output) for the identity operator problem. The Chebyshev polynomials of
the first kind T; of degree 20 can be written as uj € {Vy Ti(z)|er € [-1/4, 1/4]} with random
sampling c; from uniform distribution U[—1/4, 1/4]. The differentiation operator G : u; > u;
is considered for the second problem. Previous works handled the anti-derivative operator, which
makes the output function smoother by averaging (Lu et al.||2019}|2022). Here, we choose the dif-
ferentiation operator instead of the anti-derivative operator to focus on operator learning when the
operator’s output function space is complicated. We first sample the output function G(u) from the
above Chebyshev polynomial of degree 20. The input function is generated using the numerical
method that integrates the output function.
Published as a conference paper at ICLR 2023

Model DeepONet Shift Flex NOMAD _ Hyper(ours)
Identity 0.578£0.003  0.777£0.018 0.678£0.062 0.578£0.020 —0.036£0.005
Differentiation _0.559+0.001 _0.624+0.015 0.56240.016 0.558+0.003 _0.127+0.043

Table 1: The mean relative L? test error with standard deviation for the identity operator and the
differentiation operator. The DeepONet, its variants, and the HyperDeepONet use the target network
dy-20-20-10-1 with tanh activation function. Five training trials are performed independently.

(a) Input u(x) and output c(u)(x) _{b) Prediction using DeepOnet & variants | (c) Prediction using HyperDeepONet

: wl \

oo

serge futon uw
J — deoponet

— Shift-Deeponet

os] output os

ine eee 00 rN

ube)

as. as. fiexDeepONet
“100-075-050 -025 000 035 050 075 100 100 -075 -050 025 000 025 050 NOMAD [o7s 050-025 060 035 050 075 100
x

— HyperDeeponet,

Figure 6: One test data example of differentiation operator problem.

Finally, the solution operators of PDEs are considered. We deal with two problems with the complex
target function in previous works The solution operator of the
advection equation is considered a mapping gle shape initial input function to the
solution w(t, z) att = 0.5, ie., G : w(0,2) + w(0.5, 2). We also consider the solution operator
of Burgers’ equation which maps the random initial condition to the solution w(t, x) att = 1, ie.,
G : w(0,2) +> w(1,x). The solution of the Burgers’ equation has a discontinuity in a short time,
although the initial input function is smooth. For a challenging benchmark, we consider the solution
operator of the shallow water equation that aims to predict the fluid c(h. h(t, 21,22) from the

initial condition h(0, 71,2), i.e., G : h(0, 21,22) + A(t, x1, x2) (Figure/1). In this case, the input
of the target network is three dimension (t, x1, 22), which makes the solution operator complex.
Detail explanation is provided in Appendix

Expressivity of target network. We have compared the expressivity of the small target network
using different models. We focus on the identity and differentiation operators in this experiment.
All models employ the small target network d,-20-20-10-1 with the hyperbolic tangent activation
function. The branch net and the additional networks (scale net, shift net, pre-net, and hypernetwork)
also use the same network size as the target network for all five models.

Table[I]shows that the DeepONet and its variant models have high errors in learning complex oper-
ators when the small target network is used. In contrast, the HyperDeepONet has lower errors than
the other models. This is consistent with the theorem in the previous section that HyperDeepONet
can achieve improved approximations than the DeepONet when the complexity of the target net-
work is the same. Figure [6] shows a prediction on the differentiation operator, which has a highly
complex target function. The same trends are observed when the activation function or the number
of sensor points changes (Table|5) and the number of layers in the branch net and the hypernetwork
vary (Figure

Same number of learnable parameters. The previous experiments compare the models using
the same target network structure. In this section, the comparison between the DeepONet and the
HyperDeepONet is considered when using the same number of learnable parameters. We focus on
the solution operators of the PDEs.

Branch net (Hypernetwork) Target Param Rel error
DeepONet m-256-256 7,-256-256-256-256-1 274K __0,0046=0.001T

Advection —_ Hyper(ours) m-10-70-70-70-70-) dy-33-33-33-33-1 268K —_0.0048+0.0009
c-Hyper(ours) _m-128-128-128-128-128-1024 _dy-256-256-256-256-1___208K___0.0043+0.0004

DeepONet m-128- 128-128-128 d,-128-128-128-128-1 115K 0.0391=0.0040

Burgers Hyper(ours) m-66-66-66- dy-20-20-20-20-1 114K 0.0196+0.0044
c-Hyper(ours) m-66-66-66-66- dy-128-128-128-128-1__ 115K __0.0066+0.0009

Shallow DeepONet m-100-100-100-100 d,-100-100-100-100-1_ 107K 0.0279 = 0.0042
Hyper(ours) m-30-30-30-3 dy-30-30-30-30-1 101K 0.0148 + 0.0002

Shallow DeepONet m-20-20-10 <,,-20-20-10-1 635K 0.0391 = 0.0066

w/ small param _ Hyper(ours) m -10-10-10-No d,-10-10-10-1 5.7K 0.0209 + 0.0013

Table 2: The mean relative L? test error with standard deviation for solution operator learning
problems. Ng and #Param denote the number of parameters in the target network and the number
of learnable parameters, respectively. Five training trials are performed independently.
Published as a conference paper at ICLR 2023

ao -_(2) Input u(x) and output G(u)(x) sob) Prediction after 10000 epochs _._(c) Prediction after 40000 epochs
= ¥| — input ro 104
Sos output os] os
oo 00) Target function
05, -05 =) DecpONet ae ee |
oo ~«02”~=~« SCS SCS HyperDeepONet| of 06 08 10
5 -__(a) Input u(x) and output o(u)(x) 1s__(b) Prediction after 5000 epochs 1s _{¢) Prediction after 50000 epochs

— input

output
os os} os}
] oof

Figure 7: One test data example of prediction on the advection equation (First row) and Burgers’
equation (Second row) using the DeepONet and the HyperDeepONet.

ux)

For the three solution operator learning problems, we use the same hyperparameters proposed in
and {Seidman et al.](2022) for DeepONet. First, we use the smaller target network
with the larger hypernetwork for the HyperDeepONet to compare the DeepONet. Note that the
vanilla DeepONet is used without the output normalization or the boundary condition enforcing
techniques explained eee ore to focus on the primary limitation of the DeepONet. More
Details are in Appendix |E] Table [2] shows that the HyperDeepONet achieves a similar or better
performance than the DeepONet when the two models use the same number of learnable parameters.
The HyperDeepONet has a slightly higher error for advection equation problem, but this error is
close to perfect operator prediction. It shows that the complexity of target network and the number
of learnable parameters can be reduced to obtain the desired accuracy using the HyperDeepONet.
The fourth row of Table [2] shows that HyperDeepONet is much more effective than DeepONet in
approximating the solution operator of the shallow water equation when the number of parameters is
limited. Figure [7]and Figure [T2|show that the HyperDeepONet learns the complex target functions
in fewer epochs for the desired accuracy than the DeepONet although the HyperDeepONet requires
more time to train for one epoch (Table|8p.

Scalability. When the size of the target network for the HyperDeepONet is large, the output of
the hypernetwork would be high-dimensional (Ha et al. 2017} Pawlowski et al. 2017) so that its
complexity increases. In this case, the chunked HyperDeepONet (c-HyperDeepONet) can be used
with a trade-off between accuracy and memory based on the chunk embedding method developed
by (2020). It generates the subset of target network parameters multiple times
iteratively reusing the smaller chunked hypernetwork. The c-HyperDeepONet shows a better accu-
racy than the DeepONet and the HyperDeepONet using an almost similar number of parameters,
as shown in Table 2] However, it takes almost 2x training time and 2~30x memory usage than the
HyperDeepOnet. More details on the chunked hypernetwork are in Appendix |[D]

6 CONCLUSION AND DISCUSSION

In this work, the HyperDeepONet is developed to overcome the expressivity limitation of Deep-
ONet. The method of incorporating an additional network and a nonlinear reconstructor could not
thoroughly solve this limitation. The hypernetwork, which involves multiple weights simultane-
ously, had a desired complexity-reducing structure based on theory and experiments.

We only focused on when the hypernetwork and the target network is fully connected neural net-
works. In the future, the structure of the two networks can be replaced with a CNN or ResNet, as the
structure of the branch net and trunk net of DeepONet can be changed to another network
2022). Additionally, it seems interesting to research a simplified modulation network proposed by
(2021), which still has the same expressivity as HyperDeepONet.

Some techniques from implicit neural representation can improve the expressivity of the target net-
work (Sitzmann et al.|{2020). Using a sine function as an activation function with preprocessing will
promote the expressivity of the target network. We also leave the research on the class of activation
functions satisfying the assumption except for hyperbolic tangent or sigmoid functions as a future
work.
Published as a conference paper at ICLR 2023

ACKNOWLEDGMENTS

J. Y. Lee was supported by a KIAS Individual Grant (AP086901) via the Center for AI and Natural
Sciences at Korea Institute for Advanced Study and by the Center for Advanced Computation at
Korea Institute for Advanced Study. H. J. Hwang and S. W. Cho were supported by the National
Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (RS-2022-
00165268) and by Institute for Information & Communications Technology Promotion (IITP) grant
funded by the Korea government(MSIP) (No.2019-0-01906, Artificial Intelligence Graduate School
Program (POSTECH)).

REFERENCES

Anima Anandkumar, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Nikola Kovachki, Zongyi
Li, Burigede Liu, and Andrew Stuart. Neural operator: Graph kernel network for partial differ-
ential equations. In JCLR 2020 Workshop on Integration of Deep Neural Models and Differential

Equations, 2019. URL https: //openreview.net/forum?id=fg2Z2FmxXFO3

Saakaar Bhatnagar, Yaser Afshar, Shaowu Pan, Karthik Duraisamy, and Shailendra Kaushik. Pre-
diction of aerodynamic flow fields using convolutional neural networks. Comput. Mech., 64
(2):525-545, 2019. ISSN 0178-7675. doi: 10.1007/s00466-019-01740-0. URL
//doi.org/10.1007/s00466-019-01740-0

Shengze Cai, Zhicheng Wang, Lu Lu, Tamer A Zaki, and George Em Karniadakis. Deepm&mnet:
Inferring the electroconvection multiphysics fields based on operator approximation by neural
networks. Journal of Computational Physics, 436:110296, 2021.

Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks
with arbitrary activation functions and its application to dynamical systems. [EEE Transactions
on Neural Networks, 6(4):911-917, 1995.

Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5939-5948, 2019.

Tejalal Choudhary, Vipul Mishra, Anurag Goswami, and Jagannathan Sarangapani. A compre-
hensive survey on model compression and acceleration. Artificial Intelligence Review, 53(7):
5113-5155, 2020.

Filipe de Avila Belbute-Peres, Yi-fan Chen, and Fei Sha. Hyperpinn: Learning parameterized differ-
ential equations with physics-informed hypernetworks. In The Symbiosis of Deep Learning and
Differential Equations, 2021.

Weinan E and Bing Yu. The deep Ritz method: a deep learning-based numerical algorithm for
solving variational problems. Commun. Math. Stat., 6(1):1-12, 2018. ISSN 2194-6701. doi: 10.

1007/s40304-018-0127-z. URL|https://doi.org/10.1007/s40304-018-0127-z

Tomer Galanti and Lior Wolf. On the modularity of hypernetworks. Advances in Neural Information
Processing Systems, 33:10409-10419, 2020.

Xiaoxiao Guo, Wei Li, and Francesco Iorio. Convolutional neural networks for steady flow ap-
proximation. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge
discovery and data mining, pp. 481-490, 2016.

David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In International Conference on Learn-

ing Representations, 2017. URL|https: //openreview.net/forum?id=rkpACel1lx

Patrik Simon Hadorn. Shift-deeponet: Extending deep operator networks for discontinuous output
functions. ETH Zurich, Seminar for Applied Mathematics, 2022.

Boris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width.
arXiv preprint arXiv:1710.11278, 2017.

Rakhoon Hwang, Jae Yong Lee, Jin Young Shin, and Hyung Ju Hwang. Solving pde-constrained
control problems using operator learning. arXiv preprint arXiv:2111.04941, 2021.

10
Published as a conference paper at ICLR 2023

Yuehaw Khoo, Jianfeng Lu, and Lexing Ying. Solving parametric PDE problems with artificial
neural networks. European J. Appl. Math., 32(3):421-435, 2021. ISSN 0956-7925. doi: 10.1017/

$0956792520000182. URL|https://doi.org/10.1017/S0956792520000182

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL|http:|
//arxiv.org/abs/1412.6980

Sylwester Klocek, Lukasz Maziarka, Maciej Wotczyk, Jacek Tabor, Jakub Nowak, and Marek
Smieja. Hypernetwork functional image representation. In International Conference on Arti-
ficial Neural Networks, pp. 496-510. Springer, 2019.

Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra. On universal approximation and error
bounds for fourier neural operators. Journal of Machine Learning Research, 22:Art—No, 202 1a.

Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, An-
drew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces.
arXiv preprint arXiv:2108.08481, 2021b.

Thorsten Kurth, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David
Hall, Andrea Miele, Karthik Kashinath, and Animashree Anandkumar. Fourcastnet: Accelerating
global high-resolution weather forecasting using adaptive fourier neural operators. arXiv preprint
arXiv:2208.05419, 2022.

Samuel Lanthaler, Siddhartha Mishra, and George E Karniadakis. Error estimates for deeponets: A
deep learning framework in infinite dimensions. Transactions of Mathematics and Its Applica-
tions, 6(1):tnac001, 2022.

Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhat-
tacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial
differential equations. In International Conference on Learning Representations, 2021. URL

https: //openreview.net/forum?id=c8P 9NQVtmnO

Chensen Lin, Zhen Li, Lu Lu, Shengze Cai, Martin Maxey, and George Em Karniadakis. Operator
learning for predicting multiscale bubble growth dynamics. The Journal of Chemical Physics,
154(10):104118, 2021.

Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for iden-
tifying differential equations based on the universal approximation theorem of operators. arXiv
preprint arXiv:1910.03193, 2019.

Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning
nonlinear operators via DeepONet based on the universal approximation theorem of operators.
Nature Machine Intelligence, 3(3):218-229, 2021.

Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and
George Em Karniadakis. A comprehensive and fair comparison of two neural operators (with
practical extensions) based on fair data. Computer Methods in Applied Mechanics and Engineer-
ing, 393:114778, 2022.

Zhiping Mao, Lu Lu, Olaf Marxen, Tamer A. Zaki, and George Em Karniadakis. DeepM&Mnet for
hypersonics: predicting the coupled flow and finite-rate chemistry behind a normal shock using
neural-network approximation of operators. J. Comput. Phys., 447:Paper No. 110698, 24, 2021.

ISSN 0021-9991. doi: 10.1016/j.jcep.2021.110698. URL|https://doi.org/10.1016/j.

jep.2021.110698

Ishit Mehta, Michaél Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, and Manmohan
Chandraker. Modulated periodic activations for generalizable local functional representations. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14214-14223,
2021.

11
Published as a conference paper at ICLR 2023

Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Oc-
cupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pp. 4460-4470, 2019.

Hrushikesh N Mhaskar. Neural networks for optimal approximation of smooth and analytic func-
tions. Neural computation, 8(1):164-177, 1996.

MG Sarwar Murshed, Christopher Murphy, Daqing Hou, Nazar Khan, Ganesh Ananthanarayanan,
and Faraz Hussain. Machine learning at the network edge: A survey. ACM Computing Surveys
(CSUR), 54(8):1-37, 2021.

Shaowu Pan, Steven L Brunton, and J Nathan Kutz. Neural implicit flow: a mesh-agnostic dimen-
sionality reduction paradigm of spatio-temporal data. arXiv preprint arXiv:2204.03216, 2022.

Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.
Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition, pp. 165-174, 2019.

Nick Pawlowski, Andrew Brock, Matthew CH Lee, Martin Rajchl, and Ben Glocker. Implicit weight
uncertainty in neural networks. arXiv preprint arXiv: 1711.01297, 2017.

Michael Prasthofer, Tim De Ryck, and Siddhartha Mishra. Variable-input deep operator networks.
arXiv preprint arXiv:2205.11404, 2022.

M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: a deep learning
framework for solving forward and inverse problems involving nonlinear partial differential equa-
tions. J. Comput. Phys., 378:686—-707, 2019. ISSN 0021-9991. doi: 10.1016/j.jep.2018.10.045.

URL\https: //doi.org/10.1016/j. jcp.2018.10.045

Tim De Ryck and Siddhartha Mishra. Generic bounds on the approximation error for physics-
informed (and) operator learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL
https: //openreview.net/forum?id=bF4eYy3LTR9

Jacob H Seidman, Georgios Kissas, Paris Perdikaris, and George J Pappas. Nomad: Nonlinear
manifold decoders for operator learning. arXiv preprint arXiv:2206.03551, 2022.

Justin Sirignano and Konstantinos Spiliopoulos. DGM: a deep learning algorithm for solving partial
differential equations. J. Comput. Phys., 375:1339-1364, 2018. ISSN 0021-9991. doi: 10.1016/

j.icp.2018.08.029. URL|https://doi.org/10.1016/j. jcp.2018.08.029

Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-
plicit neural representations with periodic activation functions. Advances in Neural Information
Processing Systems, 33:7462-7473, 2020.

Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani, Dirk
Pfliiger, and Mathias Niepert. PDEBench: An extensive benchmark for scientific machine learn-
ing. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Bench-

marks Track, 2022. URL|https: //openreview.net/forum?id=dh_Mkx0Qfrk

Simone Venturi and Tiernan Casey. Svd perspectives for augmenting deeponet flexibility and inter-
pretability. Computer Methods in Applied Mechanics and Engineering, 403:115718, 2023.

Johannes von Oswald, Christian Henning, Benjamin F. Grewe, and Joao Sacramento. Continual
learning with hypernetworks. In International Conference on Learning Representations, 2020.

URLihttps://openreview.net/forum?id=SJgwNerKvB

Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric par-
tial differential equations with physics-informed deeponets. Science advances, 7(40):eabi8605,
2021.

Sifan Wang, Hanwen Wang, and Paris Perdikaris. Improved architectures and training algorithms
for deep operator networks. Journal of Scientific Computing, 92(2):1-42, 2022.

12
Published as a conference paper at ICLR 2023

Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-
constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification
without labeled data. J. Comput. Phys., 394:56-81, 2019. ISSN 0021-9991. doi: 10.1016/j.jcp.
2019.05.024. URL https://d

13
Published as a conference paper at ICLR 2023

A NOTATIONS

We list the main notations in Table[3]which is not concretely described in this paper.

Uu Domain of input function

y Domain of output function

dy, Dimension of U

dy Dimension of Y

dy Dimension of the codomain of input function

ds Dimension of the codomain of output function

{x1,°++ ,@m} Sensor points

m Number of sensor points

R¢ Euclidean space of dimension d

c’(R) Set of functions that has continuous r-th deriva-
tive.

C(y;R*) Set of continuous function from to R¢

C*({=-1, 1J";R) Set of functions from [—1,1]” to R whose r-th
partial derivatives are continuous.

n= O(e) There exists a constant C such thatn < C-
€,Ve>0

n= Qe) There exists a constant C such thatn > C -
€,Ve>0

n= o(e) n/« converges to 0 as € approaches to 0.

Table 3: Notation

B_ ORIGINAL STRUCTURE OF DEEPONET

u(x) - - by
u u (%2) Branch net a
U (%m) Dp

(Xo GWO)

a t
f t. Inner
y | Trunk net 2 Product
fp

Figure 8: The structure of DeepONet

For a clear understanding of previous works, we briefly leave a description of DeepONet. In partic-
ular, we explain the structure of the unstacked DeepONet in (2019) which is being widely
used in various experiments of the papers. Note that Figure/4fa) represents the corresponding model
which is called simply DeepONet throughout this paper. The overall architecture of the model is
formulated as

Re, 0 Ag(uler).-*+ ,U(am))(y) = (Bala), ++ , (2m): 49).7(Y:4r)).
where 7 and £ are referred to as the trunk net and the branch net respectively. Note that R,
and Ag denote the reconstructor and the operator in Section For m fixed observation points
(21,+++ ,@m) € &™, the unstacked DeepONet consists of an inner product of branch Net and trunk
Net, which are fully connected neural networks. For a function wu, the branch net receives pointwise
projection values (u(x1),--+ ,u(2m)) as inputs to detect which function needs to be transformed.
The trunk net queries a location y € Y of interest where Y denotes the domain of output functions.

14
Published as a conference paper at ICLR 2023

It was revealed that the stacked DeepONet, the simplified version of the unstacked DeepONet, is
a universal approximator in the set of continuous functions. Therefore, the general structure also
becomes a universal approximator which enables close approximation by using a sufficient number
of parameters. Motivated by the property, we focus on how large complexity should be required for
DeepONet and its variants to achieve the desired error.

C ON COMPLEXITY OF DEEPONET AND ITS VARIANTS

C.1 PROOF OF THEOREM[2]ON DEEPONET COMPLEXITY

The following lemma implies that the class of neural networks is sufficiently efficient to approximate
the inner product.

Lemma 1. For the number of basis p € N, consider the inner product function m, : [—1,1]?? + R
defined by
P
Tp(Q1,°°* ,Qp,b1,°++ , bp) = So aidi = ((a1,+++ ,@p), (b1,+++ , bp).
i=1

For an arbitrary positive t, there exists a class of neural network F with universal activation o :
R = R such that the number of parameters of F is O(p'+!/te-"/*) with inf rex || f — Tplloo < &

Proof. Suppose that t is a positive integer and the Sobolev space W2;,2 is well defined. First, we
would like to approximate the product function 7 : [—1, 1]? — R which is defined as

™1(a,b) = ab.

Note that partial derivatives Dkr, = 0 for all multi-index k € {NU {0}}” such that |k| > 2. For
a multi-index k with |k| = 1, D7, contains only one term which is either a or b. In this case, we
can simply observe that }?yjj—1 ||D¥m1|l00 < 2-1 = 2 by the construction of the domain [—1, 1]?
for 7. And finally,

[IT 1 loo < |lablloo < lal] oo||Blloo $ 1-1 =1,

so that a function 7; /3 should be contained in W,.2 for any r € N. In particular, 7 /3 lies in W242
so that there exists a neural network approximation f,,,, in some class of neural network F* with
an universal activation function o such that the number of parameters of F* is O((€/3p)~?/?") =
O(p'/te-1/*), and

\I™1/3 — fanlloo < €/3p,

by Definition|1| Then the neural network 3f,,,, approximates the function 7 by an error €/p which
can be constructed by adjusting the last weight values directly involved in the output layer of neural
network frp.

Finally, we construct a neural network approximation for the inner product function 7. Decompose
ne 2p— dimensional inner product function 7, into p product functions {Proj;(m,) }?_, which are
lefined as

Proj; (7p) : R? >R, Proj; (p)(a1,°** 5 @p,b1,+++ , bp) = 71 (ai, bi) = aidj,

for Vi € {1,--- ,p}. Then each function Proj;(z,) could be approximated within an error ¢/p
by neural network NN; which has O(p'/‘e~!/*) parameters by the above discussion. Finally, by
adding the last weight [1,1,--- ,1] € R!*? which has input as the outputs of p neural networks
{N.N;};_1, we can construct the neural network approximation NN of t, = )>?_, Proj;(mp) such
that the number of parameters is O(1+-p+p-p'/te-1/*) = O(p'+!/te-1/*), Class of neural network
F, which represents the structure of N N, satisfies the desired property.

Obviously, the statement holds for an arbitrary real t which is not an integer.

15
Published as a conference paper at ICLR 2023

Now we assume that O (defined in Eq. (8)) lies in the Sobolev space Wy,dy+m- Then, we can
obtain the following lemma which presents the lower bound on the number of basis p in DeepONet
structure. Note that we apply L.,,-norm for the outputs of branch net and trunk net which are multi-
dimensional vectors.

Lemma 2. Let o : R > R be a universal activation function in C"(R) such that o' is a bounded
variation. Suppose that the class of branch net B has a bounded Sobolev norm (i.e., S<h,VbeE
B). Assume that supremum norm |\-||.0 of the class of trunk net T is bounded by lz and the number of
parameters in T is o(e~(4ut™)/"). Ifany non-constant  € Wy.,dy+m does not belong to any class of
neural network, then the number of basis p in T is Q(e~4#/") when d(Feepone(B, T); Wrdy+m) S
€

Proof. To prove the above lemma by contradiction, we assume the opposite of the conclusion. Sup-
pose that there is no constant C' that satisfies the inequality p > C(e~%/") for Ve > 0. In other
words, there exists a sequence of DeepONet which has {Pn} as the number of basis with a
sequence of error fen}rr i in R such that €,, + 0, and satisfies

1
—d,/r
€, ul

Pn S— (ie. Pn = 0(e,4"/") with respect to n) ; (9)

and
d(Fpecponet(Bn; Tn); Wrydy+m) < €ns
where B,, and 7,, denote the corresponding class sequence of branch net and trunk net respec-

tively. Then, we can choose the sequence of branch net {, : R'™ — R?»}>°, and trunk net
{tm : RY > RP} satisfying

OCE(u), 9) — mp, (Bn (E(u), T(Y)) loo S Ben, VE(u), y] € [-1, Ye", O € Wray +m
for the above sequence of DeepONet by the definition of d(Fpeeponet (Bn, Tn); Wrdy+m)-
Now, we would like to construct neural network approximations f,, for the branch net 6,,. By the

assumption on the boundedness of B, the i-th component [,,]; of 3,, has a Sobolev norm bounded
by 1;. In other words, ||[8,.]i/J1||% < 1 and therefore, [8,,];/l, is contained in W1,,,. Since o is a

universal activation function, we can choose a neural network approximation [f;,]; of [Bn]; such that
the number of parameters is O((€n/l1)-”"/") = O(en™”/"), and

W[Fnli — [Bnlilloo < én/hi.
Then, fr = (li[fnlisti[fnl2,-++ ti [fn]p, ) becomes neural network approximation of 8, which
has O(pnen””") parameters within an error €,,.

Recall the target function corresponding to m observation €(u) € [—1, 1]™ by O(E(u),-) : R4v >
R which is defined in Eq. (8). Then, for VE(w), we can observe the following inequalities:

]O(E(u), y) — Mp, (Fr(E(u)), Tr(¥)) loo
S$ ]O(E(u), y) — mM, (Bn (E(u) . Tr(y)) lloo + Ip (Bn (E(u) = fir (E(u)  Tr(Y)) loo
< 2€n + EnllTn(Y) lloo < €n(2 +12),
by the assumption on the boundedness of 7. Now we would like to consider the sequence of neural

network which is an approximation of inner product between p,,—dimensional vector in [—1, 1]?”.
Note the following inequality

Fn (E(u) )lloo SI fn(E(u)) = Bn(E(u))loo + [Bn (EU) hoo
Sn + |[Bn(E(u))||>
<e+h < 2h,
with ||Tplloo < lz for large n. It implies that f,,(E(u))/2l, and 7,(x)/l2 lie in [—1,1]?». By
Lemma [i] there exists a class of neural network ,, such that the number of parameters is
141/Zdyr —1/2dyr
O(pn en /Y") and,

jinf [Ih = tp, loc Se

16
Published as a conference paper at ICLR 2023

where 7,,, is the inner product corresponding to p,—dimensional vector. Choose a neural network
hn : [-1,1]??» > R such that ||hn — 7p, |loo < 2€n. Then, by the triangular inequality,

|]O(E(u), y) — WAlehn(fn(E(u))/2h, mt (y)/l2)|loo
S |]O(E(u), y) — Tp, (fn(E(u)), A (Y))Iloo
+ lyle||7,, fr(E (Oya alent) ~ 1 (fn(E(u))/211, Tr(y)/l2) lo
<e,(24 + 2Iyl2(2€n) = €n(2+ lo +4hil2). (10)
Finally, we Eat the number of parameters which is required to implement the function
QA lohn(fn(E(u))/2l1, T(y)/l2). The only part that needs further consideration is scalar multi-

plication. Since we need one weight to multiply a constant with one real value, three scalar multi-
plications

hn (fn(E(u))/211, Tr(y)/l2) 4 2h lohn (fn (E(u))/211, Tr(y)/l2),
fr(E(u)) > fr(E(u))/2h, and T(x) Tr(y)/le,
require 1, p,,, p,-parameters respectively. Combining all the previous results with the size of trunk
net, the total number of parameters is obtained in the form of
O(L + 2pq + phtl/24ureW2dur 4 ye m/t) 4 (ex (yt M)/7) = oer dvtm)/ry,
since the initial assumption (d) on the number of basis gives the following inequality.

141/2dy,r/—1/2dyr —m/r 1/2dyr_—1/2dyr —m/r
Pda Adu be” < py(pal ete, Ader 4 el”)

Pn
< ej, tu/" (eq V2? “1/2dyr e"/")

1 2
< ner ta/rg9g—m/t = 2 = (dy+m)/r_
= nr En nn
On the one hand, the sequence of function {2,l2hn(fn(E(u)) /2l1, Tn(y)/l2) $7, is an sequence of
approximation for O(E(u), y) within a corresponding sequence of error {€,,(2 + lz + dlil2)}°

Denote the sequence of the class of neural networks corresponding to the sequence of the function
{2lyloln (fr (E(u))/2l1, Tr(y)/l2)}_, by {F,.}7,. By the assumption, Theorem|1]|implies the
number of parameters in {Fy}, is Q((€n(2 + lo + Alla) t™/7) = Oe, ™"), There-
fore, the initial assumption (9 would result in a contradiction so the desired property is valid.

Note that the assumption on the boundedness of the trunk net could be valid if we use the bounded
universal activation function o. Using the above results, we can prove our main theorem, Theorem

2

Proof of Theorem{2| Denote the number of parameters in T by N77. Suppose that there is no con-
stant C satisfies the inequality Nz > C'e~4v/", Ve > 0. That is, there exists a sequence of DeepONet
with the corresponding sequence of trunk net class {7;,}°°_, and sequence of error {e,,}°°_, such
that €,, + 0, and it satisfies

1 7
Ny, < —6,4u/" (ie. Nr, = o(e;,4¥/") with respect to n).
n

Note that the above implies N7,, = o(€n. (dytm)/ry On the one hand, the following inequality holds

where B,, denotes the corresponding class sequence of branch net.

dF peeponet(Bns Tn); Wrdytm) S €n-
Since o is bounded, 7;, consists of bounded functions with respect to the supremum norm. There-

fore, if we apply the Lemmaf2]with respect to n, the number of basis p,, should be Qen*/"). Since
Pn is also the number of output dimensions for the class of trunk net 7;,, the number of parameters

in J, should be larger than pp, = Q(en niult ). This leads to a contradiction.

Finally, we present a lower bound on the total number of parameters of DeepONet, considering the
size of the branch net. Keep in mind that the proof of this theorem can be applied to other variants
of DeepONet.

17
Published as a conference paper at ICLR 2023

Theorem 3. (Total Complexity of DeepONet) Let o : R — R be a universal activation function
in C"(R) such that o and o' are bounded. Suppose that the class of branch net B has a bounded
Sobolev norm (i.e., ||8\|% < 1,,V8 € B). If any non-constant yy € W,.,, does not belong to any class

of neural network, then the number of parameters in DeepONet is Q(e~(41+™/®) for any R > r
when d(Fpeeponet(B, T); Wray +m) < &

Proof. For a positive € < 1,, suppose that there exists a class of branch net B and trunk net T such
that d(Fpeeponet(B, T); Wr,dy+m) < e. By the boundedness of o, there exists a constant 2 which is
the upper bound on the supremum norm || - ||, of the trunk net class T. Let us denote the number
of parameters in Fpeeponet by NFp.coxa- Using the LemmalT}to replace DeepONet’s inner products
with neural networks as in the inequality {10}, we can construct a class of neural network F such
that the number of parameters F is O(NFp.joxa + PUT Y/te71/*) and,

dF; Wy,a,+m) < (1+ hils)e.

Suppose that Ny, oo = o(e7v+™/"). Then, by Theorem |I| p'*!/e1/ should be
Qe“ (+™/"). Since t can be arbitrary large, the number of basis p should be 2(e~(4v+™)/®)
for any R > r.

C.2 COMPLEXITY ANALYSIS ON VARIANTS OF DEEPONET.

We would like to verify that variant models of DeepONet require numerous units in the first
hidden layer of the target network. Now we denote the class of Pre-net in Shift-DeepONet
and flexDeepONet by P. The class of Shift-DeepONet and flexDeepONet will be written as
F spitt-DeepoNet(P, B, T) and Frexpeeponet(P, B, T) respectively. The structure of Shift-DeepONet
can be summarized as follows. Denote the width of the first hidden layer of the target network by
w. We define the pre-net as p = [p1,p2] : R™ > RY*(4v+) where py : R™ > RY*¢y and
p2 : R™ — RY", the branch net as 8 : R™ — R?, and the trunk net as 7 : RY — R?. The
Shift-DeepONet Fsnitt-DeepoNet (P; B,7) is defined as

Sshitt-Deeponet(P, 8, 7)(E(u), ¥) = Tp(B(E(u)), T(®(p1 (E(u), y) + p2(E(u))))
where is defined in Eq. (Ii).

We claim that it does not improve performance for the branch net to additionally output the weights
on the first layer of a target network. The following lemma shows that the procedure can be replaced
by a small neural network structure.

Lemma 3. Consider a function ® : R¢v(“+) — R” which is defined below.

B(x1, +++, Ldyy0 +, L(dy—tyw41* »Ldyws Ys? + Yay)

dy dy
= Sori Yo 8a, —1)w+ id » (1)
i=l i=1

For any arbitrary positive t, there exists a class of neural network F with universal activation o :

R > R such that the number of parameters of F is O(wdy /te“4/*) with inf ex ||f — ®lloo Sé

w

Proof. Using the Lemma|[I] we can construct a sequence of neural network {f;};"., which is an

€-approximation of the inner product with o(ayt!/ tev ‘) parameters. If we combine all of the w
approximations, we get the desired neural network.

Now we present the lower bound on the number of parameters for Shift-DeepONet. We derive the
following theorem with an additional assumption that the class of trunk net is Lipschitz continuous.
The function 7 : R?¥ — R? is called Lipschitz continuous if there exists a constant C' such that

71) — T(y2)lla S Cllya — yell

For the neural network f, the upper bound of the Lipschitz constant for f could be obtained as
Lk~T1*_,||W*||;, where L is the Lipschitz constant of o and the norm || - ||; denotes the matrix

18
Published as a conference paper at ICLR 2023

norm induced by vector 1-norms. We can impose constraints on the upper bound of the weights,
which consequently enforce affine transformation W* to be bounded with respect to the L! norm.
Therefore, we can guarantee the Lipschitz continuity of the entire neural network in this way.

We would like to remark on the validity of the weight assumptions in the theorem since the bounded
assumptions of the weight may be a possible reason for increasing the number of parameters. How-
ever, the definition of Sobolev space forces all elements to have the supremum norm || - ||, less
than 1. It may be somewhat inefficient to insist on large weights for approximating functions with a
limited range.

Theorem 4. Let co : R — R be a universal activation function in C"(R) such that o and o! are
bounded. Suppose that the class of branch net B and pre-net P has a bounded Sobolev norm
(i.e, ||6\|f < 4,V6 € B, and ||p||f < l3,Vp € P) and any neural network in the class of
trunk net T is Lipschitz continuous with constant lz. If any non-constant yy € Wy,» does not
belong to any class of neural network, then the number of parameters in T is Q(e~¢/") when

dF snif-Deeponer(P, B, T); Wed, +m) < €

Proof. Denote the number of parameters in T by N77. Suppose that there exists a sequence of pre-
net {p,}°°_,, branch net {(3,, }>°_, and trunk net {7,,}°°_, with the corresponding sequence of error
{en }>°_, such that €,, — 0 and,

Ny =o(e,/"), and sup || fonitt-Deeponet(Pns Bn, Tn) — Vloo < €n-
WEWr,dy+m

The proof can be divided into three parts. Firstly, we come up with a neural network approximation
PNN of pn of which size is O(w - en”!") within an error €,. Next, construct a neural network

approximation of ® using the Lemma|3] Finally, the inner product 7p, (8n, Tm) is replaced with a
neural network as in of Lemma[2}

Since all techniques such as triangular inequality are consistent with the previous discussion, we wil
briefly explain why additional Lipschitz continuity is required for the trunk network, and omit the
details. Approximating the Pre-Net of Shift DeepOnet, which is not in DeepOnet, inevitably results
in an error in the input of the trunk net. We are reluctant to allow this error to change the output of
the trunk net significantly. In this situation, the Lipschitz continuity provides the desired result.

For dy = 1, the additional rotation is only multiplying by 1 or —1. Since the weight and bias of
the first layer alone can cover the scalar multiplication, flexDeepONet has the same properties as
Shift-DeepONet in the above theorem.

Theorem 5. Consider the case dy = 1. Let a : R + R be a universal activation function in C’ (R)
such that o and o' are bounded. Suppose that the class of branch net B and pre-net P has a bounded
Sobolev normi(i.e., ||B\|— < 4,V8 € B, and |\p\|% < 13,Vp € P), and any neural network in the
class of trunk net T is Lipschitz continuous with constant la. If any non-constant q) € Wy.» does
not belong to any class of neural network, then the number of parameters in T is Q(e~ 4/7) when

d(Fpexdeeponet(B, T); Wr,dy+m) < €

Proof. The main difference between flexDeepONet and Shift-DeepONet, which is not mentioned
earlier, is that the branch net affects the bias of the output layer. However, adding the values of the
two neurons can be implemented in a neural network by adding only one weight of value 1 for each
neuron, so all of the previous discussion are valid.

In fact, NOMAD can be substituted with the embedding method handled by|Galanti & Wolf] (2020).

Suppose that the branch net of NOMAD is continuously differentiable. Let’s also assume that the
Lipschitz constant of branch net and trunk net is bounded. We would like to briefly cite the relevant
theorem here.

Theorem 6. (Galanti & Wolf| '2020)) Suppose that o is a universal activation in C1(R) such that o!
is a bounded variation on R. Additionally, suppose that there is no class of neural network that can
represent any function in W,,d, +m other than a constant function. If the weight on the first layer of
target network in NOMAD is bounded with respect to L+-norm, then d(N; W1,dy+m) < € implies
the number of parameters in N is Q(e~™"(4ut™2-™y)) where N denotes the class of function
contained as a target network of NOMAD.

19
Published as a conference paper at ICLR 2023

D CHUNKED EMBEDDING METHOD

The HyperDeepONet may suffer from the large complexity of the hypernetwork when the size of
the target network increases. Although even a small target network can learn various operators with
proper performance, a larger target network will be required for more accurate training. To take into
account this case, we employ a chunk embedding method which is developed by

. The original hypernetwork was designed to generate all of the target network’s weights so
that the complexity of hypernetwork could be larger than the complexity of the target network. Such
a problem can be overcome by using a hypernetwork with smaller outputs.

Target Network => 6wO)
I I I

Parameter Parametery Paramete!
Group | Group 2 — Group 3

Parameter
Group Nc

I
1
t, t,t,

Ay AR OAH AN

{uC} Zr {UC} 22 fuai)} 23 {u(xi)} Zne

Figure 9: Chunk Embedding Method

More specifically, Figure [9] describes how the chunk embedding method reduces the number of
learnable parameters. First, they partition the weights and biases of the target network. The hyper-
network then creates the weights of each parameter group by using the sensor values {u(x;)}/"
with a latent vector z;. All groups share the hypernetwork so that the complexity decreases by a
factor of the number of groups. Since the latent vectors {zj}Ne learn the characteristics of each
group during the training period, the chunked embedding method preserves the expressivity of the
hypernetwork. The chunked architecture is a universal approximator for the set of continuous func-
tions with the existence of proper partitions (Proposition 1 in[von Oswald et al.|(2020)). We remark
that the method can also generate the additional weights and discard the unnecessary ones when the
number of the target network’s parameters is not multiple of Nc@, which is the number of group.

E EXPERIMENTAL DETAILS

DeepONet Shift Flex NOMAD Hyper(ours)
Identity 0.0005 0.0002 0.0005 0.0001 0.0001
Differentiation 0.0005 0.0002 0.0005 0.0001 0.0001
Advection 0.0005 0.0001 0.0001 0.0002 0.0005
Burgers 0.0001 0.0005 0.0002 0.0001 0.0001
Shallow 0.0001 0.0005 - 0.0001 0.0005

Table 4: Setting of the decay rate for each operator problem

In most experiments, we follow the hyperparameter setting in|Lu et al. (2019} 2021} 2022). We use
ADAM in|Kingma & Ba|(2015) as an optimizer with a learning rate of le — 3 and zero weight decay.
In all experiments, an InverseTimeDecay scheduler was used, and the step size was fixed to 1. In
the experiments of identity and differentiation operators, grid search was performed using the sets
0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005 for decay rates. The selected values of the decay rate
for each model can be found in Table[4]

20
Published as a conference paper at ICLR 2023

(a) Input u(x) and output 6(u)(x) (b) Prediction using DeepOnet & variants. (c) Prediction using HyperDeepONet

Inn

\ Target function
0.57 — input -05 “——= DeepONet

+ output — Shift-DeepONet
flexDeepONet
— Nomad

— HyperDeeponet!

10 -10
“100 -0.75 -050 -025 0.00 025 050 075 100 100 -0.75 -050 -0.25 0.00 025 0.50
x

[o7s -0'50 -0.25 0.00 0.25 0.50 075 1.00

Figure 10: One test data example of differentiation operator problem.

E.1 IDENTITY

As in the text, we developed an experiment to learn the identity operator for the 20th-order Cheby-
shev polynomials (Figure [T0p. Note that the absolute value of the coefficients of all orders is less
than or equal to 1/4. We discretize the domain [—1, 1] with a spatial resolution of 50. For the experi-
ments described in the text, we construct all of the neural networks with tanh. We use 1,000 training
pairs and 200 pairs to validate our experiments. The batch size during the training is determined to
be 5,000, which is one-tenth the size of the entire dataset.

E.2 DIFFERENTIATION

In this experiment, we set functions whose derivatives are 20th-order Chebyshev polynomials as
input functions. As mentioned above, all coefficients of the Chebyshev polynomial are between
—1/4 and 1/4. We use the 100 uniform grid on the domain [-1, 1]. The number of training and
test samples is 1,000 and 200, respectively. We use a batch size of 10,000, which is one-tenth the
size(100, 000 = 100 - 1, 000) of the entire dataset.

E.3. ADVECTION EQUATION

We consider the linear advection equation on the torus T := R/Z as follows:

Ow Ow _
{2 oo

w(x,0) = wo(x), «x ET, (12)

where c is a constant which denotes the propagation speed of w. By constructing of the domain as
T, we implicitly assume the periodic boundary condition. In this paper, we consider the case when
c = 1. Our goal is to learn the operator which maps wo(x)(= w(0,x)) to w(0.5, 2). We use the
same data as in[Lu etal] (2022). We discretize the domain (0, 1] with a spatial resolution of 40. The
number of training samples and test samples is 1,000 and 200, respectively. We use the full batch
for training so that the batch size is 40 - 1,000 = 40, 000.

E.4 BURGERS’ EQUATION

We consider the 1D Burgers’ equation which describes the movement of the viscous fluid

ane hiked (0,1) € (0,1) x (0,1), «a

w(x,0) = wo(2), x € (0,1),

where wo is the initial state and v is the viscosity. Our goal is to learn the nonlinear solution
operator of the Burgers’ equation, which is a mapping from the initial state wo(x)(= w(z,0)) to
the solution w(x, 1) at t = 1. We use the same data of Burgers’ equation provided ist ey
The initial state wo(z) is generated from the Gaussian random field V(0,54(—A + 257)~ 7) with
the periodic boundary conditions. The split step and the fine forward Euler methods were employed

to generate a solution at t = 1. We set viscosity v and a spatial resolution to 0.1 and 27 = 128,
respectively. The size of the training sample and test sample we used are 1,000 and 200, respectively.

We take the ReLU activation function and the InverseTimeDecay scheduler to experiment with the
same setting as in{Lu et al.| (2022). For a fair comparison, all experiments on DeepONet retained

21
Published as a conference paper at ICLR 2023

the hyperparameter values used in{Lu et al.| (2022). We use the full batch so that the batch size is
1,280,000.

E.5 SHALLOW WATER EQUATION

The shallow water equations are hyperbolic PDEs which describe the free-surface fluid flow prob-
lems. They are derived from the compressible Navier-Stokes equations. The physical conservation
laws of the mass and the momentum holds in the shock of the solution. The specific form of the
equation can be written as

oh + Z (hu) + 2(hv) =0,
thu) 4 a (u?h + $gh?) 4 5, (hue) 0,

ot! Ox
O(he) 4 Hi (v?h + 3gh?) + 2 (huv) = 0,
h(0, 21, 2) = ho(21, 22),

(14)

for ¢ € [0,1] and 21, x2 € [—2.5, 2.5] where h(t, x1, x2) denotes the height of water with horizontal
and vertical velocity (u,v). g denotes the gravitational acceleration. In this paper, we aim to learn the
operator ho(a1,%2) +> {h(t, 21,22) },<[1/4,1) Without the information of (u,v). For the sampling
of initial conditions and the corresponding solutions, we directly followed the setting of
(2022). The 2D radial dam break scenario is considered so that the initialization of the water
height is generated as a circular bump in the center of the domain. The initial condition is generated

by
2.0, forr < \/a? + 23
h(t = 0,21, 22) = 15
(£=0,21,22) {re forr > \/x? + 23 (5)

with the radius r randomly sampled from U[0.3, 0.7]. The spatial domain is determined to be a 2-
dimensional rectangle [—2.5, 2.5] x [—2.5, 2.5]. We use 256 = 16? grids for the spatial domain. We
train the models with three snapshots at t = 0.25, 0.5, 0.75, and predict the solution h(t, x1, 72) for
four snapshots t = 0.25, 0.5, 0.75, 1 on the same grid. We use 100 training samples and the batch
size is determined to be 25,600.

F ADDITIONAL EXPERIMENTS

F.1 COMPARISON UNDER VARIOUS CONDITIONS

The experimental results under various conditions are included in Table[5|by modifying the network
structure, activation function, and number of sensor points. Although the DeepONet shows good
performance in certain settings, the proposed HyperDeepONet shows good performance without
dependency on the various conditions.

Activation: ReLU, M = 30 DeepONet Shift Flex NOMAD _ Hyper(Ours)
dy-30-30-30- 0.16797 30852 = 1.04292 0.27209 0.02059
dy-50-50- 0.04822 08760 1.11957 0.21391 0.05562
Activation: ReLU, M = 100 DeepONet Shift Flex NOMAD _ Hyper(Ours)
dy-30-30-30- 0.02234 08310 1.03741 0.19089 0.01743
dy-50-50- 0.07255 47373 1.13217 (0.14020 0.04645
Activation: PReLU, M = 30 DeepONet Shift Flex NOMAD _ Hyper(Ours)
dy-30-30-30- 0.11354 09395 1.03502 0.25651 0.02844
dy-50-50- 0.00873 -14073 1.06947 0.04054 0.04302
Activation: PReLU, M = 100 DeepONet Shift Flex NOMAD _ Hyper(Ours)
dy-30-30-30- 0.01035 05080 1.07791 0.16592 0.01083
dy-50-50- 0.07255 47373 1.13217 (0.14020 0.04645

Target networ!

Target networ!

Target networ!

Target networ!

Table 5: The relative L? test errors for experiments on training the identity operator under various
conditions

22
Published as a conference paper at ICLR 2023

0.54 a 0.5 4
0.4 4 0.4 4
L SS L
£ — train_Deeponet £ — train_Deeponet
o 0.34 bt test_DeepoNnet o 0.34 bt test_DeepoNnet
a —{— train_HyperDeepONet a —{— train_HyperDeepONet
Bo24 Hb test_HyperDeepoNet Zo2 Hb test_HyperDeepoNet
0.14
0.0 +, 1 1 T 7 1 1 T 7
1 2 3 4 5 1 2 3 4 5
# hidden layers in branch net (hypernetwork) # hidden layers in branch net (hypernetwork)
Figure 11: Varying the number of layers of branch net and hypernetwork in DeepONet and Hyper-
DeepONet for identity operator problem (left) and differentiation operator problem (right).

F.2. VARYING THE NUMBER OF LAYERS IN BRANCH NET AND HYPERNETWORK

Figure [I 1] compares the relative L? error of the training data and test data for the DeepONet and
the HyperDeepONet by varying the number of layers in the branch net and the hypernetwork while
maintaining the same small target network. Note that the bottom 150 training and test data with
lower errors are selected to observe trends cleary. The training and test error for the DeepONet is
not reduced despite the depth of the branch net becoming larger. This is a limitation of DeepONet’s
linear approximation. DeepONet approximates the operator with the dot product of the trunk net’s
output that approximates the basis of the target function and the branch net’s output that approxi-
mates the target function’s coefficient. Even if a more accurate coefficient is predicted by increasing
the depth of the branch net, the error does not decrease because there is a limit to approximating the
operator with a linear approximation using the already fixed trunk net.

The HyperDeepONet approximates the operator with a low test error in all cases with a different
number of layers. Figure hows that the training error of the HyperDeepONet remains small as
the depth of the hypernetwork increases, while the test error increases. The increasing gap between
the training and test errors is because of overfitting. HyperDeepONet overfits the training data
because the learnable parameters of the model are more than necessary to approximate the target
operator.

F.3. COMPARISON OF HYPERDEEPONET WITH FOURIER NEURAL OPERATOR

The Fourier Neural Operator (FNO) ( is a well-known method for operator learning.
022) consider 16 different tasks plain the relative performance of the DeepONet and
the FNO. They show that each method has its advantages and limitations. In particular, DeepONet
has a great advantage over FNO when the input function domain is complicated, or the position
of the sensor points is not uniform. Moreover, the DeepONet and the HyperDeepONet enable the
inference of the solution of time-dependent PDE even in a finer time grid than a time grid used for
training, e.g.the continuous-in-time solution operator of the shallow water equation in our experi-
ment. Since the FNO is image-to-image based operator learning model, it cannot obtain a continuous
solution operator over time ¢ and position x), x2. In this paper, while retaining these advantages of
DeepONets, we focused on overcoming the difficulties of DeepONets learning complex target func-
tions because of linear approximation. Therefore, we mainly compared the vanilla DeepONet and
its variants models to learn the complex target function without the result of the FNO.

Table|6|shows the simple comparison of the HyperDeepONet with the FNO for the identity operator
and differentiation operator problems. Although the FNO structure has four Fourier layers, we
use only one Fourier layer with 2,4,8, and 16 modes for fair comparison using similar number
of parameters. The FNO shows a better performance than the HyperDeepONet for the identity
operator problem. Because the FNO has a linear transform structure with a Fourier layer, the identity

23
Published as a conference paper at ICLR 2023

Fourier Neural Operator

Model HyperDeepONet (ours) Mode 2 Mode4 Mode8 Mode 16

Identity 0.0358 0.0005 0.0004 ~—-:0.0003 0.0004
Differentiation 0.1268 0.8256 0.6084 0.3437 0.0118

Param 15741 (or 16741) 20993 29185 = 45569 78337

Table 6: The relative L? test errors and the number of parameters for the identity and differentia-
tion operator problems using HyperDeepONet and FNO with different number of modes. #Param
denote the number of learnable parameters.

operator is easily approximated even with the 2 modes. In contrast, the differentiation operator is
hard to approximate using the FNO with 2, 4, and 8 modes. Although the FNO with mode 16
can approximate the differentiation operator with better performance than the HyperDeepONet, it
requires approximately 4.7 times as many parameters as the HyperDeepONet.

Model DeepONet — Shift Flex © NOMAD — Hyper(Ours)
Advection #Param 274K 281K = - 282K 270K 268K
Rel error 0.0046 0.0095 0.0391 0.0083 0.0048
Burgers #Param 115K 122K 122K TI7K 114K
: Rel error 0.0391 0.1570 0.1277 0.0160 0.0196
Shallow #Param 107K THK - TI7K 101K
Rel error 0.0279 0.0299 - 0.0167 0.0148
Shallow 7##Param 6.5K 8.5K - 6.4K 5.6K
w/ small param _ Rel error 0.0391 0.0380 - 0.0216 0.0209

Table 7: The relative L? test errors and the number of parameters for the solution operators of PDEs
experiments. ##Param denote the number of learnable parameters. Note that the all five models use
the similar number of parameters for each problem.

F.4 PERFORMANCE OF OTHER BASELINES WITH THE SAME NUMBER OF LEARNABLE
PARAMETERS

For three different PDEs with complicated target functions, we compare all the baseline methods in
Table[7|to evaluate the performances. We analyze the model’s computation efficiency based on the
number of parameters and fix the model’s complexity for each equation. All five models demon-
strated their prediction abilities for the advection equation. DeepONet shows the greatest perfor-
mance in this case, and other variants can no longer improve the performance. For the Burgers’
equation, NOMAD and HyperDeepONet are the two outstanding algorithms from the perspective
of relative test error. NOMAD seems slightly dominant to our architectures, but the two models
compete within the margin of error. Furthermore, HyperDeepONet improves its accuracy using the
chunk embedding method, which enlarge the target network’s size while maintaining the complex-
ity. Finally, HyperDeepONet and NOMAD outperform the other models for 2-dimensional shallow
water equations. The HyperDeepONet still succeeds in accurate prediction even with a few pa-
rameters. It can be observed from Table [7] that NOMAD is slightly more sensitive to an extreme
case using a low-complexity model. Because of the limitation in computing 3-dimensional rotation,
FlexDeepONet cannot be applied to this problem.

Figure [13] shows the results on prediction of shallow water equations’ solution operator using the
DeepONet and the HyperDeepONet. The overall performance of the DeepONet is inferior to that of
the HyperDeepONet, which is consistent with the result in Figure [12] In particular, the DeepONet
has difficulty matching the overall circular shape of the solution when the number of parameters is
small. This demonstrates the advantages of the HyperDeepONet when the computational resource
is limited.

24
Published as a conference paper at ICLR 2023

(a) Shallow water equations with large models (b) Shallow water equations with small models
OR te tet tet nt
L L ‘ .
5 6x10" S \ deep
w Wi 6x10-7 shift
g 4x 10-2 g nomad
ic] 3x 10-2 ic] 4x10-? hyper
@ i}
a om 3x10? x.
o 2 fr ©
tA 2x10 A Se
a] a] He 0 #-9-0-0-0.9 9% 0
2 F ox 10-8 2 SS Mo HK K OME MoM
2500 5000 7500 10000 12500 15000 17500 20000 2500 5000 7500 10000 12500 15000 17500 20000
Epoch(s) Epoch(s)

Figure 12: The test L? relative errors of four methods during training for the solution operator of
shallow water equations.

Training time (s)

(per 1 epoch) Inference time (ms)

Same target (Differentiation) HyperBeepoNet roos ro
. DeepONet 0.466 0.921
Same #param (Advection) HyperDeepONet 0.500 1.912

Table 8: The training time and inference time for the differentiation operator problem and the solu-
tion operator of advection equation problem using DeepONet and HyperDeepONet.

F.5 COMPARISON OF TRAINING TIME AND INFERENCE TIME

Table[8|shows the training time and the inference time for the DeepONet and the HyperDeepONet
for two different operator problems. When the same small target network is employed for the Deep-
ONet and the HyperDeepONet, the training time and inference time for the HyperDeepONet are
larger than for the DeepONet. However, in this case, the time is meaningless because DeepONet
does not learn the operator with the desired accuracy at all (Table[I]and Figure|6p.

Even when both models use the same number of training parameters, HyperDeepONet takes slightly
longer to train for one epoch than the DeepONet. However, the training complex operator using the
HyperDeepONet takes fewer epochs to get the desired accuracy than DeepONet, as seen in Figure
This phenomenon can also be observed for the shallow water problem in Figure[12] It shows that
the HyperDeepONet converges to the desired accuracy faster than any other variants of DeepONet.

The HyperDeepONet also requires a larger inference time because it can infer the target network
after the hypernetwork is used to generate the target network’s parameters. However, when the
input function’s sensor values are already fixed, the inference time to predict the output of the target
function for various query points is faster than that of the DeepONet. This is because the size of the
target network for HyperDeepONet is smaller than that of the DeepONet, although the total number
of parameters is the same.

25
Published as a conference paper at ICLR 2023

HyperDeepONet

Exact Solution « ONet DeepONet (small)
HyperDeepONet
Exact Solution ¢ ONet DeepONet (small)
HyperDeepONet
Exact Solution C ONet DeepONet (small)
Exact Solution C ONet DeepONet

HyperDeepONet

(small)

Figure 13: The examples of predictions on the solution operator of shallow water equations using
the DeepONet and the HyperDeepONet. The first column represents the exact solution generated
, and the other four columns denote the predicted solutions using the
corresponding methods. The four rows shows the predictions h(t,21,22) at four snapshots t =

(0.25, 0.5, 0.75, 1].

26

1o

@

DeepONet
(small)

x

DeepONet
(small)

DeepONet
(small)

x

DeepONet
(small)

14

13

12

la

10
