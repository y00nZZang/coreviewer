Published as a conference paper at ICLR 2023

SUBQUADRATIC ALGORITHMS FOR KERNEL MATRI-
CES VIA KERNEL DENSITY ESTIMATION

Ainesh Bakshi
MIT
ainesh@mit.edu

Sandeep Silwal
MIT
silwal@mit.edu

Piotr Indyk
MIT
indyk@mit.edu

Praneeth Kacham
CMU
pkacham@cs.cmu.edu

Samson Zhou
UC Berkeley and Rice University
samsonzhou@gmail.com

ABSTRACT

Kernel matrices, as well as weighted graphs represented by them, are ubiquitous
objects in machine learning, statistics and other related ﬁelds. The main drawback
of using kernel methods (learning and inference using kernel matrices) is efﬁ-
ciency – given n input points, most kernel-based algorithms need to materialize
the full n × n kernel matrix before performing any subsequent computation, thus
incurring Ω(n2) runtime. Breaking this quadratic barrier for various problems has
therefore, been a subject of extensive research efforts.
We break the quadratic barrier and obtain subquadratic time algorithms for several
fundamental linear-algebraic and graph processing primitives, including approx-
imating the top eigenvalue and eigenvector, spectral sparsiﬁcation, solving lin-
ear systems, local clustering, low-rank approximation, arboricity estimation and
counting weighted triangles. We build on the recently developed Kernel Density
Estimation framework, which (after preprocessing in time subquadratic in n) can
return estimates of row/column sums of the kernel matrix. In particular, we de-
velop efﬁcient reductions from weighted vertex and weighted edge sampling on
kernel graphs, simulating random walks on kernel graphs, and importance sam-
pling on matrices to Kernel Density Estimation and show that we can generate
samples from these distributions in sublinear (in the support of the distribution)
time. Our reductions are the central ingredient in each of our applications and
we believe they may be of independent interest. We empirically demonstrate the
efﬁcacy of our algorithms on low-rank approximation (LRA) and spectral sparsi-
ﬁcation, where we observe a 9x decrease in the number of kernel evaluations over
baselines for LRA and a 41x reduction in the graph size for spectral sparsiﬁcation.

1

Introduction

For a kernel function k : Rd × Rd → R and a set X = {x1 . . . xn} ⊂ Rd of n points, the entries
of the n × n kernel matrix K are deﬁned as Ki,j = k(xi, xj). Alternatively, one can view X as
the vertex set of a complete weighted graph where the weights between points are deﬁned by the
kernel matrix K. Popular choices of kernel functions k include the Gaussian kernel, the Laplace
kernel, exponential kernel, etc; see (Sch¨olkopf et al., 2002; Shawe-Taylor et al., 2004; Hofmann
et al., 2008) for a comprehensive overview.

Despite their wide applicability, kernel methods suffer from drawbacks, one of the main being efﬁ-
ciency – given n input points in d dimensions, many kernel-based algorithms need to materialize the
full n × n kernel matrix K before performing the computation. For some problems this is unavoid-
able, especially if high-precision results are required (Backurs et al., 2017). In this work, we show
that we can in fact break this Ω(n2) barrier for several fundamental problems in numerical linear
algebra and graph processing. We obtain algorithms that run in o(n2) time and scale inversely-
proportional to the smallest entry of the kernel matrix. This allows us to skirt several known lower

1

Published as a conference paper at ICLR 2023

bounds, where the hard instances require the smallest kernel entry to be polynomially small in n.
Our parameterization in terms of the smallest entry is motivated by the fact in practice, the smallest
kernel value is often a ﬁxed constant (March et al., 2015; Siminelakis et al., 2019; Backurs et al.,
2019; 2021; Karppa et al., 2022). We build on recently developed fast approximate algorithms for
Kernel Density Estimation (Charikar & Siminelakis, 2017; Backurs et al., 2018; Siminelakis et al.,
2019; Backurs et al., 2019; Charikar et al., 2020). Speciﬁcally, these papers present fast approximate
data structures with the following functionality:
Deﬁnition 1.1 (Kernel Density Estimation (KDE) Queries). For a given dataset X ⊂ Rd of size
n, kernel function k, and precision parameter ε > 0, a KDE data structure supports the following
operation: given a query y ∈ Rd, return a value KDEX (y) that lies in the interval [(1 − ε)z, (1 +
ε) z], where z = (cid:80)

x∈X k(x, y), assuming that k(x, y) ≥ τ for all x ∈ X.

The performance of the state of the art algorithms for KDE also scales proportional to the smallest
kernel value of the dataset (see Table 1). In short, after a preprocessing time that is sub-quadratic (in
n), KDE data structures use time sublinear in n to answer queries deﬁned as above. Note that for all
of our kernels, k(x, y) ≤ 1 for all inputs x, y.

Table 1: Instantiations of KDE queries. The query times depend on the dimension d, accuracy ε,
and lower bound τ . The parameter β is assumed to be a constant.

2

k(x, y)
e−(cid:107)x−y(cid:107)2
e−(cid:107)x−y(cid:107)2
e−(cid:107)x−y(cid:107)1
1

(1+(cid:107)x−y(cid:107)2

2)β

Preprocessing Time Query Time

nd
ε2τ 0.173+o(1)
nd
ε2τ 0.1+o(1)
nd
ε2τ 0.5
nd
ε2

d
ε2τ 0.173+o(1)
d
ε2τ 0.1+o(1)
d
ε2τ 0.5
d
ε2

Reference
(Charikar et al., 2020)
(Charikar et al., 2020)
(Backurs et al., 2019)
(Backurs et al., 2018)

Type
Gaussian
Exponential
Laplacian
Rational Quadratic

1.1 Our Results

We show that given a KDE data structure as described above, it is possible to solve a variety of matrix
and graph problems in subquadratic time o(n2), i.e., sublinear in the matrix size. We emphasize
that in our applications, we only require black-box access to KDE queries. Given this, we design
algorithms for problems such as eigenvalue/eigenvector estimation, low-rank approximation, graph
sparsiﬁcation, local clustering, aboricity estimation, and estimating the total weight of triangles.

Our results are obtained via the following two-pronged approach. First, we use KDE data structures
to design algorithms for the following basic primitives, frequently used in sublinear time algorithms
and property testing:

1. sampling vertices by their (weighted) degree in K (Theorems C.2 and C.4 and Algorithms 2 / 4),

2. sampling random neighbors of a given vertex by edge weights in K and sampling a random

weighted edge (Theorem C.5 and Algorithms 5 and 6),

3. performing random walks in the graph K (Theorem C.7 and Algorithm 7), and

4. sampling the rows of the edge-vertex incident matrix and the kernel matrix K, both with proba-
bility proportional to respective row norms squared (Section D.1, Theorem D.1, and Section D.2,
Corollary D.10 respectively).

In the second step, we use these primitives to implement a host of algorithms for the aforementioned
problems. We emphasize that these primitives are used in a black-box manner, meaning that any
further improvements to their running times will automatically translate into improved algorithms
for the downstream problems. For our applications, we make the following parameterization, which
we expand upon in Remark B.1 and Section B.1. At a high level, many of our applications, such as
spectral sparsiﬁcation, are succinctly characterized by the following parameterization.

Parameterization 1.1. All of our algorithms are parameterized by the smallest edge weight in the
kernel matrix, i.e., the smallest edge weight in the matrix K is at least τ .

2

Published as a conference paper at ICLR 2023

Table 2: Summary of linear algebra and graph applications for KDE subroutines. We suppress
dependence on the precision ε. In spectral/local clustering and low-rank approximation, k denotes
the number of clusters and the rank of the approximation desired, respectively. The parameter φ
refers to the quality of the underlying clusters; see Section E.1.

Problem

Spectral sparsiﬁcation (Thm. 1.2)

Laplacian system solver (Thm. 1.2)
Low-rank approx. (Thm. 1.5)
Eigenvalue Spectrum approx. (Thm. 1.3)
Approximating 1st Eigenvalue (Thm. 1.4)
Local clustering (Thm. 1.6)

(cid:101)O

Spectral clustering (Thm. E.7)

Arboricity estimation (Thm. 1.8)

Triangle estimation (Thm. 1.9)

(cid:17)

(cid:17)

(cid:101)O

(cid:101)O

# of KDE Queries
(cid:16) n
τ 3
(cid:16) n
τ 3
O(n)
(cid:101)O(1/τ )
Remark B.2
poly(k) · 1
φ2
(cid:17)

√

(cid:16)

(cid:17)

n
τ 1.5

(cid:16) n
(cid:101)O
τ 2
(cid:101)O (cid:0) n
τ
(cid:16) 1
(cid:101)O
τ 3

(cid:1)

(cid:17)

(cid:17)

Post-processing time
(cid:16) nd
τ 3
(cid:16) nd
τ 3

O

O

(cid:17)

(cid:16)

O (n · poly (k) + nkd)
O(d/τ )
d · poly(1/τ )
√
n
poly(k) · 1
φ2
τ 1.5
(cid:17)
(cid:16) nd
+ (cid:101)O (nk)
τ 2
(cid:17)
(cid:101)O

O

(cid:101)O

(cid:17)

(cid:16) n2
τ
(cid:16) 1
τ 3

(cid:17)

(cid:101)O

Prior Work

Remark B.1

Remark B.1
Remark B.3
Ω(n2d)
ω(n) (Remark B.2)
Remark B.4

Remark B.4
(cid:101)O(n3) + O(n2d)
Ω(n2d)

Our applications derived from the basic graph primitives above can be partitioned into two overlap-
ping classes, linear-algebraic and graph theoretic results. Table 2 lists our applications along with
the number of KDE queries required in addition to any post-processing time. We refer to the spe-
ciﬁc sections of the body listed below for full details. We note that in all of our theorems below, we
assume access to a KDE data structure of Deﬁnition 1.1 with parameters ε and τ .

One of our main results is spectral sparsiﬁcation of the kernel matrix K interpreted as a weighted
graph. In Section D.1, we compute a sparse subgraph whose associated matrix closely approximates
that of the kernel matrix K. The most meaningful matrix to study for such a sparsiﬁcation is the
Laplacian matrix, deﬁned as D − K where D is a diagonal matrix of vertex degrees. The Laplacian
matrix encodes fundamental combinatorial properties of the underlying graph and has been well-
studied for numerous applications, including sparsiﬁcation; see (Merris, 1994; Batson et al., 2013;
Spielman, 2016) for a survey of the Laplacian and its applications. Our result computes a sparse
graph, with a number of edges that is linear in n, whose Laplacian matrix spectrally approximates
the Laplacian matrix of the original graph K under Parameterization 1.1.

Theorem 1.2 (Informal; see Thm. D.1). Let L be the Laplacian matrix corresponding to the graph
K. Then, for any ε ∈ (0, 1), there exists an algorithm that outputs a weighted graph G(cid:48) with only
m = O(n log n/(ε2τ 3)) edges, such that with probability at least 9/10, (1−ε)L (cid:22) LG(cid:48) (cid:22) (1+ε)L.
The algorithm makes (cid:101)O(m) KDE queries and requires (cid:101)O(md) post-processing time.

We compare our results with prior works in Remark B.1. We also show that Parameterization 1.1 is
inherent for spectral sparsiﬁcation. In particular, we use a hardness result from (Alman et al., 2020)
to show that for the Gaussian kernel, under the strong exponential time hypothesis (Impagliazzo
& Paturi, 2001), any algorithm that returns an O(1)-approximate spectral sparsiﬁer with O(n1.99)
edges requires Ω
time (see Theorem D.4 for a formal statement). Obtaining the
optimal dependence on τ remains an outstanding open question, even for Gaussian and Laplace ker-
nels. Spectral sparsiﬁcation has further downstream applications in solving Laplacian linear systems,
which we present in Section D.1.1.

n · 2log(1/τ )0.32(cid:17)

(cid:16)

Continuing the theme of the Laplacian matrix, in Section D.3, we also obtain a succinct summary of
the entire eigenvalue spectrum of the (normalized) Laplacian matrix using a total number of KDE
queries independent of n, the size of the dataset. The error of the approximation is measured in terms
of the earth mover distance (see Eq. (D.1)), or EMD, between the approximation and the true set
of eigenvalues. Such a result has applications in determining whether an underlying graph can be
modeled from a speciﬁc graph generative process (Cohen-Steiner et al., 2018).

Theorem 1.3 (Informal; see Theorem D.11). Let ε ∈ (0, 1) be the error parameter and L be the
normalized Laplacian of the kernel graph K. Let λ1 ≥ λ2 . . . ≥ λn be the eigenvalues of L and let
λ be the resulting vector. Then, there exists an algorithm that uses (cid:101)O (cid:0)exp (cid:0)1/ε2(cid:1) /τ (cid:1) KDE queries

3

Published as a conference paper at ICLR 2023

and exp (cid:0)1/ε2(cid:1) · d/τ post-processing time and outputs a vector (cid:101)λ such that with probability 99/100,
EMD

≤ ε.

(cid:16)

(cid:17)

λ, (cid:101)λ

Again to the best of our knowledge, all prior works for approximating the spectrum in EMD require
constructing the full graph beforehand, and thus have runtime Ω(n2d). Next, we obtain truly sub-
linear time algorithms for approximating the top eigenvalue and eigenvector of the kernel matrix,
a problem which was studied in (Backurs et al., 2021). Our result is the following theorem. Our
bounds, and those of prior work, depend on the parameter p, which refers to the exponent of τ in the
KDE query runtimes. For example for the Gaussian kernel, p ≈ 0.173. See Table 1 for other kernels.

Theorem 1.4 (Informal; see Theorem D.15). Given an n × n kernel matrix K that admits a KDE
data-structure with query time d/(ε2τ p) (Table 1), there exists an algorithm that outputs a unit
(cid:17)
(cid:16) ˜O(d/(ε4.5τ 4)), ˜O(d/(ε9+6pτ 2+2p))
vector v such that vT Kv ≥ (1 − ε)λ1(K) in time min
,
where λ1(K) denotes the largest eigenvalue of K.

We discuss related works in Remark B.2. In summary, the best prior result of (Backurs et al., 2021)
had a runtime of Ω(n1+p) whereas our bound has no dependence on n. Finally, our last linear-
algebraic result is an additive-error low-rank approximation of the kernel matrix, presented in Sec-
tion D.2.

Theorem 1.5 (Informal; see Cor. D.10). There exists an algorithm that outputs a rank- r matrix B
such that (cid:107)K − B(cid:107)2
F with probability 99% where Kr is the optimal rank-r
approximation of K. It uses n KDE queries and O(n · poly(r, 1/ε) + nrd/ε) post-processing time.

F ≤ (cid:107)K − Kr(cid:107)2

F + ε(cid:107)K(cid:107)2

We give detailed comparisons between our results and prior work in Remark B.3. As a summary,
nd (r/ε)ω−1(cid:17)
(Bakshi et al., 2020b) obtain a relative error approximation with a running time of (cid:101)O
,
where ω denotes the matrix multiplication constant, whereas our running time is dominated by
O(nrd/ε) and we obtain only additive error guarantees. Nevertheless, the algorithm we obtain,
which builds upon the sampling scheme of (Frieze et al., 2004), is a conceptually simpler algorithm
than the algorithm of (Bakshi et al., 2020b) and easier to empirically evaluate. Indeed, we implement
this algorithm in Section 2 and show that it is highly competitive to the SVD.

(cid:16)

We now move onto graph applications. We obtain an algorithm for local clustering, where we are
asked whether two vertices belong to the same or different vertex communities. The notion of a
cluster structure is based on the deﬁnition of a k-clusterable graph, formally introduced in Deﬁni-
tion E.3. Intuitively, it describes a graph whose vertices can be partitioned into k disjoint clusters
with high-connectivity within clusters and relatively sparse connectivity in-between clusters.

Theorem 1.6 (Informal; see Theorem E.5). Let K be a k-clusterable kernel graph with clusters
V = ∪1≤i≤kVi. Let U, W be one of (not necessarily distinct) clusters Vi. Let u, w be randomly
chosen vertices in partitions U and W with probability proportional to their degrees. There exists
√
n/τ 1.5) KDE queries and post-processing time,
c = c(ε, k) and an algorithm that uses (cid:101)O(c(k, ε)
with the property that with probability 1 − ε, if U = W then the algorithm reports that u and w are
in the same cluster and if U (cid:54)= W , the algorithm reports that u and w are in different clusters.

Our deﬁnitions for the local clustering result are adopted from prior literature in property testing; see
Remark B.4 for an overview of related works. Our sparsiﬁcation result also automatically lends itself
to an application in spectral clustering, an algorithm that clusters vertices based on the eigenvectors
of the Laplacian matrix, which is outlined in Section E.2. We obtain an algorithm for approximately
computing the top few eigenvectors of the Laplacian matrix, which is one of the main bottlenecks in
spectral clustering in practice, with subquadratic runtime. These approximate eigenvectors are used
to form the clusters.

Theorem 1.7 (Informal; see Theorem E.8). Let L be the Laplacian matrix of the spectral sparsiﬁer.
There exists an algorithm that can compute (1 + ε)-approximations of the ﬁrst k eigenvectors of L
in time (cid:101)O (cid:0)kn/(τ 2ε2.5)(cid:1).

We also give algorithms for approximating the arboricity of a graph, which is the density of the
densest subgraph of the kernel graph (see exact deﬁnition in Section E.3).

4

Published as a conference paper at ICLR 2023

Theorem 1.8 (Informal; see Theorem E.9). There exists an algorithm that uses m = (cid:101)O(n/(ε2τ ))
KDE queries and O(mn) post-processing time and outputs a sparse subgraph G(cid:48) of the kernel graph
such that with high probability, (1 − ε)αG ≤ αG(cid:48) ≤ (1 + ε)αG, where αG is the arboricity of G.

To the best of our knowledge, all prior works on computing the arboricity require the entire graph
to be known beforehand. In addition, computing the arboricity requires time (cid:101)O(nm) where m is
the number of edges leading to a runtime of (cid:101)O(n3) + O(n2d) (Gallo et al., 1989). In Section E.4,
we also give an algorithm for approximating the total weight of all triangles of K, again interpreted
as a weighted graph. We deﬁne weight of a triangle as the product of its edge weights. This is
a natural deﬁnition if weighted edges are interpreted as parallel unweighted edges, in addition to
having applications in deﬁning cluster coefﬁcients of weighted graphs (Kalna & Higham, 2006;
Li et al., 2007; Antoniou & Tsompa, 2008). Our bound is similar in spirit to the bound of the
unweighted case given in (Eden et al., 2017), under a different computation model. We refer to
Remark B.5 for discussions on related works.
Theorem 1.9 (Informal; see Theorem E.10). There exists an algorithm that makes (cid:101)O(1/τ 3) KDE
queries and the same bound for post-processing time and with probability at least 2
3 , outputs a
(1 ± ε)-approximation to the total weight of the triangles in the kernel graph.

On the other hand, there is a line of work that considers dimensionality reduction for kernel density
estimation e.g., through coresets (Phillips & Tai, 2018; 2020a; Tai, 2022). We view this direction of
work as orthogonal to our line of study. Lastly, the work (Backurs et al., 2021) is similar in spirit to
our work as they also utilize KDE queries to speed up algorithms for kernel matrices. Besides top
eigenvalue estimation mentioned before, (Backurs et al., 2021) also study the problem of estimating
the sum of all entries in the kernel matrix and obtain tight bounds for the latter.

1.2 Technical Overview

We provide a high-level overview and intuition for our algorithms. We ﬁrst highlight our algorithmic
building blocks for fundamental tasks and then describe how these components can be used to handle
a wide range of problems. We note that our building blocks use KDE data structures in a black-box
way and thus we describe their performance in terms of the number of queries to a KDE oracle.
We also note that a permeating theme across all subsequent applications is that we want to perform
some algorithmic task on a kernel matrix K without computing each of its entries k(xi, xj).

Algorithmic Building Blocks. We ﬁrst describe the “multi-level” KDE data structure, which con-
structs a KDE data structure on the entire input dataset X, and then recursively partitions X into
two halves, building a KDE data structure on each half. The main observation here is that if the
initialization of a KDE data structure uses runtime linear in the size n of X, then at each recursive
level, the initialization of the KDE data structures across all partitions remains linear. Since there
are O(log n) levels, the overall runtime to initialize our multi-level KDE data structure incurs only
a logarithmic overhead (see Figure 1 for an illustration).

Weighted vertex sampling. We describe how to sample vertices approximately proportional to their
weighted degree, where the weighted degree of a vertex xi with i ∈ [n] is wi = (cid:80)
j(cid:54)=i k(xi, xj).
We observe that performing n KDE queries sufﬁces to get an approximation of the weighted vertex
degree of all n vertices. We can thus think of vertex sampling as a preprocessing step that uses n
queries upfront and then allows for arbitrary sample access at any point in the future with no query
cost. Moreover, this preprocessing step of taking n queries only needs to be performed once. Further,
we can then perform weighted vertex sampling from a distribution that is ε-close in total variation to
the true distribution (see Theorem C.4 for details). Here, we use a multi-level tree structure to itera-
tively choose a subset of vertices with probability proportional to its approximate sum of weighted
degrees determined by the preprocessing step, until the ﬁnal vertex is sampled. Hence after the initial
n KDE queries, each query only uses O(log n) runtime, which is signiﬁcantly better than the na¨ıve
implementation that uses quadratic time to compute the entire kernel matrix.

Weighted neighbor edge sampling. We describe how to perform weighted neighbor edge sampling
for a given vertex x. The goal of weighted neighbor edge sampling is to efﬁciently output a vertex
j∈[n],xj (cid:54)=x k(x,xj ) for all k ∈ [n]. Unlike the degree case, edge sampling
v such that Pr[v = xk] =

(1±ε)k(x,xk)

(cid:80)

5

Published as a conference paper at ICLR 2023

is not a straightforward KDE query since the sampling probability is proportional to the kernel
value between two points, rather than the sum of multiple kernel values that a KDE query provides.
However, we can utilize a similar tree procedure as in Figure 1 in conjunction with KDE queries.

In particular, consider the tree in Figure 1 where each internal node corresponds to a subset of
neighbors of x. The two children of a parent node in the tree are simply the two approximately
equal subsets whose union make up the subset representing the parent node. We can descend down
the tree using the same probabilistic procedure as in the vertex sampling case: at every node, we
pick one of the children to descend into with probability proportional to the sum of the edge weights
represented by the children. The sum of edge weights of the children can be approximated by a query
to an appropriate KDE data structure in the “multi-level” KDE data structure described previously.
By appropriately decreasing the error of KDE data structures at each level of the tree, the sampled
neighbor satisﬁes the aforementioned sampling guarantee. Since the tree has height O(log n), then
we can perform weighted neighbor edge sampling, up to a tunably small total variation distance,
using O(log n) KDE queries and O(log n) time (see theorems C.5 and C.6 for details).

Random walks. We use our edge sampling procedure to output a random walk on the kernel graph,
where at any current vertex v of the walk, the next neighbor of v visited by the random walk is
chosen with probability proportional to the edge weights adjacent to v. In particular, for a random
walk with T steps, we can simply sequentially call our edge sampling procedure T times, with each
instance corresponding to a separate step in the random walk. Thus we can perform T steps of a
random walk, again up to a tunably small total variation distance, using O(T log n) KDE queries
and O(T log n) additional time.

Importance Sampling for the edge-vertex incidence matrix and the kernel matrix. We now
describe how to sample the rows of the edge vertex incident matrix H and the kernel matrix K
with probability proportional to the importance sampling score / leverage score (see Deﬁnition D.2).
We remark that approximately sampling proportional to the leverage score distribution for H is a
fundamental algorithmic primitive in spectral graph theory and numerical linear algebra. We note
that apriori, such a task seems impossible to perform in o(n2) time, even if the leverage scores are
precomputed for us, since the support of the distribution has size Θ(n2). However, note we do not
need to compute (even approximately) each leverage score to perform the sampling, but rather just
output an edge proportional to the right distribution.

2

We accomplish this by instead sampling proportional to the squared Euclidean norm of the rows of
H. It is known that oversampling the rows of a matrix by a factor that depends on the condition
number is sufﬁcient to approximate leverage score sampling (see proof of Theorem D.1). Further,
we show that H has a condition number (Lemma D.3) that is bounded by poly(1/τ ). Recall, the
(cid:1) × n matrix with the rows indexed by all possible
edge-vertex incident matrix is deﬁned as the (cid:0)n
edges and the columns indexed by vertices. For each e = {i, j}, we have H{i,j},i = (cid:112)k(xi, xj) and
H{i,j},j = −(cid:112)k(xi, xj). We pick the ordering of i and j arbitrarily. Note that this is a weighted
analogue of the standard edge-vertex incident matrix and satisﬁes H T H = LG where LG is the
Laplacian matrix of the graph corresponding to the kernel matrix K. For both H and K, we wish
to sample the rows with probability proportional to row normed squared. For example, the row re
corresponding to edge e = (xi, xj) in H satisﬁes (cid:107)re(cid:107)2
2 = 2k(xi, xj). Since the squared norm of
each row is proportional to the weight of the corresponding edge, we can perform this sampling
by combining the weighted vertex sampling and weighted neighbor edge sampling primitives: we
ﬁrst sample a vertex with probability proportional to its degree and then sample an appropriate
random neighbor. Thus our row norm sampling procedure is sufﬁcient to simulate leverage score
sampling (up to a condition number factor), which implies our downstream application of spectral
sparsiﬁcation.

We now describe the related primitive of sampling the rows of the kernel matrix K. Na¨ıvely per-
forming this sampling would require us to implicitly compute the entire kernel matrix, which as
mentioned previously, is prohibitive. However, if there exists a constant c such that the kernel func-
tion k that deﬁnes the matrix K satisﬁes k(x, y)2 = k(cx, cy) for all inputs x, y, then the (cid:96)2
2 norm
of each row can be approximated via a KDE query on the transformed dataset X (cid:48) = cX. In par-
ticular, the (cid:96)2
2 row norms of K are the vertex degrees of the kernel graph for X (cid:48). The property that
k(x, y)2 = k(cx, cy) holds for the most popular kernels such as the Laplacian, exponential, and
Gaussian kernels. Thus, we can sample the rows of the kernel matrix with the desired probabilities.

6

Published as a conference paper at ICLR 2023

Linear Algebra Applications. We now discuss our linear algebra applications.

Spectral sparsiﬁcation. Using the previously described primitives of weighted vertex sampling and
weighted neighbor edge sampling, we show that an ε spectral sparsiﬁer for the kernel density graph
G can be computed i.e., we compute a graph G(cid:48) such that for all vectors x, (1 − ε)xT LGx ≤
xT LG(cid:48)x ≤ (1 + ε)xT LG(cid:48)x, where LG and LG(cid:48) denote the Laplacian matrices of the graphs G
(cid:1) × n matrix such that H{i,j},i = (cid:112)k(xi, xj) and H{i,j},j =
and G(cid:48). Recall that H is the (cid:0)n
−(cid:112)k(xi, xj). Here we use subsets of [n] of size 2 to index the rows of H and the entry to be made
negative in the above deﬁnition is picked arbitrarily. It can be veriﬁed that H T H = LG. It is known
that sampling t = O(n log(n)/ε2) rows of the matrix H by using the so-called leverage scores gives
a t × (cid:0)n

(cid:1) selecting-and-scaling matrix S such that with probability at least 9/10,

2

2

(1 − ε)LG = (1 − ε)H T H (cid:22) H T ST SH (cid:22) (1 + ε)H T H = (1 + ε)LG.
(1.1)
Thus the matrix SH directly corresponds to a graph G(cid:48), which is an ε spectral sparsiﬁer for graph
G. The leverage scores of rows of H are also called “effective resistances” of edges of graph G.
Unfortunately, with the edge and neighbor vertex sampling primitives that we have, we cannot per-
form leverage score sampling of H. On the other hand, observe that the squared norm of row {i, j}
of H is 2k(xi, xj) and with an application of vertex sampling and edge sampling, we can sample
a row of H from the length squared distribution i.e., the distribution on rows where probability of
sampling a row is proportional to its squared norm. It is a standard result that sampling from squared
length distribution gives a selecting-and-scaling matrix S that satisﬁes (1.1), although we have to
sample t = O(κ2n log(n)/ε2) rows from this distribution, where κ = σmax(H)/σmin(H) denotes
the condition number of H (σmax(H) (resp. σmin(H)) denotes the largest (resp. smallest) positive
singular values).
With the parameterization that for all i (cid:54)= j, k(xi, xj) ≥ τ , we are able to show that κ ≤ O(1/τ 1.5).
Importantly, our upper bound on the condition number is independent of the data dimension and
number of input points. We obtain the upper bound on condition number by using a Cheeger-type
inequality for weighted graphs. Note that σmin(H) ≥ (cid:112)λ2(H T H) = (cid:112)λ2(LG), where we use
λ2(M ) to denote the second smallest eigenvalue of a positive semideﬁnite matrix. Cheeger’s in-
equality lower bounds exactly the quantity λ2(LG) in terms of graph conductance. A lower bound
of τ on every kernel value implies that every node in the Kernel Graph has a high weighted degree
and this lets us lower bound λ2(G) in terms of τ using a Cheeger-type inequality from (Friedland
& Nabben, 2002) and shows that O(n log(n)/τ 3ε2) samples from the approximate squared length
sampling distribution gives an ε spectral sparsiﬁer for the graph G.

First eigenvalue and eigenvector approximation. Our goal is to compute a 1 − ε approximation
to λ, the ﬁrst eigenvalue of K, and an accompanying approximate eigenvector. Such a task is key in
kernel PCA and related methods. We begin by noting that under the natural constraint that each row
of K sums to at least nτ , a condition used in prior works (Backurs et al., 2021), the ﬁrst eigenvalue
must be at least nτ by looking at the quadratic form associated with the all-ones vector.

√

Now we combine two disparate families of algorithms: ﬁrst the guarantees of (Bhattacharjee et al.,
2021; Bakshi et al., 2020a) show that sub-sampling a t × t principal submatrix of a PSD matrix
t) factor. Since we’ve shown the
preserves the eigenvalues of the matrix up to an additive O(n/
ﬁrst eigenvalue of K is at least nτ , we can set t roughly O(1/(ε2τ 2)) with the guarantee that the top
eigenvalue of the sub-sampled matrix is at lest (1 − ε)λ. Now we can either run the standard Krylov
method algorithm (Musco & Musco, 2015) to compute the top eigenvalue of the sampled matrix or
alternatively, we can instead use the algorithm of (Backurs et al., 2021), the prior state of the art, to
compute the eigenvalues of the sampled matrix. At a high level, their algorithm utilizes KDE queries
to approximately perform power method on the kernel graph without creating the kernel matrix. In
our case, we can instead run their algorithm on the smaller sampled dataset, which represents a
smaller kernel matrix. Our ﬁnal runtime is independent of n, the size of the dataset, whereas the
prior state of the art result of (Backurs et al., 2021) have a ω(n) runtime.

Graph Applications. We now discuss our graph applications.

Local clustering. The random walks primitive allow us to run a well-studied local clustering al-
gorithm on the kernel graph. The algorithm is quite standard in the property testing literature (see
(Czumaj et al., 2015) and (Peng, 2020)) so we see our main contribution here as showing how the
algorithm can be initialized for kernel matrices using our building blocks. At a high level, the goal

7

Published as a conference paper at ICLR 2023

of the algorithm is to determine if two input vertices u and v belong to the same cluster of the kernel
graph if the graph has a natural cluster structure (see Deﬁnition E.3 for the formal deﬁnition). The
well-studied algorithm in literature performs approximately O(
n) random walks from u and v of
a logarithmic length which is sufﬁcient to estimate the distance between the endpoint distribution of
the random walks. If the vertices belong to the same cluster, the distributions are close in (cid:96)2 distance
which can be detected via a standard distribution tester of (Chan et al., 2014). The guarantees of the
overall local clustering algorithm of (Czumaj et al., 2015) follow for kernel graphs since we only
need to access the graph via random walks.

√

w(E(GU ))
|U |

Arboricity estimation. The arboricity of a weighted graph G = (V, E, w) is deﬁned as α :=
maxU ⊆V
. Informally, the arboricity of a (weighted) graph represents the maximum
(weighted) density of a subgraph of G. To approximate the weighted arboricity, we adapt a re-
sult of (McGregor et al., 2015), who observed that to estimate the arboricity on unweighted graphs,
it sufﬁces to sample a set of ˜O(|V |/ε2) edges of G and computes the arboricity of the subsampled
graph, after rescaling the weight of edges inversely proportional to their sampling probabilities.

We show that a similar idea works for estimating arboricity on weighted graphs. Although (McGre-
gor et al., 2015) showed that each edge should be sampled independently without replacement, we
show that it sufﬁces to sample a ﬁxed number of edges with replacement. Moreover, we show that
each edge should be one of the weighted edges with probability proportional to the weight of the
edges, i.e., importance sampling. In fact, a similar result still holds if we only have upper bounds
on the weight of each edge, provided that we increase the number of ﬁxed edges that we sample by
the gap between the upper bound and the actual weight of the edge. Thus, our arboricity algorithm
requires sampling a ﬁxed number of edges, where each edge is sampled with probability propor-
tional to some known upper bound on its weight. However for kernel density graphs, this is just
our weighted edge sampling subroutine. Therefore, we achieve improved runtime over the na¨ıve ap-
proach of querying each edge in the kernel graph by using our weighted edge sampling subroutine
to sample a ﬁxed number of edges. Finally, we compute and output the arboricity of the subsampled
graph as an approximation to the arboricity of the input graph.

2 Empirical Evaluation

We present empirical evaluations for our algorithms. We chose to evaluate algorithms for low-rank
approximation (LRA) and spectral sparsiﬁcation (and spectral clustering as a corollary) as they are
arguably two of the most well studied examples in our applications and utilize a wide variety of
techniques present in our other examples of Sections D and E. Our evaluations serve as a proof
of concept that our queries which we constructed are efﬁcient and easy to implement in practice.
For our experiments, we use the Laplacian kernel k(x, y) = exp(−(cid:107)x − y(cid:107)1/σ). A fast KDE
implementation of this kernel exists due to (Backurs et al., 2019), which builds upon the techniques
of (Charikar & Siminelakis, 2017). Note that the focus of our work is to use KDE queries in a mostly
black box fashion to solve important algorithmic problems for kernel matrices. This viewpoint has
the important advantage that it is ﬂexible to the choice of any particular KDE query instantiation.
We chose to work with the implementation of (Backurs et al., 2019) since it possesses theoretical
guarantees, has an accessible implementation1, and has been used in experiments in prior works
such as (Backurs et al., 2019; 2021). However, we envision other choices of KDE queries, which
maybe have practical beneﬁts but are theoretically incomparable would also work well due to our
ﬂexibility.

Datasets. We use two real and two synthetic datasets in our experiments. The datasets used in the
low-rank approximation experiments are MNIST (points in R784) (LeCun, 1998) and Glove word
embeddings (points in R200) (Pennington et al., 2014). We use 104 points from each of the test
datasets. These datasets have been used in prior experimental works on kernel density estimation
(Siminelakis et al., 2019; Backurs et al., 2019). The datasets in experimental results for spectral
sparsiﬁcation and clustering are described in detail in F.

Evaluation Metrics. For LRA, we use the additive error algorithm detailed in Corollary D.10 of
Section D.2. It requires sampling the rows of the kernel matrix according to squared row norms,

1from https://github.com/talwagner/efficient_kde

8

Published as a conference paper at ICLR 2023

which can be done via KDE queries as outlined there. Once the (small) number of rows are sampled,
we explicitly construct these rows using kernel evaluations. We compare the approximation error of
this method computed via the standard frobenius norm error to a state of the art sketching algorithm
for computing low-rank approximations, which is the input-sparsity time algorithm of Clarkson and
Woodruff (Clarkson & Woodruff, 2013) (IS). We also compare to an iterative SVD solver (SVD).
All linear algebra subroutines rely on Numpy, Scipy, and Numba implementations when applicable.

Low-rank approximation results. Note that the algorithm in Corollary D.10 has a O(k) depen-
dence on the number of rows sampled. Concretely we sample 25k rows for a rank k approximation
which we ﬁx it for all experiments. For the MNIST dataset, the rank versus approximation error is
shown in Figure 2a. The performance of our algorithm labeled as KDE is given by the blue curve
while the orange curve represents the IS algorithm. The green curve represents the SVD error, which
is a lower bound on the error for any algorithm. Note that for SVD calculations, we do not calculate
the full SVD since that is computationally prohibitive; instead, we use an iterative solver. We can
see that the errors of all three methods are comparable to each other. In terms of runtime, the KDE
based method took 24.7 seconds on average for the rank 50 approximation whereas IS took 71.5
seconds and iterative SVD took 74.72 seconds on average. This represents a 2.9x decrease in the
running time. The time measured includes the time to initialize the data structures and matrices used
for the respective algorithms. In terms of the number of kernel evaluations, both IS and iterative
SVD require the kernel matrix, which is 108 kernel evaluations. On the other hand for the rank 50
approximation, our method required only 1.1 · 107 kernel evaluations, which is a 9x decrease in the
number of evaluations. In terms of space, IS and iterative SVD require 108 ﬂoating point numbers
stored due to initializing the full 104 × 104 matrix whereas our method only requires 104 · 25 · 50
ﬂoating point numbers for the rank equal to 50 case and smaller for other. This is a 8x decrease in the
space required. Lastly, we verify that we are indeed sampling from the correct distribution required
by Corollary D.10. In Figure 2b, we plot the points (xi, yi) where xi is the row norm squared for
the ith row of the kernel matrix K and yi is the row norm squared computed in our approximation
algorithm (see Algorithm 9). As shown in Figure 2b, the data points fall very close to the y = x line
indicating that our algorithm is indeed sampling from approximately the correct ideal distribution.

The qualitatively similar results for the Glove dataset are given in Figures 2c and 2d. For the glove
dataset, the average time taken by the three algorithms were 37.7s, 37.7s, and 44.2s respectively,
indicating that KDE and IS were comparable in runtime whereas SVD took slightly longer. How-
ever, the number of kernel evaluations required by the latter two algorithms was signiﬁcantly larger:
for rank equal to 10, our algorithm only required 2.6 · 106 kernel evaluations while the other meth-
ods both required 108 due to initializing the matrix. The space required by our algorithm was also
smaller by a factor of 40 since we only explicitly compute 25 · 10 rows for the rank = 10 case. For
Glove, we only perform our experiments up to rank 10 since the iterative SVD failed to converge for
higher ranks. While computing the full SVD avoids the convergence issue, it’s computationally pro-
hibitive. For example for MNIST, computing the full SVD of the kernel matrix took 552.9s, which
is approximately an order of magnitude longer than any of the other methods.

Acknowledgements Ainesh Bakshi was supported by Ankur Moitra’s ONR grant. Praneeth
Kacham was supported by National Institute of Health (NIH) grant 5401 HG 10798-2, a Simons In-
vestigator Award of David P. Woodruff, and Google as part of the “Research Collabs” program. Piotr
Indyk and Sandeep Silwal were supported by the NSF TRIPODS program (award DMS-2022448),
Simons Investigator Award, MIT-IBM Watson AI Lab and NSF Graduate Research Fellowship un-
der Grant No. 1745302. Work done in part while Samson Zhou was at Carnegie Mellon University
and supported by a Simons Investigator Award of David P. Woodruff and by the National Science
Foundation under Grant No. CCF-1815840.

References

Mohammad Al Hasan and Vachik S Dave. Triangle counting in large networks: a review. Wiley

Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(2):e1226, 2018.

9

Published as a conference paper at ICLR 2023

Josh Alman, Timothy Chu, Aaron Schild, and Zhao Song. Algorithms and hardness for linear
algebra on geometric graphs. In 2020 IEEE 61st Annual Symposium on Foundations of Computer
Science (FOCS), pp. 541–552. IEEE, 2020.

Ioannis E Antoniou and ET Tsompa. Statistical analysis of weighted networks. Discrete dynamics

in Nature and Society, 2008, 2008.

Albert Atserias, Martin Grohe, and Daniel Marx. Size bounds and query plans for relational joins.

In 49th Annual IEEE Symposium on Foundations of Computer Science, 2008.

Arturs Backurs, Piotr Indyk, and Ludwig Schmidt. On the ﬁne-grained complexity of empirical
In Advances in Neural Information
risk minimization: Kernel methods and neural networks.
Processing Systems 30: Annual Conference on Neural Information Processing Systems, pp. 4308–
4318, 2017.

Arturs Backurs, Moses Charikar, Piotr Indyk, and Paris Siminelakis. Efﬁcient density evaluation
for smooth kernels. 2018 IEEE 59th Annual Symposium on Foundations of Computer Science
(FOCS), pp. 615–626, 2018.

Arturs Backurs, Piotr Indyk, and Tal Wagner. Space and time efﬁcient kernel density estimation in
high dimensions. In Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems, NeurIPS, pp. 15773–15782, 2019.

Arturs Backurs, Piotr Indyk, Cameron Musco, and Tal Wagner. Faster kernel matrix algebra via
density estimation. In Proceedings of the 38th International Conference on Machine Learning,
pp. 500–510, 2021.

Ainesh Bakshi and David Woodruff. Sublinear time low-rank approximation of distance matrices.

Advances in Neural Information Processing Systems, 31, 2018.

Ainesh Bakshi, Nadiia Chepurko, and Rajesh Jayaram. Testing positive semi-deﬁniteness via ran-
dom submatrices. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science
(FOCS), pp. 1191–1202. IEEE, 2020a.

Ainesh Bakshi, Nadiia Chepurko, and David P Woodruff. Robust and sample optimal algorithms for
psd low rank approximation. In 2020 IEEE 61st Annual Symposium on Foundations of Computer
Science (FOCS), pp. 506–516. IEEE, 2020b.

Joshua Batson, Daniel A Spielman, Nikhil Srivastava, and Shang-Hua Teng. Spectral sparsiﬁcation

of graphs: theory and algorithms. Communications of the ACM, 56(8):87–94, 2013.

Suman K. Bera and Amit Chakrabarti. Towards tighter space bounds for counting triangles and
other substructures in graph streams. In Symposium on Theoretical Aspects of Computer Science
(STACS 2017), 2017.

Rajarshi Bhattacharjee, Cameron Musco, and Archan Ray. Sublinear time eigenvalue approxima-
tion via random sampling. CoRR, abs/2109.07647, 2021. URL https://arxiv.org/abs/
2109.07647.

Siu-on Chan, Ilias Diakonikolas, Paul Valiant, and Gregory Valiant. Optimal algorithms for test-
ing closeness of discrete distributions. In Proceedings of the Twenty-Fifth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA, pp. 1193–1203, 2014.

Moses Charikar. Greedy approximation algorithms for ﬁnding dense components in a graph. In
Approximation Algorithms for Combinatorial Optimization, Third International Workshop, AP-
PROX, Proceedings, pp. 84–95, 2000.

Moses Charikar and Paris Siminelakis. Hashing-based-estimators for kernel density in high di-
In 58th IEEE Annual Symposium on Foundations of Computer Science, FOCS, pp.

mensions.
1032–1043, 2017.

Moses Charikar, Michael Kapralov, Navid Nouri, and Paris Siminelakis. Kernel density estima-
tion through density constrained near neighbor search. 2020 IEEE 61st Annual Symposium on
Foundations of Computer Science (FOCS), pp. 172–183, 2020.

10

Published as a conference paper at ICLR 2023

Justin Y. Chen, Talya Eden, Piotr Indyk, Honghao Lin, Shyam Narayanan, Ronitt Rubinfeld,
Sandeep Silwal, Tal Wagner, David P. Woodruff, and Michael Zhang. Triangle and four cycle
counting with predictions in graph streams. CoRR, abs/2203.09572, 2022.

Xue Chen and Eric Price. Condition number-free query and active learning of linear families. CoRR,

abs/1711.10051, 2017.

Ashish Chiplunkar, Michael Kapralov, Sanjeev Khanna, Aida Mousavifar, and Yuval Peres. Testing
graph clusterability: Algorithms and lower bounds. In 2018 IEEE 59th Annual Symposium on
Foundations of Computer Science (FOCS), pp. 497–508. IEEE, 2018.

Fan RK Chung and Fan Chung Graham. Spectral graph theory. American Mathematical Soc., 1997.

Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input

sparsity time. In Symposium on Theory of Computing Conference, STOC, pp. 81–90, 2013.

David Cohen-Steiner, Weihao Kong, Christian Sohler, and Gregory Valiant. Approximating the
In Proceedings of the 24th ACM SIGKDD International Conference on

spectrum of a graph.
Knowledge Discovery & Data Mining, KDD, pp. 1263–1271, 2018.

Artur Czumaj and Christian Sohler. Testing expansion in bounded-degree graphs. Combinatorics,

Probability and Computing, 19(5-6):693–709, 2010.

Artur Czumaj, Pan Peng, and Christian Sohler. Testing cluster structure of graphs. In Proceedings
of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC, pp. 723–732,
2015.

Tamal K Dey, Pan Peng, Alfred Rossi, and Anastasios Sidiropoulos. Spectral concentration and

greedy k-clustering. Computational Geometry, 76:19–32, 2019.

Talya Eden, Amit Levi, Dana Ron, and C. Seshadhri. Approximately counting triangles in sublinear

time. SIAM J. Comput., 46(5):1603–1646, 2017.

Brooke Foucault Welles, Anne Van Devender, and Noshir Contractor. Is a ”friend” a friend? inves-
tigating the structure of friendship networks in virtual worlds. In CHI - The 28th Annual CHI
Conference on Human Factors in Computing Systems, Conference Proceedings and Extended
Abstracts, pp. 4027–4032, 2010.

Shmuel Friedland. Lower bounds for the ﬁrst eigenvalue of certain m-matrices associated with

graphs. Linear Algebra and its Applications, 172:71–84, 1992.

Shmuel Friedland and Reinhard Nabben. On Cheeger-type inequalities for weighted graphs. Journal

of Graph Theory, 41(1):1–17, 2002.

Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for ﬁnding low-rank

approximations. Journal of the ACM (JACM), 51(6):1025–1041, 2004.

Giorgio Gallo, Michael D. Grigoriadis, and Robert Endre Tarjan. A fast parametric maximum ﬂow

algorithm and applications. SIAM J. Comput., 18(1):30–55, 1989.

Grzegorz Gluch, Michael Kapralov, Silvio Lattanzi, Aida Mousavifar, and Christian Sohler. Spec-
tral clustering oracles in sublinear time. In Proceedings of the 2021 ACM-SIAM Symposium on
Discrete Algorithms (SODA), pp. 1598–1617. SIAM, 2021.

Oded Goldreich. Introduction to property testing. Cambridge University Press, 2017.

Oded Goldreich and Dana Ron. On testing expansion in bounded-degree graphs. In Studies in Com-
plexity and Cryptography. Miscellanea on the Interplay between Randomness and Computation,
pp. 68–75. Springer, 2011.

Alexander G Gray and Andrew W Moore. N-body problems in statistical learning. Advances in

neural information processing systems, pp. 521–527, 2001.

11

Published as a conference paper at ICLR 2023

Alexander G Gray and Andrew W Moore. Nonparametric density estimation: Toward computational
tractability. In Proceedings of the 2003 SIAM International Conference on Data Mining, pp. 203–
211. SIAM, 2003.

Thomas Hofmann, Bernhard Sch¨olkopf, and Alexander J Smola. Kernel methods in machine learn-

ing. The annals of statistics, 36(3):1171–1220, 2008.

Russell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. Journal of Computer and

System Sciences, 62(2):367–375, 2001.

Piotr Indyk, Ali Vakilian, Tal Wagner, and David P. Woodruff. Sample-optimal low-rank approxi-
mation of distance matrices. In Conference on Learning Theory, COLT, pp. 1723–1751, 2019.

Satyen Kale and C Seshadhri. Testing expansion in bounded degree graphs. 35th ICALP, pp. 527–

538, 2008.

Gabriela Kalna and Desmond J Higham. Clustering coefﬁcients for weighted networks. In Sympo-

sium on network analysis in natural sciences and engineering, pp. 45, 2006.

Matti Karppa, Martin Aum¨uller, and Rasmus Pagh. Deann: Speeding up kernel-density estimation
using approximate nearest neighbor search. In Proceedings of The 25th International Conference
on Artiﬁcial Intelligence and Statistics, pp. 3108–3137, 2022.

Mihail N. Kolountzakis, Gary L. Miller, Richard Peng, and Charalampos E. Tsourakakis. Efﬁcient
triangle counting in large graphs via degree-based vertex partitioning. Lecture Notes in Computer
Science, pp. 15–24, 2010.

Ioannis Koutis, Gary L. Miller, and Richard Peng. A nearly-m log n time solver for sdd linear
systems. In IEEE 52nd Annual Symposium on Foundations of Computer Science, pp. 590–598,
2011.

Tsz Chiu Kwok, Lap Chi Lau, Yin Tat Lee, Shayan Oveis Gharan, and Luca Trevisan. Improved
cheeger’s inequality: analysis of spectral partitioning algorithms through higher order spectral
gap. In Symposium on Theory of Computing Conference, STOC’13, pp. 11–20, 2013.

Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.

Dongryeol Lee and Alexander Gray. Fast high-dimensional kernel summations using the monte
carlo multipole method. Advances in Neural Information Processing Systems, 21:929–936, 2008.

Dongryeol Lee, Andrew W Moore, and Alexander G Gray. Dual-tree fast gauss transforms.

In

Advances in Neural Information Processing Systems, pp. 747–754, 2006.

James R. Lee, Shayan Oveis Gharan, and Luca Trevisan. Multi-way spectral partitioning and higher-
order cheeger inequalities. In Proceedings of the 44th Symposium on Theory of Computing Con-
ference, STOC, pp. 1117–1130, 2012.

Victor E. Lee, Ning Ruan, Ruoming Jin, and Charu C. Aggarwal. A survey of algorithms for dense
subgraph discovery. In Managing and Mining Graph Data, volume 40 of Advances in Database
Systems, pp. 303–336. Springer, 2010.

Jure Leskovec, Lars Backstrom, Ravi Kumar, and Andrew Tomkins. Microscopic evolution of social
In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge

networks.
Discovery and Data Mining, pp. 462–470, 2008.

Wenyuan Li, Yongjing Lin, and Ying Liu. The structure of weighted small-world networks. Physica

A: Statistical Mechanics and its Applications, 376:708–718, 2007.

Anand Louis, Prasad Raghavendra, Prasad Tetali, and Santosh S. Vempala. Many sparse cuts via
higher eigenvalues. In Proceedings of the 44th Symposium on Theory of Computing Conference,
STOC, pp. 1131–1140, 2012.

William B March, Bo Xiao, and George Biros. Askit: Approximate skeletonization kernel-
independent treecode in high dimensions. SIAM Journal on Scientiﬁc Computing, 37(2):A1089–
A1110, 2015.

12

Published as a conference paper at ICLR 2023

Andrew McGregor, David Tench, Sofya Vorotnikova, and Hoa T. Vu. Densest subgraph in dynamic
In Mathematical Foundations of Computer Science 2015 - 40th International

graph streams.
Symposium, MFCS, Proceedings, Part II, pp. 472–482, 2015.

Russell Merris. Laplacian matrices of graphs: a survey. Linear algebra and its applications, 197:

143–176, 1994.

R. Milo, S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon. Network motifs: Simple

building blocks of complex networks. Science, 298(5594):824–827, 2002.

Vlad I Morariu, Balaji Vasan Srinivasan, Vikas C Raykar, Ramani Duraiswami, and Larry S Davis.

Automatic online tuning for fast gaussian summation. In NIPS, pp. 1113–1120, 2008.

Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and faster
approximate singular value decomposition. In Advances in Neural Information Processing Sys-
tems 28: Annual Conference on Neural Information Processing Systems, pp. 1396–1404, 2015.

Cameron Musco and David P Woodruff. Sublinear time low-rank approximation of positive semidef-
In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science

inite matrices.
(FOCS), pp. 672–683. IEEE, 2017.

Pan Peng. Robust clustering oracle and local reconstructor of cluster structure of graphs. In Pro-
ceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA, pp. 2953–2972.
SIAM, 2020. doi: 10.1137/1.9781611975994.179. URL https://doi.org/10.1137/1.
9781611975994.179.

Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532–1543, 2014.

Jeff M Phillips. ε-samples for kernels.

In Proceedings of the twenty-fourth annual ACM-SIAM

symposium on Discrete algorithms, pp. 1622–1632. SIAM, 2013.

Jeff M. Phillips and Wai Ming Tai. Improved coresets for kernel density estimates. In Proceedings of
the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA, pp. 2718–2727,
2018.

Jeff M. Phillips and Wai Ming Tai. Near-optimal coresets of kernel density estimates. Discret.

Comput. Geom., 63(4):867–887, 2020a.

Jeff M Phillips and Wai Ming Tai. Near-optimal coresets of kernel density estimates. Discrete &

Computational Geometry, 63(4):867–887, 2020b.

Kent Quanrud. Spectral sparsiﬁcation of metrics and kernels. In Proceedings of the 2021 ACM-SIAM

Symposium on Discrete Algorithms, SODA, pp. 1445–1464, 2021.

Parikshit Ram, Dongryeol Lee, William March, and Alexander Gray. Linear-time algorithms for
pairwise statistical problems. Advances in Neural Information Processing Systems, 22:1527–
1535, 2009.

Bernhard Sch¨olkopf, Alexander J Smola, Francis Bach, et al. Learning with kernels: support vector

machines, regularization, optimization, and beyond. MIT press, 2002.

C. Seshadhri, Ali Pinar, and Tamara G. Kolda. Fast triangle counting through wedge sampling. In

the International Conference on Data Mining (ICDM), 2013.

John Shawe-Taylor, Nello Cristianini, et al. Kernel methods for pattern analysis. Cambridge uni-

versity press, 2004.

Paris Siminelakis, Kexin Rong, Peter Bailis, Moses Charikar, and Philip Alexander Levis. Rehashing
kernel evaluation in high dimensions. In Proceedings of the 36th International Conference on
Machine Learning, ICML, pp. 5789–5798, 2019.

Daniel A Spielman. The laplacian matrices of graphs. In Plenary Talk, IEEE Intern. Symp. Inf.

Theory (ISIT)n, 2016.

13

Published as a conference paper at ICLR 2023

Daniel A Spielman and Nikhil Srivastava. Graph sparsiﬁcation by effective resistances. SIAM

Journal on Computing, 40(6):1913–1926, 2011.

Daniel A. Spielman and Shang-Hua Teng. Nearly-linear time algorithms for graph partitioning,
graph sparsiﬁcation, and solving linear systems. In Proceedings of the 36th Annual ACM Sympo-
sium on Theory of Computing, pp. 81–90, 2004.

Wai Ming Tai. Optimal coreset for gaussian kernel density estimation. In 38th International Sym-

posium on Computational Geometry, SoCG, pp. 63:1–63:15, 2022.

Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416,

2007.

David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends® in

Theoretical Computer Science, 10(1–2):1–157, 2014.

Yan Zheng, Jeffrey Jestes, Jeff M Phillips, and Feifei Li. Quality and efﬁciency for kernel density
estimates in large data. In Proceedings of the 2013 ACM SIGMOD International Conference on
Management of Data, pp. 433–444, 2013.

A Omitted Technical Overview

We provide a technical overview for applications whose discussions were omitted from the main
text.

Kernel matrix low-rank approximation.
such that

In this setting, our goal here is to output a matrix B

(cid:107)K − B(cid:107)2

F ≤ (cid:107)K − Kt(cid:107)2

F + ε(cid:107)K(cid:107)2
F

where Kt is the best rank-t approximation to the kernel matrix K. The efﬁcient algorithm of (Frieze
et al., 2004) is able to achieve this guarantee if one can sample the ith row ri of K with probability
pi ≥ Ω(1) · (cid:107)ri(cid:107)2
F . We can perform such an action using our primitive, which is capable
of sampling the rows of K with probability proportional to the squared row norms for the Lapla-
cian, exponential, and Gaussian kernels. Thus for these kernels, we can immediately obtain efﬁcient
algorithms for computing a low-rank approximation.

2/(cid:107)K(cid:107)2

Spectrum approximation. For this problem, the goal is to compute approximations of all the
eigenvalues of the normalized Laplacian matrix of the kernel graph such that the error between
the approximations and the true set of eigenvalues has small error in the earth mover metric. The
algorithm of (Cohen-Steiner et al., 2018) achieves this guarantee in time independent in the graph
size given the ability to perform random walks on uniformly sampled vertices. Surprisingly, the
number of random walks and their length does not depend on the number of vertices. Thus given
our random walk primitive, we can efﬁciently simulate the algorithm of (Cohen-Steiner et al., 2018)
on kernel graphs in a black-box manner.

Spectral clustering. Given our spectral sparsiﬁcation result, we can immediately obtain a fast
version of a heuristic algorithm used in practice for graph clustering: we embed each vertex into Rk
using k eigenvectors of the Laplacian matrix and run k-means clustering. Clearly if we have a sparse
graph, the eigenvector computation is faster. Theoretically, we can show that spectral sparsiﬁcation
preserves a notion of clusterability which is weaker than the deﬁnition used in the local clustering
section and we additionally give empirical evidence of the validity of this procedure.

14

Published as a conference paper at ICLR 2023

Weighted triangle estimation. We deﬁne the weight of a triangle as the product of its edges, gen-
eralizing the case were the edges have integer lengths, so that an edge can be thought of as multiple
parallel edges. Under this deﬁnition, we adapt an algorithm from (Eden et al., 2017), who considered
the problem in unweighted graphs given query access to the underlying graph. Speciﬁcally, we show
that it sufﬁces to sample a “small” set R of edges uniformly at random and then estimate the total
weight of triangles including the edges of R under some predetermined ordering. In particular, the
procedure of estimating the total weight of triangles including the edges e of R involves sampling
neighbors of the vertices of e, which we can efﬁciently implement using our weighted neighbor edge
sampling subroutine.

B Further Related Works

Remark B.1. Spectral sparsiﬁcation for kernel graphs has also been studied in prior works, no-
tably in (Alman et al., 2020) and (Quanrud, 2021). We ﬁrst compare to (Alman et al., 2020), who
obtain a spectral sparsiﬁcation using an entirely different approach. They obtain an almost linear
time sparsiﬁer (n1+o(1)) when the kernel is multiplicativily Lipschitz (see Section 1.1.2 in (Alman
et al., 2020) for deﬁnition) and show hardness for constructing such a sparsiﬁer when it is not.
Focusing on the Gaussian kernel, under Parameterization 1.1, (Alman et al., 2020) obtain an al-
gorithm that runs in time O
, whereas our algorithm runs in
O (cid:0)nd log2(n)/(ε2τ 2.0173+o(1))(cid:1) time. We also note that the dimension d can be upper bounded
by O(log n/ε2) by applying Johnson-Lindenstrauss to the initial dataset. Therefore, (Alman et al.,
2020) obtain a better dependence on 1/τ , whereas we obtain a better dependence on n. A similar
comparison can be established for other kernels as well. In practice, τ is set to be a small ﬁxed con-
stant, whereas n can be arbitrarily large. Indeed in practice, a common setting of τ is 0.01 or 0.001,
irrespective of the size of the dataset (March et al., 2015; Siminelakis et al., 2019; Backurs et al.,
2019; 2021; Karppa et al., 2022).

log(1/τ ) log n log(log n)/ε2(cid:17)

nd + n2

√

(cid:16)

We now compare our guarantees to that of (Quanrud, 2021). The author studies spectral sparsiﬁ-
cation resurrected to smooth kernels (for example kernels of the form 1/(cid:107)x − y(cid:107)t
2 which have a
polynomial decay; see (Quanrud, 2021) for a formal deﬁnition). This family does not include Gaus-
sian, Laplacian, or exponential kernels. For smooth kernels, (Quanrud, 2021) obtained a sparsiﬁer
with a nearly optimal ˜O(n/ε2) number of edges in time ˜O(nd/ε2). Our algorithm obtains a similar
dependence in n, d, ε but includes an additional 1/τ 3 factor. However, it generalizes for any kernel
supporting a KDE data structure, which includes smooth kernels (Backurs et al., 2018) (see Table
1 for a summary of kernels where our results apply). Our techniques are also different: (Quanrud,
2021) does not use KDE data structures in a black-box manner to compute the sparsiﬁcation as we
do. Rather, they simulate importance sampling on the edges of the kernel graph directly. In addition
to the nearly linear sparisiﬁer, another interesting feature of (Quanrud, 2021) is that it enriches the
connections between spectral sparsiﬁcation of kernel graphs and KDE data structures. Indeed, the
data structures used in (Quanrud, 2021) are inspired by and were used in the prior work of (Backurs
et al., 2018) to create KDE query data structures themselves. Furthermore, the paper demonstrates
how to instantiate KDE data structures for smooth kernels using the kernel graph sparsiﬁer itself.
We refer to (Quanrud, 2021) for details.
Remark B.2. We remark that our algorithm returns a sparse vector v supported on roughly
O(1/(ε2τ 2)) coordinates. The best prior result is that of (Backurs et al., 2021) which presented
an algorithm with total runtime O

(cid:16) dn1+p log(n/ε)2+p
ε7+4p

(cid:17)

.

In comparison, our bound has no dependence on n and is thus a truly sublinear runtime. Note that
the bound of (Backurs et al., 2021) does not depend on τ . We do not state the number of KDE
queries used explicitly in Table 2 since our algorithm uses KDE queries on a subsampled dataset
and in addition, only uses them by calling the algorithm of (Backurs et al., 2021) as a subroutine
(on the subsampled dataset). The algorithm of (Backurs et al., 2021) uses ˜O(1/ε) KDE queries but
with various different initialization of τ so it is not meaningful to state “one” bound for the number
of KDE queries used and thus the ﬁnal runtime is a more meaningful quantity to state. Lastly, the
authors in (Backurs et al., 2021) present a lower bound of Ω(nd) for estimating the top eigenvalue
λ1, which ostensibly seems at odds with our stated bound which has no dependence on n. However,

15

Published as a conference paper at ICLR 2023

the lower bound presented in (Backurs et al., 2021) essentially sets τ = 1/ poly(n) for a large
polynomial factor depending on n (we estimate this factor to be Ω(n2)). Since we parameterize our
dependence via τ , which in practice is often set to a ﬁxed constant, we can bypass the lower bound.
Remark B.3. We now compare our low-rank approximation result with a recent work of (Musco &
Woodruff, 2017; Bakshi et al., 2020b). They showed the following theorem:
Theorem B.1 (Theorem 4.2, (Bakshi et al., 2020b)). Given a n × n PSD matrix A, target rank
r ∈ [n] and accuracy parameter ε ∈ (0, 1), there exists an algorithm that queries (cid:101)O (nr/ε) entries
in A and with probability at least 9/10, outputs a rank-r matrix B such that
(cid:107)A − B(cid:107)2

F ≤ (1 + ε) (cid:107)A − Ar(cid:107)2
F ,

where Ar is the best rank-r approximation to A. Further, the running time is (cid:101)O
ω is the matrix multiplication constant.

(cid:16)

n (r/ε)ω−1(cid:17)

, where

We note that their result applies to kernel matrices as well via the following fact.
Fact B.2 (Kernel Matrices are PSD, (Sch¨olkopf et al., 2002)). Let k be a reproducing kernel and X
be n data points in Rd. Let K be the associated n × n kernel matrix such that Ki,j = k(xi, xj).
Then, K (cid:31) 0.

Here, the family of reproducing kernels is quite broad and includes polynomial kernels, Gaussian,
and Laplacian kernel, among others. Therefore, their theorem immediately implies a relative error
low-rank approximation algorithm for kernel matrices. Our result and the theorem of (Bakshi et al.,
2020b) have comparable runtimes. While (Bakshi et al., 2020b) obtain relative-error guarantees, we
only obtain additive-error guarantees.

(cid:16)

nd (r/ε)ω−1(cid:17)

However, reading each entry of the kernel matrix require O(d) time and thus (Bakshi et al., 2020b)
, whereas our running time is dominated by O(nrd/ε).
obtain an running time of (cid:101)O
We note that similar ideas as our algorithm for additive error LRA were previously used to de-
sign subquadratic algorithms running in time o(n2) for low-rank approximation of distance matri-
ces (Bakshi & Woodruff, 2018; Indyk et al., 2019).
Remark B.4. Our deﬁnitions for the local clustering result are adopted from prior literature in
property testing; see (Kale & Seshadhri, 2008; Czumaj & Sohler, 2010; Goldreich & Ron, 2011;
Czumaj et al., 2015; Chiplunkar et al., 2018; Dey et al., 2019; Peng, 2020; Gluch et al., 2021) and
the references within. Our algorithmic details for the local cluster section are also derived from prior
works, such as the works of (Czumaj et al., 2015) and (Peng, 2020); indeed, many of the lemmas of
the local clustering section follow in a straightforward fashion from (Czumaj et al., 2015) and (Peng,
2020). However, the key difference between these works and our work is that they are in the property
testing model where one assumes access to various graph queries in order to design sublinear graph
algorithms. To the best of our knowledge, implementation of prior works on local clustering requires
having access to the entire neighbor of a vertex when performing random walks, thereby implying
the runtime of Ω(nd) per step of the walk. In contrast, we give efﬁcient constructions of these
commonly assumed queries for kernel graphs, rather than assuming oracle access. Indeed, the fact
that one can easily take existing algorithms which hold in non kernel settings and apply them to
kernel settings in a straightforward manner via our queries can be viewed a major strength of our
work.
Remark B.5. Our general bound of the number of KDE qeuries required to approximate the total
weight of triangles in Theorem E.10 is (cid:101)O(m
wG/wT ), where wG is the sum of all entries of K and
wT is the total weight of triangles we wish to approximate. This bound is a natural generalization of
the result of (Eden et al., 2017). There, the goal is to approximate the total number of triangles in an
unweighted graph given access to queries of an underlying graph in the form of random vertices and
random neighbors of a given vertex (assuming the entire graph is stored in memory). While their
model differs from our work, we note that KDE queries constructed in Section C play a similar role
to the queries used in (Eden et al., 2017). There the authors give a bound of (cid:101)O(m3/2/T ) queries
where T is the total number of triangles. In our case, we indeed get a bound of the order of m3/2 in
the numerator as wG ≤ mwmax and wT is the natural analogue of T in (Eden et al., 2017). Finally
note that under our parameterization of every edge in the kernel graph possessing weight at most 1
and at least τ , our bound reduces to at most (cid:101)O(1/τ 3) KDE queries.

√

16

Published as a conference paper at ICLR 2023

We ﬁnally note that to the best of our knowledge, all prior works for approximating the number of
triangles in a graph require the full graph to be instantiated, which implies a lower bound of time
Ω(n2d) in our setting.

We also note that our paper is closely related to the ﬁeld of (graph) property testing. In graph prop-
erty testing, it is customary to assume query access to an unknown graph via vertex and edge
queries (Goldreich, 2017). While speciﬁc details vary, common queries include access to random
vertices and random neighbors of a given vertex, among others. The goal of the ﬁeld is to design
algorithms that require queries sublinear in n, the number of vertices, or n2, the size of the graph.
We can interpret the graph primitives we construct as a realization of the property testing model
where queries are explicitly constructed.

B.1 Preliminaries

First, we discuss the cost of constructing KDE data structure and performing the queries described
in Deﬁnition 1.1. Table 1 summarizes previous work on kernel density estimation though for the
sake of uniformity, we list only “high-dimensional” data structures, whose running times are poly-
nomial in the dimension d. Those data structures have construction times of the form O(dn/(τ pε2))
and answer KDE queries in time O(d/(τ pε2)), under the condition that for all queries y we have
1
x∈X k(x, y) ≥ τ (which clearly holds under our Parameterization 1.1). The algorithms are ran-
n
domized, and report correct answers with a constant probability. The values of p lie in the interval
[0, 1), and depend on the kernel. For comparison, note that a simple random sampling approach,
which selects a random subset R ⊂ X of size O(1/(τ ε2)) and reports n
x∈R k(x, y), achieves
|R|
the exponent of p = 1 for any kernel whose values lie in [0, 1].

(cid:80)

(cid:80)

We view our algorithms as parameterized in terms of τ , the smallest edge length. We argue this is
a natural parameterization. When picking a kernel function k, we also have to pick a scale term σ
(for example, the exponential kernel is of the form k(x, y) = exp(−(cid:107)x − y(cid:107)2/σ)). In practice, a
common choice of σ follows the so called ‘median rule’ where σ is set to be the median distance
among all pairs of points in X. Thus, according to the median rule, the ‘typical’ kernel values in the
graph K are Ω(1). While this is only true for ‘typical,’ and not all, edge weights in K, we believe the
KDE query abstraction of Deﬁnition 1.1 still provides nontrivial and useful algorithms for working
with kernel graphs. Typically in practice, the setting of τ is a small constant, independent of the size
of the dataset (Karppa et al., 2022).

We note that, in addition to the aforementioned algorithms with theoretical guarantees, there are
other practical algorithms based on random sampling, space partition trees (Gray & Moore, 2001;
2003; Lee et al., 2006; Lee & Gray, 2008; Morariu et al., 2008; Ram et al., 2009; March et al.,
2015), coresets (Phillips, 2013; Zheng et al., 2013; Phillips & Tai, 2020b), or combinations of these
methods (Karppa et al., 2022), which support queries needed in Deﬁnition 1.1; see (Karppa et al.,
2022) for an in-depth discussion on applied works.

While these algorithms do not necessarily have as strong theoretical guarantees as the ones discussed
above and in Table 1, we can nonetheless use them via black box access in our algorithms and utilize
their practical beneﬁts.

C Algorithmic Building Blocks

C.1 Multi-level KDE

We ﬁrst describe the “multi-level” KDE data structure, which is required in our algorithms. The data
structure recursively constructs a KDE data structure on the entire dataset X, and then recursively
partitions X into two halves, building a KDE data structure on each half. See Algorithm 1 for more
details.

17

Published as a conference paper at ICLR 2023

A1,n

A1,n/2

An/2+1,n

A1,n/4

. . .

. . .

. . .

Figure 1: Multi-level Kernel Density Estimation Data Structure.

Algorithm 1 Multi-level KDE Construction
Require: Input dataset X ⊂ Rd, precision ε > 0
1: T = X
2: while |T | > 1 do
3:
4:
5: end while
6: Return all the data structures associated with the KDE query constructions

Construct KDEX queries
Recursively apply Multi-level KDE Construction to T [1 : (cid:98)m/2(cid:99)] and T [(cid:98)m/2(cid:99) + 1 : m]

Lemma C.1. Given a dataset X ⊂ Rd, suppose the initialization of the KDE data structure deﬁned
in Deﬁnition 1.1 uses runtime f (n, ε) for some function linear in n. Then the total construction time
of Algorithm 1 is f (n log n, ε).

Proof. The proof follows from the fact that at each recursive level, we do O(f (n, ε)) total work
since f is linear in n and there are O(log n) levels.

C.2 Weighted Vertex Sampling

We now discuss our fundamental primitives. The ﬁrst one is sampling vertices by their (weighted)
degree.

Deﬁnition C.1 (Weighted Vertex Sampling). The weighted degree of a vertex xi with i ∈ [n] is
wi = (cid:80)
j(cid:54)=i k(xi, xj). The goal of weighted vertex sampling is to output a vertex v such that Pr[v =
xi] = (1±ε)wi
(cid:80)
j∈[n] wj

for all i ∈ [n].

This is a straightforward application of using n KDE queries to get the (weighted) vertex degree of
all n vertices. Note that this only takes n queries and only has to be done once. Therefore, we can
think of vertex sampling as a preprocessing step that uses O(n) queries upfront and then allows for
arbitrary access at any point in the future with no query cost.

Algorithm 2 Vertex Sampling by (Weighted) Degree

Require: Precision ε
Ensure: Reals pi such that (1 − ε)deg(xi) ≤ pi ≤ (1 + ε)deg(xi) for all 1 ≤ i ≤ n
1: for i = 1 to i = n do
2:
3: end for
4: Return {pi}n

pi ← KDEX (xi) − (1 − ε) k(xi, xi).

i=1

Once we acquire {pi}n
rithm, which we state in slightly more general terms.

i=1, we can perform a fast sampling procedure through the following algo-

18

Published as a conference paper at ICLR 2023

Algorithm 3 Sample from Positive Array
Require: Input array A = [a1, · · · , an] with ai > 0 for all i. Access to queries Ai,j = (cid:80)

i≤t≤j at

for 1 ≤ i ≤ j ≤ n.

a ← (cid:80)(T [1 : (cid:98)m/2(cid:99)]) // Can be simulated using an Ai,j query
b ← (cid:80)(T [(cid:98)m/2(cid:99) + 1 : m])
if Unif[0, 1] ≤ a/(a + b) then

1: T = A
2: while |T | > 1 do
3: m = len(T )
4:
5:
6:
7:
8:
9:
10:
11: end while
12: Return the single element in T

T ← T [(cid:98)m/2(cid:99) + 1 : m]

T ← T [1 : (cid:98)m/2(cid:99)]

end if

else

Combining Algorithms 2 and 3, we can perfectly sample from the degree distribution of the graph
K.

Algorithm 4 Faster Degree Sampling

Require: Reals pi such that (1 − ε)deg(xi) ≤ pi ≤ (1 + ε)deg(xi) for all 1 ≤ i ≤ n
1: i ← index in [n], which is the output of running Algorithm 3 on the array {pi}n
i=1
2: Return xi

We now analyze the correctness and the runtimes of the algorithms proposed in Section C. First, we
give guarantees on Algorithm 2.
Theorem C.2. Algorithm 2 returns {pi}n
1 ≤ i ≤ n.

i=1 such that (1 − ε)deg(xi) ≤ pi ≤ (1 + ε)deg(xi) for all

Proof. The proof follows by the Deﬁnition of a KDE query, Deﬁnition 1.1.

We now analyze Algorithm 3, which samples from an array based on a tree data structure given
access to consecutive sum queries. The analysis of this process will also greatly facilitate the analysis
of other algorithms from Section C.
Lemma C.3. Algorithm 3 samples an index i ∈ [n] proportional to ai in O(log n) time with
O(log n) queries.

Proof. Consider the sampling diagram given in Figure 1. Algorithm 3 does the following: it ﬁrst
queries the root node A1,n and then its two children A1,m, Am+1,n where m = (cid:98)n/2(cid:99). Note that
A1,n = A1,m + Am+1,n. It then picks the tree rooted at A1,m with probability
and oth-
erwise, picks the tree rooted at Am+1,n. The procedure recursively continues by querying the root
node, its two children, and picking one of its children to be the new root node with probability pro-
portional to the child’s weight given by an appropriate query access. This is done until we reach a
leaf node that corresponds to an index i ∈ [n].

i∈[m] ai
i∈[n] ai

(cid:80)
(cid:80)

We now prove correctness. Note that each node of the tree in Figure 1 corresponds to a subset
S ⊆ [n]. We prove inductively that the probability of landing on the vertex is equal to (cid:80)
i∈S ai.
This is true for the root node of the tree since the algorithm begins at the root note. Now consider
transitioning from some node S to one of its children S1, S2. We know that we are at node S with
probability (cid:80)
j∈S aj.
Therefore, the probability of being at S1 is equal to

j aj. Furthermore, we transition to S1 with probability (cid:80)

i∈S ai/ (cid:80)

ai/ (cid:80)

i∈S1

(cid:80)
(cid:80)

ai
i∈S1
j∈S aj

·

(cid:80)
i∈S ai
(cid:80)
j aj

=

19

(cid:80)

ai

.

i∈S1
(cid:80)
j aj

Published as a conference paper at ICLR 2023

Since there is only one path from the root node to any vertex of a tree, this completes the induction.

The runtime and the number of queries taken follows from the fact that the sampling procedure
descends on a tree with O(log n) height.

Combining Algorithms 2 and 3 allows us to sample from the degree distribution of the graph K up
to low error in total variation (TV) distance.
Theorem C.4. Algorithm 4 samples from the degree distribution of K up to TV error O(ε) using a
ﬁxed overhead of n KDE queries and runtime O(log n).

i=1, which proves the ﬁrst part of the theorem.

Proof. Since pi is with a 1±ε factor of deg(xi) for all i, then {pi}n
i=1 is O(ε) close in total variation
distance from the true degree distribution. Moreover, Algorithm 3 perfectly samples from the array
{pi}n
For the second part, note that acquiring {pi}n
i=1 requires n KDE queries. We can then construct the
data structure for Algorithm 3 by computing all the partial preﬁx sums in O(n) time. Now the query
access required by Algorithm 3 can be computed in O(1) time through an appropriate subtraction of
two preﬁx sums. Note that the previous steps need to be only done once and can be utilized for all
future runs of Algorithm 3. It follows from Lemma C.3 that Algorithm 4 takes O(log n) time.

C.3 Weighted Edge Sampling and Weighted Neighbor Edge Sampling

We describe how to perform weighted neighbor edge sampling.
Deﬁnition C.2 (Weighted Neighbor Edge Sampling). Given a vertex xi, the goal of weighted neigh-
bor edge sampling is to output a vertex v such that Pr[v = xk] = (1±ε)k(xi,xk)
j∈n,j(cid:54)=i k(xi,xj ) for all i ∈ [n].

(cid:80)

a ← a − (1 − ε(cid:48))k(xi, xi)

a ← KDET [1:m/2],ε(cid:48)(xi)
b ← KDET [m/2+1:m],ε(cid:48)(xi)
if xi ∈ T [1 : m/2] then

Algorithm 5 Sample Random Neighbor
Require: Input vertex xi ∈ X, precision ε
Ensure: x ∈ X \ {xi} such that the probability of selecting x is proportional to k(xi, x)
1: ε(cid:48) = ε/ log n
2: while |T | > 1 do
3: m ← |T |
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end while
18: Return the single element in T

end if
if xi ∈ T [m/2 + 1 : m] then
b ← b − (1 − ε(cid:48))k(xi, xi)

end if
if Unif[0, 1] ≤ a/(a + b) then

T ← T [m/2 + 1 : m]

T ← T [1 : m/2]

end if

else

We now prove the correctness of Algorithm 5 based on the ideas in Lemma C.3. Note that Algorithm
5 takes in input a precision level ε, which can be adjusted and impacts the accuracy of KDE queries.
We will discuss the cost of initializing KDE queries with various precisions in Section B.1.
Theorem C.5. Let xi ∈ X be an input vertex. Consider the distribution D over X \ {xi}, the neigh-
bors of xi in the graph K, induced by the edge weights in K. Algorithm 5 samples a neighbor from
a distribution that is within TV distance O(ε) from D using O(log n) KDE queries and O(log n)
time. In addition, we can perfectly sample from D using O(log n/τ ) additional kernel evaluations
in expectation.

20

Published as a conference paper at ICLR 2023

Proof. The proof idea is similar to that of Lemma C.3. Given a vertex xi, its adjacent edges have
associated weights and our goal is to sample an edge proportion to these weights. However, unlike
the degree case, performing edge sampling is not a straightforward KDE query as an edge only
cares about the kernel value between two points, rather than the sum of kernel values that a KDE
query provides. Nevertheless, we can utilize the tree procedure outline in the proof of Lemma C.3
in conjunction with KDE queries with over various subsets of X.

Imagine the same tree as in Figure 1 where each subset corresponds to a subset of neighbors of
xi (note that xi cannot be its own neighbor and hence we subtract k(xi, xi) in line 7 or line 10).
Algorithm 5 descends down the tree using the same probabilistic procedure as in the proof of Lemma
C.3: at every node, it picks one of the children to descend to with probability proportional to its
weight. Here, the weight of a child node in the tree in Figure 1 is the sum of the weights of the edges
connecting to the corresponding neighbors of xi.

Now compare the telescoping product of probabilities that lands us in some leaf node aj to the
ideal telescoping product if we knew the exact array of edge weights as in the proof of Lemma 6.
Suppose the tree has height (cid:96). At each node in our actual path descending down the tree, we take the
next step according to the ideal descent (according to the ideal telescoping product), with the same
probability, except for possibly an overestimate or underestimate by a factor of 1 + ε(cid:48) or 1 − ε(cid:48) factor
respectively.

Therefore, we land in the correct leaf node with the same probability as in the ideal telescoping
product, except our probability can be off by a multiplicative (1 ± ε(cid:48))(cid:96) factor. However, since ε(cid:48) =
ε/ log n and (cid:96) ≤ log n, this factor is within 1 ± ε. Thus, we sample from the correct distribution over
the leaves of the trees in Figure 1 up to TV distance O(ε). Now by doing O(1/τ ) steps of rejection
sampling, we can actually get a prefect sample of the edge. This is because the denominator of the
fraction for Pr[v = xk] is at least Ω(nτ ) and at most n so we can estimate the proportionality
constant in the denominator by n which is only at most O(1/τ ) multiplicative factor larger. Hence
by standard guarantees of rejection sampling, we only need repeat the sampling procedure O(1/τ )
additional times.

Algorithm 6 Sample Random Edge by Weight

Ensure: Sample edge (xi, xj) with probability at least (1 − ε)k(xi, xj)
1: v ← random vertex by using Algorithm 4
2: w ← random Neighbor of v using Algorithm 5.
3: Return: Edge (v, w).

Theorem C.6 (Weighted Edge Sampling). Algorithm 6 returns a random edge of K with probability
proportional to at least (1 − ε) its weight using 1 call to Algorithm 5.

(cid:80)

x∈X\u k(u,x) = (1 − 2ε) k(u,v)

Proof. Consider an edge (u, v). The vertex u is sampled with probability at
least (1 −
2ε)deg(u)/ (cid:80)
x∈X deg(x). Given this, the vertex v is then sampled with probability at least
k(u,v)
(1 − 2ε)
deg(u) . Using the same analysis for sampling v and then u,
we have that any edge (u, v) is sampled with probability at least 1 − 2ε times k(u, v)/ (cid:80)
e∈K w(e).
Note that the same rejection sampling remark as in the proof of Theorem C.5 applies and we can
perfectly sample an edge proportional to its weight with an addition O(1/τ ) rejection sampling
steps.

C.4 Random Walk

Theorem C.7. Algorithm 7 outputs a vertex from a vertex within O(T ε) total variation distance
from the true random walk distribution. Each step of the walk requires 1 call to Algorithm 5 .

Proof. The proof follows from the correctness of Algorithm 5 given in Theorem C.5. Lastly we
again note that by performing an additional O(1/τ ) rounds of rejection sampling steps (as outlined
in the end of the proof of Theorem C.5), we can make sure that we are sampling from the true
random walk distribution at each step of the walk.

21

Published as a conference paper at ICLR 2023

Algorithm 7 Perform Random Walk
Require: Input vertex xi ∈ X, length of walk T ≥ 1.
1: Start at vertex xi
2: v ← xi
3: for j = 1 to T do
4:
5:
6: end for
7: Return v

w ← Sample random neighbor of v (Algorithm 5)
v ← w

D Linear Algebra Applications

We now present a wide array of applications of the algorithmic building blocks constructed in Sec-
tion C. Altogether, these applications allow us to understand or approximate fundamental and prop-
erties of the kernel matrix and the graph K. In this section we present the linear algebra applications
and the graph applications are given in Section E.

D.1 Spectral Sparsiﬁcation

Algorithm 8 Spectral Sparsiﬁcation of the Kernel Graph
1: Let t = O(n log(n)/ε2τ 3) be the number of edges that are to be sampled
2: Let ˆp denote the distribution returned by Algorithm 2 for a small enough constant ε
3: Initialize G(cid:48) = ∅
4: for i = 1, . . . , t do
5:
6:
7:
8:
9:
10:
11: end for
12: Compute an ε/2 spectral sparsiﬁer G(cid:48)(cid:48) of graph G(cid:48) using (Batson et al., 2013)
13: return G(cid:48)(cid:48)

Sample a vertex u from the distribution ˆp
Sample a neighbor v of u using Algorithm 5 with constant ε
Let ˆquv be the probability that Algorithm 5 samples v given u as input
Similarly deﬁne and compute ˆqvu
wuv = 1/(t(ˆpu ˆquv + ˆpv ˆqvu))
Add the weighted edge ({u, v}, wuv) to the graph G(cid:48)

Given a set X, |X| = n, and a kernel k : X × X → R+, we describe how to construct a spectral
sparsiﬁer for the weighted complete graph on X where weight of the edge {xi, xj} is given by
k(xi, xj).
Deﬁnition D.1 (Graph Laplacian). Given a weighted graph G = (V, E, w), the Laplacian of G,
denoted by LG = D − A, where A is the adjacency matrix of G with Ai,j = w({i, j}) and D is a
diagonal matrix such that for all i ∈ [n], Di,i = (cid:80)
Theorem D.1 (Spectral Sparsiﬁcation of Kernel Density Graphs). Given a dataset X of n points in
Rd, and a kernel k : X × X → R+, let G = (X, (cid:0)X
(cid:1), w) be the weighted complete graph on X
with the weights w({xi, xj}) = k(xi, xj). Further, for all xi, xj ∈ X, let k(xi, xj) ≥ τ , for some
τ ∈ (0, 1). Let LG be the Laplacian matrix corresponding to the graph G. Then, for any ε ∈ (0, 1),
Algorithm 8 outputs a graph G(cid:48)(cid:48) with only m = O(n log n/(ε2τ 3)) edges, such that with probability
at least 9/10,

j(cid:54)=i Ai,j.

2

The algorithm makes (cid:101)O(m/τ 3) KDE queries and requires ˜O(md/τ 3) post-processing time.

(1 − ε)LG (cid:22) LG(cid:48) (cid:22) (1 + ε)LG.

Let Gd be the weighted directed graph obtained by arbitrarily orienting the edges of the graph G
and let H be an edge-vertex incidence matrix deﬁned as follows : for each e = (xi, xj) in graph Gd,
let He,xi = (cid:112)k(xi, xj) and He,xj = −(cid:112)k(xi, xj). Note that H (cid:62)H = LG. Our idea to construct

22

Published as a conference paper at ICLR 2023

spectral sparsiﬁer is to compute a sampling-and-reweighting matrix S, i.e., a matrix that has at most
one nonzero entry in each row, that with probability ≥ 9/10, satisﬁes

(1 − ε)LG = (1 − ε)H (cid:62)H (cid:22) H (cid:62)S(cid:62)SH (cid:22) (1 + ε)H (cid:62)H = (1 + ε)LG.
The edges sampled by S form the edges of the graph G(cid:48). We construct this matrix S by sampling
rows of the matrix H from a distribution close to the distribution that samples a row of H with a
probability proportional to its squared norm. We show that this gives a spectral sparsiﬁer by showing
that such a distribution approximates the “leverage score sampling” distribution.
Deﬁnition D.2 (Leverage Scores). Let M be a n × d matrix and mi denote the i-th row of M . Then,
for all i ∈ [n], τi, the i-th leverage of M is deﬁned as follows:
τi = mi(M (cid:62)M )+m(cid:62)
i ,

where X + is the Moore-Penrose pseudoinverse for a matrix X.

We introduce the following intermediate lemmas. We begin by recalling that sampling edges propor-
tional to leverage scores (effective resistances on a graph) sufﬁces to obtain spectral sparsiﬁcation
(Spielman & Srivastava, 2011; Woodruff, 2014).
Lemma D.2 (Leverage Score Sampling implies Sparsiﬁcation). Given an n × d matrix M and
ε ∈ (0, 1), for all i ∈ [t], let τi be the i-th leverage score of M . Let p = {p1, p2, . . . , pn} be a
distribution over the rows of M such that pi = τi/ (cid:80)
j∈[n] τj. Further, for some φ ∈ (0, 1), let
(cid:16) d log(d)
. Let S ∈ Rt×n
ˆp = {ˆp1, ˆp2, . . . , ˆpn} be a distribution such that ˆpi ≥ φpi and let t = O
ε2φ
tˆpi)e(cid:62)
be a random matrix where for all j ∈ [t], the j-th row is independently chosen as (1/
i with
probability ˆpi. Then, with probability at least 99/100,

√

(cid:17)

(1 − ε)M (cid:62)M (cid:22) M (cid:62)S(cid:62)SM (cid:22) (1 + ε)M (cid:62)M.

Next, we show that the matrix H is well-conditioned, in fact the condition number is independent of
the dimension and only depends on the minimum kernel value between any two points in the dataset.
This lets us use our edge sampling routines to compute an ε spectral sparsiﬁer.
Lemma D.3 (Bounding Condition Number). Let H be the edge-vertex incidence matrix as deﬁned
and also has the property that all nonzero entries in the matrix have an absolute value of at most 1
τ . Let σmax(H) be the maximum singular value of H and σmin(H) be the minimum
and at least
nonzero singular value of H. Then σmax(H)/σmin(H) ≤ 4

2/τ 1.5.

√

√

Proof. We use the following standard upper bound on the spectral norm of an arbitrary matrix A to
upper bound the spectral norm of the matrix H:

(cid:107)A(cid:107)2 ≤



(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

max
i



(cid:32)

|Ai,j|



(cid:88)

j

max
j

(cid:88)

i

(cid:33)
.

|Ai,j|

√

For the matrix H, as each column has at most n nonzero entries and each row has at most 2 non-
zero entries and from the assumption that all the entries have magnitude at most 1, we obtain that
(cid:107)H(cid:107)2 ≤
2n. To obtain lower bounds on σmin(H), we appeal to a Cheeger-type inequality for
weighted graphs from (Friedland, 1992; Friedland & Nabben, 2002). First, we note that σmin(H) =
(cid:112)σmin(H (cid:62)H) = (cid:112)σmin(LG) where G is the kernel graph that we are considering with each edge
having a weight of at least τ . Let 0 = λ1 ≤ λ2 ≤ · · · ≤ λn be the eigenvalues of the positive
semi-deﬁnite matrix LG. Now we have that

σmin(LG) = λ2(LG) ≥ min

(δi/2)ε(G)2

i

where δi = (cid:80)

j(cid:54)=i k(xi, xj) i.e., the weighted degree of vertex xi in graph G and

min
φ(cid:54)=U ⊂V,|U |≤n/2
where |E(U )| denotes the sum of weighted degrees of vertices in U and |E(U, ¯U )| denotes the
total weight of edges with one end point in U and the other outside U . Using the fact that G is a

ε(G) =

|E(U, ¯U )|
|E(U )|

23

Published as a conference paper at ICLR 2023

complete graph with each edge having a weight of at least τ and at most 1, we obtain |E(U, ¯U )| ≥
τ |U || ¯U | and |E(U )| ≤ n|U |, which implies that ε(G) ≥ minφ(cid:54)=U ⊂V,|U |≤n/2 τ | ¯U |/n ≥ τ /2. We
also similarly have that mini δi ≥ (n − 1)τ , which overall implies that λ2(LG) ≥ nτ 3/16 and that
σmin(H) ≥

nτ 1.5/4. Thus, we obtain that σmax(H)/σmin(H) ≤ 4

2/τ 1.5.

√

√

We are now ready to complete the proof of our main theorem:

Proof of Theorem D.1. Let q = {q1, q2, . . . , q(n
all edges e = {i, j}, qe ≥ c (cid:107)He,∗(cid:107)2
(cid:107)H(cid:107)2
F

=

(cid:80)

2

2)} be a distribution over the rows of H such that for

k(xi,xj )

e(cid:48)={i(cid:48),j(cid:48) } k(xi(cid:48) ,xj(cid:48) ) , for a ﬁxed universal constant c.

Next, we show that this distribution is Θ(1/κ2) approximation to the leverage score distribution for
H. Let H = U ΣV (cid:62) be the “thin” singular value decomposition of H and therefore all the diagonal
entries of Σ are nonzero. By deﬁnition τi = (cid:107)Ui∗(cid:107)2

2. We have

(cid:107)hi(cid:107)2

2 = (cid:107)Ui∗ΣV (cid:62)(cid:107)2

2 = (cid:107)Ui∗Σ(cid:107)2
2

where the equality follows from the fact that V (cid:62) has orthonormal rows. Now, (cid:107)Ui∗Σ(cid:107)2
(cid:107)Ui∗(cid:107)2
2σ2
we have

2 ≥
(cid:1), deﬁning κ = σmin/σmax,

min and (cid:107)Ui∗Σ(cid:107)2

2 ≤ (cid:107)Ui∗(cid:107)2

2σ2

max. Therefore, for all i ∈ (cid:0)n
(cid:107)hi(cid:107)2
2/σ2
j (cid:107)hj(cid:107)2

max
2/σ2

1
κ2

(cid:80)

min

≥

=

2

(cid:107)Ui∗(cid:107)2
2
j (cid:107)Uj∗(cid:107)2
2

(cid:107)hi(cid:107)2
2
(cid:107)H(cid:107)2
F

.

τi
(cid:80)
j τj

=

(cid:80)

Then, we invoke Lemma D.2 with φ = Ω(1/κ2) and conclude that sampling t = O
rows
of H results in a sparse graph G(cid:48) with corresponding Laplacian LG(cid:48) such that with probability at
least 99/100,

(cid:16) n log n
ε2κ2

(cid:17)

(1 − ε/2)LG (cid:22) LG(cid:48) (cid:22) (1 + ε/2)LG.

Further, by Lemma D.3, we can conclude κ2 ≤ 32/τ 3 and thus sampling t = O
sufﬁces.

(cid:16) n log n
ε2τ 3

(cid:17)

edges

We do not use Algorithm 6 to sample random edges from the perfect distribution to implement
spectral sparsiﬁcation as we cannot compute the exact sampling probability of the edge that is
sampled. So, we ﬁrst use Algorithm 4 with constant ε (say 1/2) to sample a vertex u and Algo-
rithm 5 with constant ε (say 1/2) to sample a neighbor v of u. Note that Algorithms 4 and Algo-
rithms 5 can be modiﬁed to also return the probabilities ˆpu and ˆqvu with which the vertex i and
the neighbor j of i are sampled. We can further query the algorithms to return ˆpv and ˆquv. Now,
q{u,v} = ˆpu ˆqvu + ˆpv ˆquv is the probability with which this sampling process samples the edge {u, v}
k(xu,xv)
i(cid:54)=j k(xi,xj ) and we use this distribution q to implement spec-
and we have that ˆpu ˆqvu + ˆpv ˆquv ≥ c
tral sparsiﬁcation as described above. As already seen (Theorem C.5), to compute vertex sampling
distribution ˆp, we use n KDE queries and for each neighbor sampling step, we use O(log n) KDE
queries. Thus, we overall use O(n log2 n/(ε2τ 3)) constant approximate KDE queries to obtain an ε
spectral sparsiﬁer.

(cid:80)

We can further compute another graph G(cid:48)(cid:48) with only O(n/ε2) edges by computing an ε/2 spectral
sparsiﬁer for G(cid:48) using the deterministic spectral sparsiﬁcation algorithm of (Batson et al., 2013).
Conditioning on the graph G(cid:48) being an ε spectral sparsiﬁer of G, the graph G(cid:48)(cid:48) will then be an ε
sparsiﬁer for G. This procedure doesn’t require any KDE queries and solely operates on the weighted
graph G(cid:48).

Hardness for spectral sparsiﬁcation. We observe that we can use the lower bound from Alman
et. al. to establish hardness in terms of τ from Parameterization 1.1. The lower bound we obtain is
as follows:
Theorem D.4 (Lower Bound for Spectral Sparsiﬁcation under Parameterization 1.1). Let k be the
Gaussian kernel and let X be dataset such that minx,y∈X k(x, y) = τ , for some 1 > τ > 0. Then,
any algorithm that with probability 9/10 outputs an O(1)-approximate spectral sparsiﬁer for the

24

Published as a conference paper at ICLR 2023

kernel graph associated with X, with O(n2−δ) edges, where δ < 0.01 is a ﬁxed universal constant,
requires Ω

time, assuming the strong exponential time hypothesis.

n · 2log(1/τ )0.32(cid:17)

(cid:16)

First, we begin with the deﬁnition of a multiplicatively-Lipschitz function:
Deﬁnition D.3 (Multiplicatively-Lipschitz Kernels). A kernel k over a set X is (c, L)-
multiplicatively Lipschitz if for any ρ ∈ (1/c, c), and for any x, y ∈ X, c−Lk(x, y) ≤ k(ρx, ρy) ≤
cLk(x, y).

We will require the following theorem showing hardness for sparsiﬁcation when the kernel function
is not multiplicatively-Lipschitz:
Theorem D.5 (Theorem 8.3 (Alman et al., 2020)). Let k be a function and X be a dataset such that
/L.
k is not (c, L)-multiplicatively-Lipschitz on X for some L > 1 and c = 1 + 2 log
Then, there is no algorithm that returns a sparsiﬁer of the kernel graph associated with X with
O(n2−δ) edges, where δ < 0.01 is a ﬁxed universal constant, in less than O
time,
assuming the strong exponential time hypothesis.

n · 2L0.48(cid:17)
(cid:16)

10 · 2L0.48 (cid:17)

(cid:16)

Proof of Theorem D.4 . First, we show that for any c > 1, if L < log(1/τ )(c−1), then the Gaussian
kernel k is not (c, L)-multiplicatively Lipschitz. Let z = (cid:107)x − y(cid:107)2
2 and let f (z) = e−z. Observe, it
sufﬁces to show that there exists a z such that f (cz) ≤ c−Lf (z). Let z be such that f (z) = ez =
minx,y k(x, y) = τ , i.e. z = log(1/τ ). Then,

and for L < log(1/τ )(c − 1)

f (c log(1/τ )) = e−c log(1/τ ),

c−Lf (log(1/τ )) > e−c log(1/τ ).

Then, applying Theorem D.5 with c = 1+ 1√
L
Lipschitz when L < log2/3(1/τ ), which concludes the proof.

, it sufﬁces to conclude k is not (c, L)-multiplicatively

D.1.1 Solving Laplacian Systems Approximately

We describe how to approximately solve the Laplacian system LGx = b using the spectral sparsiﬁer
LG(cid:48). First, we note the following theorem that states the running time and approximation guarantees
of fast Laplacian solvers.
Theorem D.6 ((Koutis et al., 2011), (Spielman & Teng, 2004)). There is an algorithm that takes an
input a graph Laplacian L of a graph with m weighted edges, a vector b, and an error parameter α
and returns x such that with probability at least 99/100,

where (cid:107)x(cid:107)L =

√

x(cid:62)Lx. The algorithm runs in time (cid:101)O(m log(1/α)).

(cid:107)x − L+b(cid:107)L ≤ α(cid:107)L+b(cid:107)L,

We have the following theorem that bounds the difference between solutions for the exact Laplacian
system and the spectral sparsiﬁer Laplacian.
Theorem D.7. Let LG be the Laplacian of a connected graph G on n vertices and let LG(cid:48) be the
Laplacian of an ε-spectral sparsiﬁer G(cid:48) of graph G i.e.,

(1 − ε)LG (cid:22) LG(cid:48) (cid:22) (1 + ε)LG,

for ε < c for a small enough constant c. Then, for any vector b with 1(cid:62)b = 0, (cid:107)L+
√
2

ε(cid:107)L+

Gb(cid:107)LG .

Gb − L+

G(cid:48)b(cid:107)LG ≤

Proof. Note that for ε < 1, the graph G(cid:48) also has to be connected and therefore the only eigen
vectors corresponding to eigen value 0 of the matrices LG and LG(cid:48) are of the form a · 1 for a (cid:54)= 0

25

Published as a conference paper at ICLR 2023

and hence columns (and rows) of LG span all vectors orthogonal to 1. Therefore LGL+
I − (1/n)11(cid:62). Now,

G = L+

GLG =

(cid:107)L+

Gb − L+

G(cid:48)b(cid:107)2
LG

= b(cid:62)(L+
= b(cid:62)L+

≤ b(cid:62)L+

G(cid:48))LG(L+
Gb − b(cid:62)L+

G − L+
GLGL+
Gb − b(cid:62)L+

G − L+
G(cid:48))b
G(cid:48)LGL+
Gb − b(cid:62)L+
1
1 − ε

G(cid:48)b +

G(cid:48)b − b(cid:62)L+

G(cid:48)LGL+
b(cid:62)L+

G(cid:48)b

Gb + b(cid:62)L+

G(cid:48)LGL+

G(cid:48)b

where in the last inequality, we used LGL+
As the null spaces of both LG and LG(cid:48) are given by {a1 | a ∈ R}, we also obtain that

Gb = 1 and that for any vector v, v(cid:62)LGv ≤ 1

1−ε v(cid:62)LG(cid:48)v.

(1 − ε)L+

G (cid:22) L+

G(cid:48) (cid:22) (1 + ε)L+

G

using which we further obtain that

(cid:107)L+

Gb − L+

G(cid:48)b(cid:107)2
LG

≤

(cid:18) 2

1 − ε

(cid:19)

− 2

b(cid:62)L+

G(cid:48)b ≤

2ε(1 + ε)
1 − ε

b(cid:62)L+

Gb ≤ 4ε(cid:107)L+

Gb(cid:107)2
LG

.

Thus, (cid:107)L+

Gb − L+

G(cid:48)b(cid:107)LG ≤ 2

√

ε(cid:107)L+

Gb(cid:107)LG.

Therefore, if x is a vector such that (cid:107)x−LG(cid:48)b(cid:107)LG(cid:48) ≤ α(cid:107)L+
solver, then

G(cid:48)b(cid:107)LG(cid:48) obtained using the fast Laplacian

(cid:107)x − L+

Gb(cid:107)2
LG

G(cid:48)b − L+
+ (cid:107)L+

= (cid:107)x − L+
≤ 2((cid:107)x − L+
2
1 − ε

G(cid:48)b + L+
G(cid:48)b(cid:107)2
LG
(cid:107)x − L+
G(cid:48)b(cid:107)2
L(cid:48)
G

Gb(cid:107)2
LG
G(cid:48)b − L+
+ 4ε(cid:107)L+
Gb(cid:107)2
LG

≤

.

Gb(cid:107)2
LG

)

Here we used the above theorem and the fact that LG (cid:22) (1/(1 − ε))LG(cid:48). Now, (cid:107)x − L+
α2(cid:107)L+
which ﬁnally implies that

G(cid:48)b(cid:107)2
Gb ≤ (1 + ε)(cid:107)L+

G(cid:48)b ≤ (1 + ε)b(cid:62)L+

LG(cid:48) = b(cid:62)L+

LG(cid:48) and (cid:107)L+

G(cid:48)b = b(cid:62)L+

G(cid:48)LG(cid:48)L+

G(cid:48)b(cid:107)2

G(cid:48)b(cid:107)2

LG(cid:48) ≤
Gb(cid:107)2
,
LG

(cid:107)x − L+

Gb(cid:107)2
LG

≤

(cid:18) 2(1 + ε)2
1 − ε

(cid:19)

α2 + 4ε

(cid:107)L+

Gb(cid:107)2
LG

.

Thus, using a ε spectral sparsiﬁer G(cid:48) with m edges, we can in time (cid:101)O(m log(1/ε)) can obtain a
vector x such that (cid:107)x − L+

Gb(cid:107)LG for a large enough constant C.

Gb(cid:107)LG ≤ C

ε(cid:107)L+

√

D.2 Low-rank Approximation of the Kernel Matrix

We derive algorithms for low-rank approximations of the kernel matrix via KDE queries. We present
a algorithm for additive error approximation and compare to prior work for relative error approxi-
mation.

We ﬁrst recall the following two theorems. Let Ai,∗ denote the ith row of a matrix A.
Theorem D.8 ((Frieze et al., 2004)). Let A ∈ Rn×m be any matrix. Let S be a sample of O(k/ε)
2/(cid:107)A|2
rows according to a probability distribution (p1, . . . , pn) that satisﬁes pi ≥ Ω(1) · (cid:107)Ai,∗(cid:107)2
F
for every 1 ≤ i ≤ n. Then, in time O(mk/ε · poly(k, 1/ε)), we can compute from S a matrix
U ∈ Rk×m, that with probability at least 0.99 satisﬁes

(cid:107)A − AU T U (cid:107)2

F ≤ (cid:107)A − Ak(cid:107)2

F + ε(cid:107)A(cid:107)2
F .

Theorem D.9 ((Chen & Price, 2017), also see (Indyk et al., 2019)). There is a randomized algorithm
that given matrices A ∈ Rn×m and U ∈ Rk×m, reads only O(k/ε) columns of A, runs in time
O(mk) + poly(k, 1/ε), and returns V ∈ Rn×k that with probability 0.99 satisﬁes

(cid:107)A − V U (cid:107)2

F ≤ (1 + ε) min

X∈Rn×k

(cid:107)A − X(cid:107)2
F .

26

Published as a conference paper at ICLR 2023

Therefore to compute the low rank approximation, we just need sample from the distribution on rows
required by Theorem D.8. We reduce this question to evaluating KDE queries as follows: If K is
the kernel matrix, each row of K is the weight of the edges of the corresponding vertex. Therefore,
each pi in the distribution (p1, . . . , pn) is the sum of edge weights squared for vertex xi. From
vertex queries (Algorithm 4), we know that we can get the degree of each vertex, which is the sum
of edge weights. We can extend Algorithm 4 to sample from the sum of squared edge weights of
each vertex as follows. Consider a kernel k such that there exists an absolute constant c that satisﬁes
k(x, y)2 = k(cx, cy) for all x, y. Such a c exists for the most popular kernels such as the Laplacian,
exponential, and Gaussian kernels for which c = 2, 2, and 4 respectively. Thus give our dataset X,
we simply construct KDE queries for the dataset X (cid:48) := cX. Then by sampling the degrees of the
vertices associated with the kernel graph K (cid:48) of X (cid:48), we can sample from the distribution required by
Theorem D.8 by invoking Algorithm 4 on the dataset X (cid:48). In particular, using n KDE queries for X (cid:48),
we can get row norm squared values for all rows of our original kernel matrix K. We can then sample
the rows according to Theorem D.8 and fully construct the rows that are sampled. Altogether, this
takes n KDE queries and O(nk/ε) kernel function evaluations to construct a rank k approximation
of K; see Algorithm 9.
Corollary D.10. Given a dataset X of size n, there exists an algorithm that outputs a rank k matrix
B such that

F ≤ (cid:107)K − Kk(cid:107)2
with probability 99/100, where K is a kernel matrix associated with X based on a Laplacian,
exponential, or Gaussian kernel, and Kk is the optimal rank-k approximation of K. It uses n KDE
queries and O(nk/ε · poly(k, 1/ε) + nkd/ε) post-processing time.

F + ε(cid:107)K(cid:107)2
F

(cid:107)K − B(cid:107)2

We remark that for the application presented in this subsection, we can we can replace 1.1. Indeed,
since we only estimate row sums, we only require that the value of a KDE query is at least τ , that
is, the average value 1
x∈X k(x, y) ≥ τ for a query y. Note that via Cauchy Schwartz, this
|X|
automatically implies a lower bound for the average squared sum:

(cid:80)

1
|X|

(cid:88)

x∈X

k(x, y)2 ≥

1
|X|2

(cid:32)

(cid:88)

x∈X

(cid:33)2

k(x, y)

≥ τ 2.

Compute the value pi = (cid:80)n

Algorithm 9 Additive-error Low-rank Approximation
1: Let c be the constant such that k(x, y)2 = k(cx, cy) for all inputs x, y
2: for i = 1 to i = n do
3:
4: end for
5: Sample and construct O(k/ε) rows of K according to probability proportional to {pi}n
6: Compute U from the sample, using Theorem D.8
7: Compute V from the sample, using Theorem D.9
8: return Return V, U

j=1 k(cxi, cxj) using KDE queries for the dataset cX

i=1

D.3 Approximating the Spectrum in EMD

In this subsection, we obtain a sublinear time algorithm to approximate the spectrum of the nor-
malized Laplacian associated with the graph whose adjacency matrix is given by the kernel matrix
K.

The eigenvalues of the Laplacian capture fundamental combinatorial properties of the graph such as
community structures at varying scales. See the works (Lee et al., 2012; Louis et al., 2012; Kwok
et al., 2013; Czumaj et al., 2015; Gluch et al., 2021), which show that the jth eigenvalue of the
Laplacian informs us if the graph can be partitioned into j distinct clusters. However, computing
a large number of eigenvalues of the Laplacian may not be computationally feasible. Thus, it is
desirable to obtain a succinct summary of all eigenvalues, i.e. the spectrum.

Additionally, models of random graphs that aim to describe social or biological networks often
times have closed form descriptions of the spectrum for graphs drawn from the model. Borrowing

27

Published as a conference paper at ICLR 2023

an example from (Cohen-Steiner et al., 2018), “if the spectrum of random power-law graphs does not
closely resemble the spectrum of the Twitter graph, it suggests that a random power-law graph might
be a poor model for the Twitter graph.” Thus, another application of computing an approximation of
the spectrum of eigenvalues is to test the applicability of generative graph models.

Our notion of approximation deals with the Earth mover (EMD) distance.
Deﬁnition D.4 (Earth Mover Distance). Given two multi-sets of n points in Rd, denoted by A and
B, the earth-mover distance between A and B is deﬁned as the minimum cost of a perfect matching
between the two sets, i.e.

EMD(A, B) = min

π:A→B

(cid:88)

a∈A

(cid:107)a − π(a)(cid:107)2 ,

(D.1)

where π ranges over all one-to-one mappings.

We can now invoke the algorithm ApproxSpectralMoment of (Cohen-Steiner et al., 2018).
The algorithm ﬁrst selects uniformly random vertices of a weighted graph A. It then performs a
random walk of a speciﬁed length (cid:96) starting from the chosen vertex and then counts the number
of times the walk returns back to the original vertex. Now Theorem C.7 allows us to perform one
step of a random walk using O(log n) KDE queries. Note that we perform an additional ˜O(1/τ ) of
rejection sampling in Algorithm 5 to perfectly sample from the true neighbor distribution. Thus we
immediately have the following guarantee:
Theorem D.11 (Corollary of Theorem 1 in (Cohen-Steiner et al., 2018) and Theorem C.7). Given
a n × n kernel matrix K and accuracy parameter ε ∈ (0, 1), let G be the corresponding weighted
graph, and let LG = I − D−1KD−1 be the normalized Laplacian, where Di,i = (cid:80)
j Ki,j. Let
λ1 ≥ λ2 . . . ≥ λn be the eigenvalues of LG and let λ be the resulting vector. Then, there exists an
algorithm that uses (cid:101)O (cid:0)exp (cid:0)1/ε2(cid:1) /τ (cid:1) KDE queries and exp (cid:0)1/ε2(cid:1) · d/τ post-processing time and
outputs a vector (cid:101)λ such that with probability 99/100,
(cid:17)

(cid:16)

EMD

λ, (cid:101)λ

≤ ε.

We remark that the bound of exp (cid:0)1/ε2(cid:1) is independent of n, which is the size of the dataset.

D.4 First Eigenvalue and Eigenvector Approximation

Our goal is to approximate the top eigenvalue of the kernel matrix and ﬁnd a vector witnessing this
approximation. Our overall algorithm can be split into two steps: ﬁrst sample a random principal
submatrix of the kernel matrix. Under the condition that each row of the n × n kernel matrix K
satisﬁes that it’s sum is at least nτ , we can easily show that it must have a large ﬁrst eigenvalue and
thus prior works on sampling bounds automatically imply the ﬁrst eigenvalue of the sampled matrix
approximates that of K. The next step is to use a ‘noisy’ power method of (Backurs et al., 2021) on
the sampled submatrix. We note that this step employs a KDE data-structure initialized only on the
sampled indices of K. The algorithm and details follow.

Algorithm 10 First Eigenvalue and Eigenvector Approximation
Require: Input dataset X ⊂ Rd of size |X| = n, precision ε > 0
1: t ← O(1/(ε2τ 2)
2: S ← random subset of [n] of size t
3: KS ← principal submatrix of K on indices in S {Just for notation; we do not initializing K or

KS}

4: ˜K ← (n/s) · KS
5: Return the eigenvalue and eigenvector found by Algorithm 1 of (Backurs et al., 2021) (Kernel

Noisy Power Method) on KS.

We remark that the eigenvector returned by Algorithm 10 will be a sparse vector supported only on
the coordinates in S.

28

Published as a conference paper at ICLR 2023

We ﬁrst state the necessary auxiliary statements needed to prove the guarantees of Algorithm 10.

Lemma D.12. If each row of K satisﬁes that its sum is at least nτ for parameter τ ∈ (0, 1), then
the largest eigenvalue of K, denoted as λ1, satisﬁes λ1 ≥ nτ .

Proof. This follows from looking at the quadratic form 1T K1 where 1 is the vector with all entries
equal to 1:

λ1 ≥

1T K1
1T 1

≥

n2τ
n

= nτ.

We now state the guarantees of Algorithm 1 in (Backurs et al., 2021).

Theorem D.13 ((Backurs et al., 2021)). Suppose the kernel function for a m × m kernel matrix K
has a KDE data structure with query time d/(ε2τ p) (see Table 1). Then Algorithm 1 of (Backurs
et al., 2021) returns λ such that λ ≥ (1 − ε)λ1(K) in time O

(cid:16) dm1+p log(m/ε)2+p
ε7+4p

(cid:17)

Finally, we need the following result on eigenvalues of sampled PSD matrices, proven in (Bhat-
tacharjee et al., 2021).
Lemma D.14 ((Bhattacharjee et al., 2021)). Let A ∈ Rn×n be PSD with (cid:107)A(cid:107)∞ ≤ 1. Let S ⊂ [n]
be a random subset of size t and let AS×S be the submatrix restricted to columns and rows in S and
scaled by n/s. Then, for all i ∈ [|S|], λi (AS×S) = λi(A) ± n√
t

.

We are now ready to prove the guarantees of Algorithm 10.

Theorem D.15. Given a n × n kernel matrix K admitting a KDE data-structure with query time
d/(ε2τ p), Algorithm 10 returns λ such that λ ≥ (1 − ε)λ1(K) in total time

(cid:32)

min

O

(cid:18) d log(d/ε)
ε4.5τ 4

(cid:19)

, O

(cid:32)

d
ε9+6pτ 2+2p log

(cid:18) 1
ετ

(cid:19)2+p(cid:33)(cid:33)

.

Remark D.1. Two remarks are in order. First we recall that the runtime of (Backurs et al., 2021)
has a n1+p factor while our bound has no dependence on n and is thus a truly sublinear runtime.
Second, if we skip the Kernel Noisy Power method step and directly initialize and calculate the top
eigenvalue of KS (using the standard gap independent power method of (Musco & Musco, 2015)),
we would get a runtime of ˜O(d/(ε4.5τ 4)) which has a polynomially better ε dependence but a worse
τ dependence than the guarantees of Algorithm 10.

Proof of Theorem D.15. We ﬁrst prove the approximation guarantee. By our setting of t and using
Lemma D.14, we see that the additive error in approximating the ﬁrst eigenvalue of K by that of ˜K
is at most

n
√
t

≤ ετ n ≤ ελ1(K),

and thus λ1( ˜K) ≥ (1 − ε)λ1(K). Then by the guarantees of Theorem D.13, it follows that we ﬁnd
a 1 − ε multiplicative approximation to λ1( ˜K) and thus a 1 − O(ε) multiplicative approximation to
that of λ1(K).
We now prove the runtime bound. It easily follows from plugging in m = O(1/(ε2τ 2)) in Theorem
D.13.

E Graph Applications

In this section, we present our graph applications, including local clustering, spectral clustering,
arboricity estimation, and estimating the total weight of triangles.

29

Published as a conference paper at ICLR 2023

E.1 Local Clustering

Algorithm 11 Local k-Clustering

Require: Vertices u, w, random walk length t
1: For a given u, let pt
2: if (cid:96)2 distribution tester outputs (cid:107)pt
3:
4: end if
5: return u, w are in different clusters

return u, w are in the same cluster

u − pt

u be the endpoint distribution of a random walk of length t starting at v

w(cid:107)2 ≤ 1/(7n) then

We give a local clustering algorithm on graphs. The advantage of this method is that it is local as it
allows us to cluster one vertex at a time. This is especially useful in the setting of local clustering
where one might not wish to classify all vertices at once or only a small subset of vertices are of
interest.

We now present a deﬁnition for a clusterable graph that has been an extremely popular model def-
inition in the property testing and sublinear algorithms community (see (Kale & Seshadhri, 2008;
Czumaj & Sohler, 2010; Goldreich & Ron, 2011; Czumaj et al., 2015; Chiplunkar et al., 2018; Dey
et al., 2019; Gluch et al., 2021) and the references within).

First, we need to deﬁne the notion of conductance.

Deﬁnition E.1 (Conductance). Let G = (V, E, w) be a weighted graph. The conductance of a set
S ⊂ V is deﬁned as

φG(S) =

w(S, Sc)
min(w(S), w(Sc))

where w(S, Sc) denotes the sum of edge weights crossing the cut (S, Sc) and w(S) denotes the
sum of (weighted) degrees of vertices in S. The conductance of the graph G is then the minimum of
φG(S) over all sets S:

φ(G) = min

S

φG(S).

Deﬁnition E.2 (Inner/Outer Conductance). For a subset U ⊆ V , we deﬁne φ(G[U ]) to be the
conductance of the induced graph on U . φ(G[U ]) is also referred to as the inner conductance of U .
Conversely, φG(U ) is refereed to as the outer conductance of U .
Deﬁnition E.3 (k-clusterable Graph). A graph G is (k, φin, φout)-clusterable if the following holds:
There exists a partition of the vertex set into h ≤ k parts V = ∪1≤i≤hVi such that φ(G[Vi]) ≥ φin
and φG(Vi) ≤ φout.

Deﬁnition E.3 captures the intuition that one can partition the graph into h ≤ k pieces where each
piece has a strong cluster structure (captured by φin) and distinct pieces are separated by sparse cuts
(captured by φout). Note that we are interested in the regime where φout is smaller than φin. We will
also assume that each |Vi| ≥ n/ poly(k) where we allow for an arbitrary polynomial dependence
on k. This means that each cluster size is not too small.

Since we are interested in clustering, through this section, we will assume our kernel graph K is
k-clusterable according to Deﬁnition E.3 but we do not know what the partitions are.

The main algorithmic result of this section is that given a k-clusterable kernel graph and two vertices
u and w that are in parts V1 and V2 respectively of the graph (as deﬁned in Deﬁnition E.3), we can
efﬁciently test if V1 = V2 or V1 (cid:54)= V2. That is, we can efﬁciently test if u and w belong to the same
or distinct clusters. The underlying idea behind the algorithm is that if u and w belong to the same
cluster, then random walks starting from these vertices will rapidly mix inside the corresponding
cluster. Therefore, random walks in distinct clusters will be substantially different and can be de-
tected using distribution testing. Our algorithm is given in Algorithm 11. The ﬂavor of the algorithm
presented is quite standard in property testing literature, see (Czumaj et al., 2015) and (Peng, 2020).

The (cid:96)2 distribution tester we need is a standard result in distribution testing with the following
guarantees.

30

Published as a conference paper at ICLR 2023

Theorem E.1 (Theorem 1.2 in (Chan et al., 2014)). Let δ, ξ > 0 and let p, q be two discrete distri-
butions over a set of size n with b ≥ max{(cid:107)p(cid:107)2
2, (cid:107)q(cid:107)2
b log(1/δ)/ξ for an appropriate
constant c. There exists (cid:96)2 distribution tester that takes as input r samples from each distribution
p, q and accepts the distributions if (cid:107)p − q(cid:107)2
2 ≥ 4ξ with
probability at least 1 − δ. The running time of the tester is linear in its sample size.

2 ≤ ξ, and rejects the distributions if (cid:107)p − q(cid:107)2

2}. Let r ≥ c

√

We now prove the correctness of Algorithm 11. We note that many arguments from prior works are
re-derived in the proof below, rather than stating them in a black box manner, for completeness since
our setting is of weighted graphs and the usual setting in literature is unweighted or regular graphs.
We ﬁrst need the following lemmas. Recall that the random walk matrix of an arbitrary weighted
graph is given by M = AD−1 where A is the adjacency matrix and D is the diagonal degree matrix.
The normalized Laplacian matrix L is deﬁned as L = I − D−1/2AD−1/2.

Our ﬁrst result is that vertices in the same well connected cluster of G have a quantitative relationship
captured by the eigenvectors of L. This is in similar spirit to Lemma 5.3 of (Czumaj et al., 2015)
but we must show it holds for weighted graphs arising from kernel matrices whereas (Czumaj et al.,
2015) is interested in bounded degree unweighted graphs.
Lemma E.2. Let vi be the ith eigenvector of the normalized Laplacian of the kernel graph K and
let C be any subset such that φ(K[C]) ≥ φin. Then for any 1 ≤ i ≤ h, the following holds:

(cid:32)

(cid:88)

u,v∈C

vi(u)
(cid:112)w(u)

−

vi(v)
(cid:112)w(v)

(cid:33)2

(cid:46) φoutn
in|C|τ 2 .
φ2

Proof. By Lemma 5.2 in (Czumaj et al., 2015) and Theorem 1.2 in (Lee et al., 2012), we have that
in/h4 (cid:46) λh+1 and λi ≤ 2φout for any 1 ≤ i ≤ h. Now by the variational principle for eigenvalues
φ2
(Chung & Graham, 1997), we have
(cid:32)

(cid:33)2

λi =

(cid:88)

(u,v)

vi(u)
(cid:112)w(u)

−

vi(v)
(cid:112)w(v)

w(u, v) ≤ 2φout.

Now let H = K[C]. From (Chung & Graham, 1997) and our assumptions on C, we have that

volH (VH ) ·

2 · (cid:80)

(u,v)∈EH

(cid:18)

vi(u)√
w(u)

(cid:80)

u,v∈VH

(cid:18)

vi(u)√
w(u)

− vi(v)√
w(v)

− vi(v)√
w(v)
(cid:19)2

(cid:19)2

w(u, v)

dH (u)dH (v)

≥ λ2(H) ≥

φ2
in
2

,

where volH (VH ) denotes the sum of the degrees of vertices in H and dH (·) denotes the degree in
H. Note the last step is due to Cheeger’s inequality. Combining the preceding result with our earlier
derivation, we have
(cid:32)

(cid:33)2

(cid:33)2

(cid:32)

(cid:88)

vi(u)
(cid:112)w(u)

−

vi(v)
(cid:112)w(v)

(cid:88)

≤

(u,v)∈EK

vi(u)
(cid:112)w(u)

−

vi(v)
(cid:112)w(v)

≤ 2φout.

(u,v)∈EH

This implies that

|C|2τ 2 (cid:88)

(u,v)∈VH

(cid:32)

vi(u)
(cid:112)w(u)

−

vi(v)
(cid:112)w(v)

(cid:33)2

(cid:32)

(cid:88)

≤

(u,v)∈VH

(cid:33)2

vi(u)
(cid:112)w(u)

−

vi(v)
(cid:112)w(v)

dH (u)dH (v)

(cid:46) φoutvolH (VH )
φ2
in

where we have used the fact that all edge weights in K are at least τ . Using the fact that volH (VH ) ≤
|C|n, it follows that

(cid:88)

(u,v)∈VH

as desired.

(cid:33)2

(cid:46) φoutn
in|C|τ 2 ,
φ2

(cid:32)

vi(u)
(cid:112)w(u)

−

vi(v)
(cid:112)w(v)

31

Published as a conference paper at ICLR 2023

The second result states that vertices in the same well-connected cluster have similar random walk
distributions. This is again the analogue of Lemma 4.2 in (Czumaj et al., 2015) but we must show it
holds for weighted graphs.
Lemma E.3. Let 0 < β < 1/2. If graph K is (k, φin, φout)-clusterable, and C ⊆ V is any
subset such that |C| ≥ n/ poly(k) and φ(K[C]) ≥ φin. There exists a constant c = c(β) > 0 and
c(cid:48) = c(cid:48)(β, k) such that for any t ≥ c log n/φ2
in, there exists a subset (cid:101)C ⊆ C satisfying
vol( (cid:101)C) ≥ (1 − β)vol(C) such that for any u, v ∈ (cid:101)C, the following holds:

in, φout ≤ c(cid:48)φ2

(cid:107)pt

u − pt

v(cid:107)2

2 ≤

1
8n

.

Proof. Let v1, · · · , vn denote the eigenvectors of L with eigenvalues λ1, · · · , λn in non-decreasing
order. We know that the eigenvalues of M are given by 1 − λi with corresponding eigenvalues
yi = D1/2vi. The vector pt
u is the vector 1u with a one value in the uth coordinate applied to M t.
Write

(cid:88)

1u =

αiyi =

(cid:88)

αiD−1/2vi.

Taking the innerproduct of 1u with D−1/2vi tells us that αi = vi(u)/(cid:112)w(u). Thus,

i

i

u − pt
pt

v =

n
(cid:88)

i=1

vi(u)
(cid:112)w(u)

(1 − λi)tyi −

n
(cid:88)

i=1

= D1/2

(cid:32)

n
(cid:88)

i=1

vi

vi(u)
(cid:112)w(u)

−

This means that

(1 − λi)tyi

vi(v)
(cid:112)w(v)
(cid:33)

vi(v)
(cid:112)w(v)

(1 − λi)t.

(cid:107)pt

u − pt

v(cid:107)2 ≤ (cid:107)D1/2(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:32)

vi

vi(u)
(cid:112)w(u)

−

vi(v)
(cid:112)w(v)

(cid:33)

(1 − λi)t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

.

Since the vi’s are orthogonal, we know that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:32)

vi

vi(u)
(cid:112)w(u)

−

vi(v)
(cid:112)w(v)

(cid:33)

(1 − λi)t

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

(cid:32)

n
(cid:88)

≤

i=1

vi(u)
(cid:112)w(u)

−

vi(v)
(cid:112)w(v)

(cid:33)2

(1 − λi)2t.

Now the rest of the proof follows from Lemma 4.2 in (Czumaj et al., 2015). In particular, it tells
us that in the above summation, each of the terms for 1 ≤ i ≤ h can be bounded by (cid:46) φoutn
β|C|3φ2
in
whereas the rest of the sum can be bounded by 1/ poly(n) for sufﬁciently large poly(n) by adjusting
the constant in front of t. Our choice for |C| ≥ n/ poly(k) imply that the overall sum is bounded
by (cid:46) φout poly(k)
2 ≤ 1/(8n), as
desired.

. Since (cid:107)D1/2(cid:107)2 ≤ n, and φout ≤ c(cid:48)φ2

in, we have that (cid:107)pt

u − pt

βn2φ2
in

v(cid:107)2

Our next goal is to show that vertices from different well-connected partitions have very different
random walk endpoint distributions. The argument we borrow is from (Czumaj & Sohler, 2010).
Lemma E.4. Let G be a (k, φin, φout)-clusterable graph with parts V = ∪1≤i≤hVi. There exists a
constant c > 0 such that if t·φout ≤ cε, then there exists a subset V (cid:48)
1 ) ≥ (1−
ε)vol(V1) such that a t-step random walk from any vertex in V (cid:48)
1 does not leave V1 with probability
1 − ε.

1 ⊆ V1 satisfying vol(V (cid:48)

Proof. We ﬁrst bound the probability that the random walks always stay in their respective clusters.
Consider a ﬁxed partition V1; the same arguments apply for any partition. Let G(cid:48) be the graph with
the same vertex set as K but with only the following edges: edges among vertices in V1 and edges
from vertices in V1 to V \ V1. Consider a random walk on G(cid:48) of length t with the initial vertex u(cid:48)
chosen from the stationary distribution of G(cid:48), i.e., the distribution that chooses each vertex in G(cid:48) with
probability proportional to its weight. Let Yi denote the indicator random variable for the event that

32

Published as a conference paper at ICLR 2023

the ith vertex of the random walk is in V \ V1. Since we are simulating the stationary distribution,
we have that

Pr[Yi = 1] =

w(V1, V \ V1)
w(G(cid:48))

where w is the weight of edges in the original graph K. By linearity of expectations, the number of
vertices that land in V \ V1 is

(cid:35)

(cid:34) t

(cid:88)

E

i=1

Yi

= (t + 1)

w(V1, V \ V1)
w(G(cid:48))

(cid:46) tφout

due to our requirement of φout. Therefore by Markov’s inequality, the probability that any vertex in
V \ V1 is ever visited is (cid:46) tφout.
We now move our random walk back to the original graph K. The preceding calculation implies
that the probability that an t step random walk in K starting at a vertex chosen at random from
V1 according to the stationary distribution will remain in V1 with probability at least 1 − φoutt. If
φout (cid:46) ε/t, then we know that the random walk stays in V1 with probability at least 1 − O(ε) so
there must be a set of vertices V (cid:48) ⊆ V1 of at least 1 − O(ε) fraction of the total volume of V1 such
that a random walk starting from a vertex in V (cid:48) remains in V1 with probability at least 1 − O(ε).

We can now prove the correctness of Algorithm 11.
Theorem E.5. Let K be a (k, φin, φout)-clusterable kernel graph with parts V = ∪1≤i≤hVi. Let
U, W be one of (not necessarily distinct) partitions Vi. Let u, w be randomly chosen vertices in
partitions U and W with probability proportional to their degrees. There exists c = c(ε, k) such
that if φout ≤ cφ2
in/ log n, then with probability at least 1 − ε, if U = W then Algorithm 11 returns
that u and w are in the same cluster and if U (cid:54)= W , Algorithm 11 returns that u and w are in different
clusters. The algorithm requires O((cid:112)nk/(ετ ) log(1/ε)) random walks of length t ≥ c log n/φ2
in.

Proof. We ﬁrst consider the case that U (cid:54)= W . From Lemma E.4, we know that there are ‘non-
escaping’ subsets U (cid:48) and W (cid:48) of U and W respectively such that vertices u, w from U (cid:48) and W (cid:48)
respectively don’t leave U and W with probability 1 − ε. Conditioning on u and w being in those
subsets, we have that with probability 1 − O(ε), pt
v will be disjointly supported and thus,
(cid:107)pt

u and pt

v(cid:107)2

2 = (cid:107)pt

u(cid:107)2

2 + (cid:107)pt

2 ≥ 2/n.

u − pt

v(cid:107)2

Now if U = W , we know from Lemma E.3 that (cid:107)pt
coming from the large volume subset of U .

u − pt

v(cid:107)2

2 ≤ 1/(8n) if we condition on u and v

Finally, we need one last ingredient. Lemma 4.3 in (Czumaj et al., 2015) readily implies that there
exists a V (cid:48) ⊆ V satisfying vol(V (cid:48)) ≥ (1 − ε)vol(V ) such that (cid:107)pt
2 ≤ 2k/(ετ 2n). Now we can set
ξ = 1/(7n) and b = 2k/(ετ 2n) in Theorem E.1, which tells us that r = O((cid:112)nk/(ετ ) log(1/ε))
samples of the distributions pt
u −
v(cid:107)2
pt
2 ≤ 1/(8n), i.e., r samples allow us to determine if U = W or U (cid:54)= W , conditioned on a
1 − O(ε) probability event.

w sufﬁce to distinguish the cases (cid:107)pt

2 ≥ 2/n or (cid:107)pt

u and pt

u − pt

u(cid:107)2

v(cid:107)2

It is straightforward to translate the requirements of Theorem E.5 in terms of the number of KDE
queries required. Note that since we only take random walks of length O(log n/φ2
in), we can just
reduce the total variation distance from the distribution we sample our walks from and the true
random walk distribution appropriately in Theorem C.7. Alternatively, we can perform rejection
sampling as stated in the proof of Theorem C.5.
Corollary E.6. Algorithm 11 and Theorem E.5 require (cid:101)O(c(k, ε)(cid:112)nk/ε·1/(τ 1.5φ2
in)) KDE queries
(via calls to Algorithm 7, which performs random walks) as well as the same bound for post-
processing time.

E.2 Spectral Clustering

We present applications to spectral clustering. In data science, spectral clustering is often the follow-
ing clustering procedure: (a) compute k eigenvalues of the Laplacian matrix in order, (b) perform
k-means clustering on the Laplacian eigenvector embeddings of the vertices.

33

Published as a conference paper at ICLR 2023

The theory behind spectral clustering relies on the fact that the Lapalacian eigenvectors are effective
in representing the cluster structure of the underlying graph. We refer the reader to (Von Luxburg,
2007) and references within for more information. For our application to spectral clustering, we
show that a spectral sparsiﬁer, for example one computed from the prior sections, also preserves the
cluster structure of the graph.

Next we deﬁne a model of a “weakly clusterable” graph. Intuitively our model says that a graph
is k-weakly clusterable if its vertex set can be partitioned into k ‘well-connected’ pieces separated
by sparse cuts in between. Furthermore, this deﬁnition captures the notion of a well-deﬁned cluster
structure without which performing spectral clustering is meaningless. Note that this notion is less
stringent that the deﬁnitions of clusterable graphs commonly used in the property testing literature
which additionally require each piece to be well-connected internally, see Deﬁnition E.3.

Deﬁnition E.4 (Weakly clusterable Graph). A graph is (k, φout)-clusterable if the following holds:
There exists a partition of the vertex set into h ≤ k parts V = ∪1≤i≤hVi such that φG(Vi) ≤ φout.

We now prove the following result that says spectral sparsiﬁcation preserves cluster structure ac-
cording to Deﬁnition E.4. We ﬁrst remark that the spectral sparsiﬁer obtained in the previous section
is a cut sparsiﬁer as well. Recall that a cut sparsiﬁer is a subgraph that preserves the values across all
cuts up to relative error 1 ± ε. The implication follows immediately by noting that cuts are induced
by quadratic forms on the Laplacian matrix using {−1, 1}n vectors.

Theorem E.7. Let G be (k, φout)-clusterable and let G(cid:48) be a cut sparsiﬁer for G. Then G(cid:48) is
(k, (1 ± ε)φout)-clusterable.

Proof. Let Vi be one of the h ≤ k vertex partitions of G. Consider the conductance of Vi deﬁned
in Deﬁnition E.1. The numerator represents the value of a cut separating Vi and each term in the
denominator is the sum of the degrees of single vertices. Both values are appropriate cuts in the
graph. Since G(cid:48) is a cut sparsiﬁer, this implies that both the numerator and denominator are preserved
up to a 1 ± ε factor and thus, the entire ratio is also preserved up to a 1 ± O(ε) factor.

Theorem E.7 implies that the cluster structure of the sparsiﬁed graph G(cid:48) is approximately identical to
that of G. Thus, we can be conﬁdent that the spectral clustering procedure described at the beginning
of the section would perform equally as well on G(cid:48) as it would have on G. Indeed, we verify this
empirically in Section 2.

Furthermore, spectral clustering requires us to compute the ﬁrst k eigenvectors of the Laplacian
matrix. Since our sparsiﬁer has few edges, we can use Theorem 1 of (Musco & Musco, 2015), which
says (a variant of) the power method can quickly ﬁnd good approximations Laplacian eigenvectors
if the matrix is sparse.

Theorem E.8 (Corollary of Theorem 1 in (Musco & Musco, 2015) and Theorem D.1). Let L be the
Laplacian matrix of the sparsiﬁer computed in Theorem D.1. Let u1, · · · , uk be the ﬁrst k eigenvec-
tors of L. Using Theorem 1 of (Musco & Musco, 2015), we can ﬁnd k vectors v1, · · · , vk in time
(cid:101)O

such that with probability 99/100,

(cid:16) kn log n
τ 2ε2.5

(cid:17)

|uT

i Lui − vT

i Lvi| ≤ ελ2

k+1

for all i ∈ [k].

34

Published as a conference paper at ICLR 2023

E.3 Arboricity Estimation

Algorithm 12 Arboricity Estimation

w(e)
w(e(cid:48))
(cid:17)

1: ∆ = maxe,e(cid:48)∈E

(cid:16) n∆ log n
ε2

2: m ← O
3: for i = 1 to i = m do
4:

, G(cid:48) ← ∅

5:
6: end for
7: return maxU ⊆V d(G(cid:48)

U )

Sample an edge e with probability pe = (cid:99)we
(cid:80)
(cid:99)we
Add e to G(cid:48) with weight

1
mpe

, where (cid:99)we ∈ [we, 2we]

We now apply our algorithmic building blocks to the task of arboricity estimation. Consider a
weighted graph G = (V, E, w). Let GU be an induced subgraph of G on the subset of nodes U .
The density of GU is deﬁned as

d(GU ) :=

w(E(GU ))
|U |

where w(E(GU )) is the sum of the edge weights of GU . The arboricity of G is deﬁned as
α := max
U ⊆V

d(GU ).

The arboricity measures the density of the densest subgraph in a graph. Intuitively, it informs if there
is a strong cluster structure among some subset of the vertices of G. Therefore, it is an important
primitive in the analysis of massive graphs with applications ranging from community detection
in social networks, spam link identiﬁcation, and many more; see (Lee et al., 2010) for a survey of
applications and algorithmic results.

Although polynomial time algorithms exist, we are interested in efﬁciently approximating the value
of α using the building blocks of Section C. Inspired by the unweighted version of the arboricity
estimation algorithm from (McGregor et al., 2015), we ﬁrst prove the following result.
Theorem E.9. Let U (cid:48) = arg maxU d(G(cid:48)
probability at least 1 − 1/ poly(n),

U ) and let G(cid:48) be the output of Algorithm 12. Then with

Algorithm 12 uses m = (cid:101)O(n log n/(ε2τ )) KDE queries and O(mn) post-processing time.

(1 − ε)α ≤ d(G(cid:48)

U (cid:48)) ≤ (1 + ε)α.

Proof. Let U be an arbitrary set of k nodes, let W = (cid:80)
G has weight W , then the arboricity α satisﬁes α ≥ W

n , so that

e∈E w(e), and let WU = W · d(GU ) Since

m ≥

log n
α∆ε2 .

Let Xi be the random variable denoting the contribution of the i-th sample to weight of the edges in
G(cid:48)
i=1. Similarly,
we have

m · d(GU ) so that E[X] = W · d(GU ), for X = (cid:80)m

U and observe that E[Xi] = W

E[X 2

i ] =

(cid:88)

e=(u,v),u,v,∈U

pew2
e ·

1
em2 =
p2

(cid:88)

e=(u,v),u,v,∈U

W we
m2 =

W · WU
m2

.

Since m = Cn∆ log n

ε2

for an absolute constant C > 0, then

and

E[X 2

i ] ≤

kε2α2
Cm log2 n

m
(cid:88)

i=1

E[X 2

i ] ≤

kε2α2
Cm log2 n

.

35

Published as a conference paper at ICLR 2023

We also have Xi − E[Xi] ≤ αε2

Cm log2 n . Thus by Bernstein’s inequality for sufﬁciently large C,

for d(GU ) ≤ α

60 and

(cid:104)

Pr

d(G(cid:48)

U ) ≥

(cid:105)

α
10

≤ n−10k,

(cid:104)

Pr

|d(G(cid:48)

U ) − d(GU )| ≥

(cid:105)

εα
10

≤ 2n−10k,

for d(GU ) > α
60 .
Since there are (cid:0)n
k
probability at least 1 − 3n−9k, both

(cid:1) ≤ nk subsets of V with size k, then by a union bound, we have that with

d(G(cid:48)

U ) ≤

α
10

for all subsets U with d(GU ) ≤ α

60 and

(1 − ε)d(GU ) ≤ d(G(cid:48)

U ) ≤ (1 + ε)d(GU )

for all subsets U with d(GU ) > α
60 .
Hence for a set U ∗ such that d(U ∗) = α, we have d(GU ∗ ) ≥ (1 − ε)α so that d(GU (cid:48)) ≥ d(GU ∗ ) ≥
(1 − ε)α, where U (cid:48) = arg maxU ⊆V d(G(cid:48)

U ). Thus with high probability, we have that

as desired.

(1 − ε)α ≤ d(GU (cid:48)) ≤ (1 + ε)α,

To estimate the arboricity of the input graph G, it then sufﬁces Theorem E.9 to compute the arboricity
of the subsampled graph G(cid:48) output by Algorithm 12. This can be efﬁciently achieved by running an
ofﬂine algorithm such as (Charikar, 2000), which requires solving a linear program on m variables,
where m is the number of edges of the input graph. Thus our subsampling procedure serves as a
preprocessing step that ultimately signiﬁcantly improves the overall runtime.

E.4 Computing the Total Weight of Triangles

Algorithm 13 Weighted Triangle Counting
√

1: Let R ⊆ E be a random set of O

(cid:16) m

wGw3/2
max
ε2wT

(cid:17)

edges

(cid:16) w2

Gw3

maxm2

(cid:17)

wmin

2: s ← O
3: For v ∈ V , g(v) := (cid:80)
4: For (u, v) ∈ E, g(u, v) := min(w(u), w(v))
5: for i = 1 to i = s do
6:

(u,v)∈E w(u, v)

Sample e ∈ R with probability
Let e = (x, y) with x ≺ y
Sample neighbor z of x with probability w(x)
g(e)
if (x, y, z) is a triangle assigned to u then

g(e)
e∈E g(e)

7:

8:

(cid:80)

else

χi ← 1

9:
10:
11:
12:
end if
13:
14: end for
15: return m
|R|s

χi ← 0

(cid:80)s

i=1 χi

We apply the tools developed in prior section to counting the number of weighted triangles of a
kernel graph. Counting triangles is a fundamental graph algorithm task that has been explored in
numerous models and settings, including streaming algorithms, ﬁne-grained complexity, distributed

36

Published as a conference paper at ICLR 2023

shared-memory and MapReduce to name a few (Seshadhri et al., 2013; Atserias et al., 2008; Bera &
Chakrabarti, 2017; Kolountzakis et al., 2010; Chen et al., 2022). Applications include discovering
motifs in protein interaction networks (Milo et al., 2002), understanding social networks (Foucault
Welles et al., 2010), and evaluating large graph models (Leskovec et al., 2008); see the survey
(Al Hasan & Dave, 2018) for further information.

We deﬁne the weight of a triangle as the product of its edges. This deﬁnition is natural since it
generalizes the case were the edges have integer lengths. In this case, an edge can be thought of as
multiple parallel edges. The number of triangles on any set of three vertices must account for all
the parallel edge combinations. The product deﬁnition just extends this to the case of arbitrary real
non-negative weights. This deﬁnition has also been used in deﬁnitions of clustering-coefﬁcient for
weighted graphs (Kalna & Higham, 2006; Li et al., 2007; Antoniou & Tsompa, 2008).

Note that there is an alternate deﬁnition for the weight of a triangle in weighted graphs, which is just
the sum of edge weights. In the case of kernel graphs, this is not an interesting deﬁnition since we
can approximately compute the sum of all degrees using n KDE queries and divide by 3 to get an
accurate approximation.
Deﬁnition E.5. Let G = (V, E, w) with w : E → R≥0 be a weighted graph. Given a triangle
(x, y, z) ⊂ E, we deﬁne its weight as

where we abuse notation by deﬁning w(x, y) := w((x, y)).

w(x,y,z) = w(x, y) · w(y, z) · w(x, z),

For this deﬁnition, we present the following modiﬁed algorithm from (Eden et al., 2017), which
considers the problem in unweighted graphs in a different model of computing. See Remark B.5 for
comparison.

Theorem E.10. There exists an algorithm that makes (cid:101)O
bound for post-processing time and with probability at least 2
the total weight wT of the triangles in the kernel graph.

√

(cid:16) m

(cid:17)

wGw3/2
KDE queries and the same
max
wT ·ε2
3 , outputs a (1 ± ε)-approximation to

Proof. Given a graph G = (V, E), let |V | = n, |E| = m, (cid:80)
e∈E w(e) = wG, T be the number
of triangle in G, and wT be the sum of the weighted triangles in G, where the weight of a triangle
(x, y, z) ⊂ E, is the product of the weights of its edges

w(x,y,z) = w(x, y) · w(y, z) · w(x, z).

For a vertex v ∈ V , let w(v) = (cid:80)
e∈E,e=(u,v),u∈V w(e), so that we have an ordering on the vertex
set V by u ≺ v if and only if either w(u) ≤ w(v) or w(u) = w(v) and u appears v in the dictionary
ordering of the vertices. For each edge e = (u, v), we assign to e all triangle (u, v, w) such that
u ≺ v ≺ w. Let We denote the weight of the triangles assigned to e.

Suppose, by way of contradiction, there exists e ∈ E with We >
can contribute at most w3
wGw−3/2 vertices with weight at least
to e. Then there must be more than
contradicts the fact that the graph G has weight wG. Thus, we have that We ≤
e ∈ E. Moreover, we have that (cid:80)

max. Since each triangle
wGw−3/2 triangles must be assigned
√
wGw3/2
max, which
√
wGw3/2
max for all

max weight to We, then more than

e∈E We = wT so that

√

√

√

wGw3/2

and

Ee∈E[We] =

wT
m

Ee∈E[W 2

e ] ≤

√

wT

wGw3/2
max
m

.

By Chebyshev’s inequality, it sufﬁces to sample a set R with

|R| = O

(cid:32)

m

(cid:33)

√

wGw3/2
max
ε2wT

37

Published as a conference paper at ICLR 2023

edges uniformly at random, so that

Pr

(cid:34)

(cid:88)

e∈R

We ∈ (1 ± ε)|R| ·

(cid:35)

wT
m

≥ 0.99.

For each vertex v ∈ V , let g(v) = (cid:80)
min(w(u), w(v)). We write g(R) = (cid:80)

(u,v)∈E w(u, v) and for each edge e = (u, v), let g(e) =
e∈R g(e). Now for each i ∈ [|R|], we have

E[χi] =

(cid:88)

e∈R

We
g(R)

=

WR
g(R)

,

E[χ2

i ] ≤ 1.

Hence by Bernstein bounds, there exists a constant C > 0 such that it sufﬁces to repeat the procedure
ε2 · ·g(R)
C
g(R) with probability at least 2/3. We have that
WR
WR ≥ wT

times to get a (1 ± ε)-approximation of WR
√

max · |R| so that

2m · |R| and E[g(R)] ≤

wGw3/2

g(R)
WR

≤

√

2m

wGw3/2
max
wT

.

F Omitted Experimental Results

(a) Rank versus Error for Low-rank Approximation

(b) Real vs Approximate Row Norm Squared Values

(c) Rank versus Error for Low-rank Approximation

(d) Real vs Approximate Row Norm Sqaured Values

Figure 2: Figures for low rank approximation experiments.

Parameter settings. For low-rank approximation datasets, we choose the bandwidth value σ ac-
cording to the choice made in prior experiments, in particular the values given in (Backurs et al.,
2019). There, σ is chosen according to the popular median distance rule; see their experimental sec-
tion for further information. For our clustering experiments, we pick the value of σ, which results in
spectral clustering (running on the full kernel matrix) successfully clustering the input.

38

1020304050Rank120140160180200220Frobenius ErrorMNISTKDEISSVD050100150200True Normed Squared050100150200250300350KDE ApproximationMNISTy=x246810Rank122.5125.0127.5130.0132.5135.0137.5140.0Frobenius ErrorGloveKDEISSVD050100150200250300350400True Normed Squared0100200300400500KDE ApproximationGlovey=xPublished as a conference paper at ICLR 2023

Datasets for spectral sparsiﬁcation and clustering. For spectral sparsiﬁcation and clustering,
we use construct two synthetic datasets, which are challenging for other clustering method such as
k-means clustering2. The ﬁrst dataset denoted as ‘Nested’ consists of 5, 000 points, equally split
among the origin and a circle of radius 1. The two natural clusters are the points at the origin and
the points on the circle. Since one cluster is contained in the convex hull of the other, a method
like k-means clustering will not be able to separate the two clusters, but it is known that spectral
clustering can. Our second dataset, labeled ‘Rings’, is an even more challenging clustering dataset.
We consider two tori in three dimensions that pass through the interior hole of each other, i.e., they
interlock. The ‘small’ radius of each tori is 5 while the ‘large’ radius is 100. Our dataset consists
of 2500 points uniformly distributed on the two tori; see Figure 3b. Note that our focus is not to
compare the efﬁcacy of various clustering methods, which is done in other prior works (e.g., see
footnote 2). Rather, we show that spectral clustering itself can be optimized in terms of runtime,
space usage, and the number of kernel evaluations performed via our algorithms.

Evaluation metrics for spectral clustering and sparsiﬁcation. For spectral sparsiﬁcation and
clustering, we compare the accuracy of our method to the clustering solution when run on the full
initialized kernel matrix.

Note that prior works such as (Backurs et al., 2019; 2021) have used use the number of kernel
evaluations performed (i.e., how many entries of K are computed) as a measure of computational
cost. While this is a software and architecture independent basis of comparison, which is unaffected
by access to specialized libraries or hardware (e.g., SIMD, GPU), it is of interest to go beyond this
measure. Indeed, we use this measure as well as other important metrics as space usage and runtime
as points of comparison.

Spectral sparsiﬁcation and clustering results. Our algorithm consists of running the spectral spar-
siﬁcation algorithm of Theorem D.1 (Algorithm 8) and computing the ﬁrst two eigenvectors of the
normalized Laplacian of the resulting sparse graph. We then run k-means clustering on the com-
puted Laplacian embedding for k = 2. As noted above, we use two datasets that pose challenges for
traditional clustering methods such as k-means clustering. The Nested dataset is shown in Figure
3a. We sampled 3 · 105 many edges, which is 2.5% of total edges. Figure 4a shows the Laplacian
embedding of the sampled graph based on the ﬁrst two eigenvectors. The colors of the red and blue
points correspond to their cluster in Figure 3a as identiﬁed by running k-means clustering on the
Laplacian embedding. The orange crosses are the points that the spectral clustering method failed
to correctly classify. These are only 23 points, which represent a 0.5% of total points. Furthermore,
Figure 4a shows that the Laplacian embedding of the sampled graph is able to embed the two clus-
ters into distinct and disjoint regions. Note that the total space savings of the sampled graph over
storing the entire graph is 41x. In terms of the time taken, the iterative SVD method used to calculate
the Laplacian eigenvectors took 0.18 seconds on the sparse graph whereas the same method took
0.81 seconds on the entire graph. This is a 4.5x factor reduction.

We recorded qualitatively similar results for the rings dataset. Figure 3b shows a plot of the dataset.
We sampled 105 many edges for the approximation, which represents a 3.3% of total edges for form
the sparse graph. The Laplacian embedding of the sparse graph is shown in Figure 4b. In this case,
the embedding constructed from the sparse graph was able to separate the two rings into disjoint
regions perfectly. The time taken for computing the Laplacian eigenvectors for the sparse graph was
0.08 seconds whereas it took 0.27 seconds for the full dense matrix.

G Auxiliary Inequalities

Theorem G.1 (Bernstein’s inequality). Let X1, . . . , Xn be independent random variables such that
E[X 2

i ] < ∞ and Xi ≥ 0 for all i ∈ [n]. Let X = (cid:80)

Pr [X ≤ E[X] − γ] ≤ exp

i Xi and γ > 0. Then
−γ2

(cid:19)

(cid:18)

2 (cid:80)
i

E[X 2
i ]

.

2For example, see https://scikit-learn.org/stable/auto_examples/cluster/plot_

cluster_comparison.html.

39

Published as a conference paper at ICLR 2023

(a)

(a)

Figure 3: (a) Nested Dataset, (b) Rings Dataset

(b)

(b)

Figure 4: Spectral embedding of sparsiﬁed graph for (a) Nested dataset and (b) Rings dataset, re-
spectively.

If Xi − E[Xi] ≤ ∆ for all i, then for σ2

i = E[X 2

i ] − E[Xi]2,

−γ2
i + 2γ∆/3

2 (cid:80)

i σ2

(cid:19)

.

Pr [X ≥ E[X] + γ] ≤ exp

(cid:18)

40

10.07.55.02.50.02.55.07.510.010.07.55.02.50.02.55.07.510.0Nested Dataset100500501002001501005005010010050050100Rings Dataset0.0100.0120.0140.0160.0181st Laplacian Eigenvector0.030.020.010.000.010.022nd Laplacian EigenvectorNested DatasetOrigin PointsCircle PointsMislabeled Points0.010.020.030.040.050.060.070.080.080.060.040.020.000.020.04Rings Dataset