Under review as a conference paper at ICLR 2023

REMOVING STRUCTURED NOISE WITH
DIFFUSION MODELS

Anonymous authors
Paper under double-blind review

ABSTRACT

Solving ill-posed inverse problems requires careful formulation of prior beliefs
over the signals of interest and an accurate description of their manifestation into
noisy measurements. Handcrafted signal priors based on e.g. sparsity are increas-
ingly replaced by data-driven deep generative models, and several groups have
recently shown that state-of-the-art score-based diffusion models yield particu-
larly strong performance and flexibility. In this paper, we show that the powerful
paradigm of posterior sampling with diffusion models can be extended to include
rich, structured, noise models. To that end, we propose a joint conditional reverse
diffusion process with learned scores for the noise and signal-generating distribu-
tion. We demonstrate strong performance gains across various inverse problems
with structured noise, outperforming competitive baselines that use normalizing
flows and adversarial networks. This opens up new opportunities and relevant
practical applications of diffusion modeling for inverse problems in the context of
non-Gaussian measurements.

1

INTRODUCTION

Many signal and image processing problems, such as denoising, compressed sensing, or phase re-
trieval, can be formulated as inverse problems that aim to recover unknown signals from (noisy)
observations. These ill-posed problems are, by definition, subject to many solutions under the given
measurement model. Therefore, prior knowledge is required for a meaningful and physically plau-
sible recovery of the original signal. Bayesian inference and maximum a posteriori (MAP) solutions
incorporate both signal priors and observation likelihood models. Choosing an appropriate statistical
prior is not trivial and is often dependent on both the application as well as the recovery task.

Before deep learning, sparsity in some transform domain has been the go-to prior in compressed
sensing (CS) methods (Eldar & Kutyniok, 2012), such as iterative thresholding (Beck & Teboulle,
2009) or wavelet decomposition (Mallat, 1999). At present, deep generative modeling has estab-
lished itself as a strong mechanism for learning such priors for inverse problem-solving. Both gen-
erative adversarial networks (GANs) (Bora et al., 2017) and normalizing flows (NFs) (Asim et al.,
2020; Wei et al., 2022) have been applied as natural signal priors for inverse problems in image
recovery. These data-driven methods are more powerful compared to classical methods, as they can
accurately learn the natural signal manifold and do not rely on assumptions such as signal sparsity
or hand-crafted basis functions. Recently, diffusion models have shown impressive results for both
conditional and unconditional image generation and can be easily fitted to a target data distribution
using score matching (Vincent, 2011; Song et al., 2020). These deep generative models learn the
score of the data manifold and produce samples by reverting a diffusion process, guiding noise sam-
ples towards the target distribution. Diffusion models have achieved state-of-the-art performance in
many downstream tasks and applications, ranging from state-of-the-art text-to-image models such as
DALL-E 2 (Ramesh et al., 2022) to medical imaging (Song et al., 2021b; Jalal et al., 2021a; Chung
& Ye, 2022). Furthermore, understanding of diffusion models is rapidly improving and progress
in the field is extremely fast-paced (Chung et al., 2022a; Bansal et al., 2022; Daras et al., 2022a;
Karras et al., 2022; Luo, 2022). The iterative nature of the sampling procedure used by diffusion
models renders inference slow compared to GANs and VAEs. However, many recent efforts have
shown ways to significantly improve the sampling speed by accelerating the diffusion process. In-
spired by momentum methods in sampling, Daras et al. (2022b) introduces a momentum sampler for
diffusion models, which leads to increased sample quality with fewer function evaluations. Chung

1

Under review as a conference paper at ICLR 2023

et al. (2022b) offers a new sampling strategy, namely Come-Closer-Diffuse-Faster (CCDF), which
leverages the conditional quality of inverse problems. The reverse diffusion can be initialized from
the observation instead of a sample from the base distribution, which leads to faster convergence
for conditional sampling. Salimans & Ho (2021) proposes a progressive distillation method that
augments the training of the diffusion models with a student-teacher model setup. In doing this,
they were able to drastically reduce the number of sampling steps. Lastly, many methods aim to
execute the diffusion process in a reduced space to accelerate the diffusion process. While Jing et al.
(2022) restricts diffusion through projections onto subspaces, Vahdat et al. (2021) and Rombach
et al. (2022) run the diffusion in the latent space.

Despite this promise, current score-based diffusion methods for inverse problems are limited to mea-
surement models with unstructured noise. In many image processing tasks, corruptions are however
highly structured and spatially correlated. Relevant examples include interference, speckle, or haze.
Nevertheless, current conditional diffusion models naively assume that the noise follows some basic
tractable distribution (e.g. Gaussian or Poisson). Beyond the realm of diffusion models, Whang et al.
(2021) extended normalizing flow (NF)-based inference to structured noise applications. However,
compared to diffusion models, NFs require specialized network architectures, which are computa-
tionally and memory expensive.

Given the promising outlook of diffusion models, we propose to learn score models for both the noise
and the desired signal and perform joint inference of both quantities, coupled via the observation
model. The resulting sampling scheme enables solving a wide variety of inverse problems with
structured noise.

The main contributions of this work are as follows:

• We propose a novel joint conditional posterior sampling method to efficiently remove struc-
tured noise using diffusion models. Our formulation is compatible with many existing
iterative sampling methods for score-based generative models.

• We show strong performance gains across various challenging inverse problems involving
structured noise compared to competitive state-of-the-art methods based on NFs and GANs.

• We demonstrate improved robustness on out-of-distribution signals compared to baselines.

2 PROBLEM STATEMENT

Many image reconstruction tasks can be formulated as an inverse problem with the basic form

y = Ax + n,
(1)
where y ∈ Rm is the noisy observation, x ∈ Rd the desired signal or image, and n ∈ Rm the
additive noise. The linear forward operator A ∈ Rm×d captures the deterministic transformation of
x. Maximum a posteriori (MAP) inference is typically used to find an optimal solution ˆxMAP that
maximizes posterior density pX|Y (x|y):

ˆxMAP = arg max

x

= arg max

x

log pX|Y (x|y)
(cid:2)log pY |X (y|x) + log pX (x)(cid:3),

(2)

(3)

where pY |X (y|x) is the likelihood according to the measurement model and log pX (x) the signal
prior.

Assumptions on the stochastic corruption process n are of key importance too, in particular for ap-
plications for which this process is highly structured. However, most methods assume i.i.d. Gaussian
distributed noise, such that the forward model becomes pY |X (y|x) ∼ N (Ax, σ2
N I). This naturally
leads to the following simplified problem:

ˆxMAP = arg min

x

1
2σ2
N

||y − Ax||2

2 − log pX (x).

(4)

However, as mentioned, this naive assumption can be very restrictive as many noise processes are
much more structured and complex. A myriad of problems can be addressed under the formulation

2

Under review as a conference paper at ICLR 2023

of equation 1, given the freedom of choice for the noise source n. Therefore, in this work, our
aim is to solve a more broad class of inverse problems defined by any arbitrary noise distribution
n ∼ pN (n) ̸= N and signal prior x ∼ pX (x), resulting in the following, more general, MAP
estimator proposed by Whang et al. (2021):

ˆxMAP = arg max

x

log pN (y − Ax) − log pX (x).

(5)

In this paper, we propose to solve this class of problems using flexible diffusion models. Further-
more, diffusion models naturally enable posterior sampling, allowing us to take advantage of the
benefits thereof (Jalal et al., 2021b; Kawar et al., 2021; Daras et al., 2022a).

2.1 RELATED WORK

2.1.1 NORMALIZING FLOWS

Whang et al. (2021) propose to use normalizing flows (NFs) to model both the data and the noise
distributions. Normalizing flows are a special class of likelihood-based generative models that make
use of an invertible mapping G : Rd → Rd to transform samples from a base distribution pZ(z) into
a more complex multimodal distribution x = G(z) ∼ pX (x). The invertible nature of the mapping
G allows for exact density evaluation through the change of variables formula:

log pX (x) = log pZ(z) + log | det JG−1 (x)|,

(6)

where J is the Jacobian that accounts for the change in volume between densities. Since exact
likelihood computation is possible through the flow direction G−1, the parameters of the generator
network can be optimized to maximize likelihood of the training data. Subsequently, the inverse
task is solved using the MAP estimation in equation 5:

ˆx = arg max

x

{log pGN (y − Ax) + log pGX (x)} ,

(7)

where GN and GX are generative flow models for the noise and data respectively. Analog to that,
the solution can be solved in the latent space rather than the image space as follows:

ˆz = arg max

z

{log pGN (y − A(GX (z))) + λ log pGX (GX (z))} .

(8)

Note that in equation 8 a smoothing parameter λ is added to weigh the prior and likelihood terms,
as was also done in Whang et al. (2021).

2.1.2 GENERATIVE ADVERSARIAL NETWORKS

Generative adversarial networks (GANs) are implicit generative models that can learn the data man-
ifold in an adversarial manner (Goodfellow et al., 2020). The generative model is trained with an
auxiliary discriminator network that evaluates the generator’s performance in a minimax game. The
generator G(z) : Rl → Rd maps latent vectors z ∈ Rl ∼ N (0, I) to the data distribution of interest.
The structure of the generative model can also be used in inverse problem solving (Bora et al., 2017).
The objective can be derived from equation 3 and is given by:

ˆz = arg min

z

(cid:8)||y − AGX (z)|| + λ||z||2

2

(cid:9) ,

(9)

where λ weights the importance of the prior with the measurement error. The ℓ2 regularization
term on the latent variable is proportional to negative log-likelihood under the prior defined by GX ,
where the subscript denotes the density that the generator is approximating. While this method
does not explicitly model the noise, it remains an interesting comparison, as the generator cannot
reproduce the noise found in the measurement and can only recover signals that are in the range of
the generator. Therefore, due to the limited support of the learned distribution, GANs can inherently
remove structured noise. However, the representation error (i.e. observation lies far from the range
of the generator (Bora et al., 2017)) imposed by the structured noise comes at the cost of recovery
quality.

3

Under review as a conference paper at ICLR 2023

2.2 BACKGROUND ON SCORE-BASED DIFFUSION MODELS

One class of deep generative models is known as diffusion models. These generative models have
been introduced independently as score-based models (Song & Ermon, 2019; 2020) and denoising
In this work, we will consider the
diffusion probabilistic modeling (DDPM) (Ho et al., 2020).
formulation introduced in Song et al. (2020), which unifies both perspectives on diffusion models
by expressing diffusion as a continuous process through stochastic differential equations (SDE).
Diffusion models produce samples by reversing a corruption process. In essence these models are
networks trained to denoise its input. Through iteration of this process, samples can be drawn from
a learned data distribution, starting from random noise.
The diffusion process of the data (cid:8)xt ∈ Rd(cid:9)
t∈[0,1] is characterized by a continuous sequence of
Gaussian perturbations of increasing magnitude indexed by time t ∈ [0, 1]. Starting from the data
distribution at t = 0, clean images are defined by x0 ∼ p(x0) ≡ p(x). Forward diffusion can be
described using an SDE as follows:

dxt = f (t)xtdt + g(t)dw,
(10)
where w ∈ Rd is a standard Wiener process, f (t) : [0, 1] → R and g(t) : [0, 1] → R are the
drift and diffusion coefficients, respectively. Moreover, these coefficients are chosen so that the
resulting distribution p1(x) at the end of the perturbation process approximates a predefined base
distribution p1(x) ≈ π(x). Furthermore, the transition kernel of the diffusion process is defined as
q(xt|x) ∼ N (xt|α(t)x, β2(t)I), where α(t) and β(t) can be analytically derived from the SDE.

Naturally, we are interested in reversing the diffusion process, so that we can sample from x0 ∼
p0(x0). The reverse diffusion process is also a diffusion process given by the reverse-time SDE
(Anderson, 1982; Song et al., 2020):

dxt = (cid:2)f (t)xt − g(t)2 ∇xt log p(xt)
(cid:125)

(cid:124)

(cid:123)(cid:122)
score

(cid:3)dt + g(t)d ¯wt

(11)

where ¯wt is the standard Wiener process in the reverse direction. The gradient of the log-likelihood
of the data with respect to itself, a.k.a. the score function, arises from the reverse-time SDE. The
score function is a gradient field pointing back to the data manifold and can intuitively be used to
guide a random sample from the base distribution π(x) to the desired data distribution. Given a
dataset X = (cid:8)x(1), x(2), . . . , x(|X |)(cid:9) ∼ p(x), scores can be estimated by training a neural network
sθ(xt, t) parameterized by weights θ, with score-matching techniques such as the denoising score
matching (DSM) objective (Vincent, 2011):

θ∗ = arg min

θ

Et∼U [0,1]

(cid:110)
E(x,xt)∼p(x)q(xt|x)

(cid:104)

∥sθ(xt, t) − ∇xt log q(xt|x)∥2

2

(cid:105)(cid:111)

.

(12)

Given a sufficiently large dataset X and model capacity, DSM ensures that the score network con-
verges to sθ(xt, t) ≃ ∇xt log p(xt). After training the time-dependent score model sθ, it can be
used to calculate the reverse-time diffusion process and solve the trajectory using numerical sam-
plers such as the Euler-Maruyama algorithm. Alternatively, more sophisticated samplers, such as
ALD (Song & Ermon, 2019), probability flow ODE (Song et al., 2020), and Predictor-Corrector
sampler (Song et al., 2020), can be used to further improve sample quality.

These iterative sampling algorithms discretize the continuous time SDE into a sequence of time
steps {0 = t0, t1, . . . , tT = 1}, where a noisy sample ˆxti is denoised to produce a sample for the
next time step ˆxti−1. The resulting samples {ˆxti}T
i=0 constitute an approximation of the actual
diffusion process {xt}t∈[0,1].

3 METHOD

3.1 CONDITIONAL POSTERIOR SAMPLING UNDER STRUCTURED NOISE

We are interested in posterior sampling under structured noise. We recast this as a joint optimization
problem with respect to the signal x and noise n given by:

(x, n) ∼ pX,N (x, n|y) ∝ pY |X,N (y|x, n) · pX (x) · pN (n).

(13)

4

Under review as a conference paper at ICLR 2023

Solving inverse problems using diffusion models requires conditioning of the diffusion process on
the observation y, such that we can sample from the posterior pX|Y (x, n|y). Therefore, we construct
a joint conditional diffusion process {xt, nt|y}t∈[0,1], in turn producing a joint conditional reverse-
time SDE:

d(xt, nt) = (cid:2)f (t)(xt, nt) − g(t)2∇xt,nt log p(xt, nt|y)(cid:3)dt + g(t)d ¯wt.

(14)

We would like to factorize the posterior using our learned unconditional score model and tractable
measurement model, given the joint formulation. Consequently, we construct two separate dif-
fusion processes, defined by separate score models but entangled through the measurement model
pY |X,N (y|x, n). In addition to the original score model sθ(x, t), we introduce a second score model
sϕ(nt, t) ≃ ∇nt log pN (nt), parameterized by weights ϕ, to model the expressive noise component
n. These two score networks can be trained independently on datasets for x and n, respectively,
using the objective in equation 12. The gradients of the posterior with respect to x and n are now
given by:

∇xt log p(xt, nt|y) ≃ ∇xt log p(xt) + ∇xt log p(ˆyt|xt, nt)

≃ sθ⋆ (xt, t) + ∇xt log p(ˆyt|xt, nt),

∇nt log p(xt, nt|y) ≃ ∇nt log p(nt) + ∇nt log p(ˆyt|xt, nt)

≃ sϕ⋆ (nt, t) + ∇nt log p(ˆyt|xt, nt),

(15)

(16)

where ˆyt is a sample from p(yt|y), and {yt}t∈[0,1] is an additional stochastic process that essentially
corrupts the observation along the SDE trajectory together with xt. As p(yt|y) is tractable, we can
easily compute ˆyt = α(t)y + β(t)Az, using the reparameterization trick with z ∈ Rd ∼ N (0, I),
see Song et al. (2021b). Subsequently, the approximation in equation 15 and equation 16 can be
substituted for the conditional score in equation 14, resulting in two entangled diffusion processes:

(cid:26) dxt = (cid:2)f (t)xt − g(t)2 {sθ⋆ (xt, t) + ∇xt log p(ˆyt|xt, nt)} (cid:3)dt + g(t)d ¯wX,t
dnt = (cid:2)f (t)nt − g(t)2 {sϕ⋆ (nt, t) + ∇nt log p(ˆyt|xt, nt)} (cid:3)dt + g(t)d ¯wN,t

(17)

which allows us to perform posterior sampling for both the signal, such that x ≡ x0 ∼ pX|Y (x0|y),
as well as the structured noise, such that n ≡ n0 ∼ pN |Y (n0|y).

To solve the approximated joint conditional reverse-time SDE, we resort to the aforementioned
iterative scheme in Section 2.2, however, now incorporating the observation via a data-consistency
step. This is done by taking gradient steps that minimize the ℓ2 norm between the true observation
and its model prediction given current estimates of x and n. Ultimately, this results in solutions
that are consistent with the observation y and have high likelihood under both prior models. The
data-consistency update steps for both x and n are derived as follows:

ˆxt−∆t = ˆxt − ∇xt log p(ˆyt|ˆxt, ˆnt)

ˆnt−∆t = ˆnt − ∇nt log p(ˆyt|ˆxt, ˆnt)

= ˆxt − ∇xt||ˆyt − (Aˆxt + ˆnt)||2
2
= ˆxt − λAT(Aˆxt − ˆyt + ˆnt),

(18)

= ˆnt − ∇nt||ˆyt − (Aˆxt + ˆnt)||2
2
= ˆnt − µ(Aˆxt − ˆyt + ˆnt),
(19)

where the time difference between two steps ∆t = 1/T and λ and µ are weighting coefficients for
the signal and noise gradient steps, respectively. An example of the complete sampling algorithm is
shown in Algorithm 1, which adapts the Euler-Maruyama sampler (Song et al., 2020) to jointly find
the optimal data sample and the optimal noise sample while taking into account the measurement
model in line 7 and 8 using the outcome of equation 18 and equation 19, respectively. Although we
show the Euler-Maruyama method, our addition is applicable to a large family of iterative sampling
methods for score-based generative models.

5

Under review as a conference paper at ICLR 2023

Algorithm 1: Joint conditional posterior sampling with Euler-Maruyama method

Require: T, sθ, sϕ, λ, µ, y

1 ˆx1 ∼ π(x), ˆn1 ∼ π(n), ∆t ← 1
T

2

3 for i = T − 1 to 0 do

t ← i+1
T
ˆyt ∼ p0t(yt|y)

// Data consistency steps
ˆxt−∆t ← ˆxt − λAT(Aˆxt − ˆyt + ˆnt)
ˆnt−∆t ← ˆnt − µ(Aˆxt − ˆyt + ˆnt)

4

5

6

7

8

9 . . .

9 . . .

10

11

12

13

14

15

16

17

18

ˆxt−∆t ← ˆxt − f (t)ˆxt∆t
ˆxt−∆t ← ˆxt−∆t + g(t)2s∗
z ∼ N (0, I)
ˆxt−∆t ← ˆxt−∆t + g(t)

√

∆tz

θ(ˆxt, t)∆t

ˆnt−∆t ← ˆnt − f (t)ˆnt∆t
ˆnt−∆t ← ˆnt−∆t + g(t)2s∗
z ∼ N (0, I)
ˆnt−∆t ← ˆnt−∆t + g(t)
return: ˆx0

√

∆tz

ϕ(ˆnt, t)∆t

19 end

3.2 TRAINING AND INFERENCE SETUP

For training the score models, we use the NCSNv2 architecture as introduced in Song & Ermon
(2020) in combination with the Adam optimizer and a learning rate of 5e−4 until convergence. For
simplicity, no exponential moving average (EMA) filter on the network weights is applied. Given
two separate datasets, one for the data and one for the structured noise, two separate score models
can be trained independently. This allows for easy adaptation of our method, since many existing
trained score models can be reused. Only during inference, the two priors are combined through
the proposed sampling procedure as described in Algorithm 1, using the adapted Euler-Maruyama
sampler. We use the variance preserving (VP) SDE (β0 = 0.1, β1 = 7.0) (Song et al., 2020) to
define the diffusion trajectory. During each experiment, we run the sampler for T = 600 iterations.

4 EXPERIMENTS

All models are trained on the CelebA dataset (Liu et al., 2015) and the MNIST dataset with 10000
and 27000 training samples, respectively. We downsize the images to 64 × 64 pixels. Due to
computational constraints, we test on a randomly selected subset of 100 images. We use both the
peak signal-to noise ratio (PSNR) and structural similarity index (SSIM) to evaluate our results.

4.1 BASELINE METHODS

The closest to our work is the flow-based noise model proposed by Whang et al. (2021), discussed in
Section 2.1.1, which will serve as our main baseline. To boost the performance of this baseline and
to make it more competitive we moreover replace the originally-used RealNVP (Dinh et al., 2016)
with the Glow architecture (Kingma & Dhariwal, 2018). Glow is a widely used flow model highly
inspired by RealNVP, with the addition of 1 × 1 convolutions before each coupling layer. We use
the exact implementation found in Asim et al. (2020), with a flow depth of K = 18, and number
of levels L = 4, which has been optimized for the same CelebA dataset used in this work and thus
should provide a fair comparison with the proposed method.

Secondly, GANs as discussed in Section 2.1.2 are used as a comparison. We train a DCGAN (Rad-
ford et al., 2015), with a generator latent input dimension of l = 100. The generator architecture
consists of 4 strided 2D transposed convolutional layers, having 4 × 4 kernels yielding feature maps
of 512, 256, 128 and 64. Each convolutional layer is followed by a batch normalization layer and
ReLU activation.

Lastly, depending on the reconstruction task, classical non-data-driven methods are used as a com-
parison. For denoising experiments, we use the block-matching and 3D filtering algorithm (BM3D)
(Dabov et al., 2006), and in compressed sensing experiments, LASSO with wavelet basis (Tibshi-
rani, 1996).

6

Under review as a conference paper at ICLR 2023

(a) CelebA

(b) Out-of-distribution data

Figure 1: Quantitative results using PSNR (green) and SSIM (blue) for the removing MNIST digits
experiment on 64 × 64 images of the (a) CelebA and (b) out-of-distribution datasets.

Except for the flow-based method of Whang et al. (2021), none of these methods explicitly model
the noise distribution. Still, they are a valuable baseline, as they demonstrate the effectiveness of
incorporating a learned structured noise prior rather than relying on simple noise priors.

Automatic hyperparameter tuning for optimal inference was performed for all baseline methods on
a small validation set of only 5 images. For both GAN and flow-based methods, we anneal the step
size during inference based on stagnation of the objective.

4.2 RESULTS

4.2.1 REMOVING MNIST DIGITS

For comparison with Whang et al. (2021), we recreate an experiment introduced in their work,
where MNIST digits are added to CelebA faces. Moreover, the experiment is easily reproducible
as both CelebA and MNIST datasets are publicly available. The corruption process is defined by
y = 0.5 · xCelebA + 0.5 · nMNIST. In this experiment, the score network sϕ is trained on the MNIST
dataset. Fig. 1a shows a quantitative comparison of our method with all baselines. Furthermore, a
random selection of test samples is shown in Fig. 2 for qualitative analysis. Both our method and the
flow-based method are able to recover the data, and remove most of the structured noise. However,
more details are preserved using the diffusion method. In contrast, the flow-based method cannot
completely remove the digits in some cases and is unable to reconstruct some subtle features present
in the original images. Furthermore, we observe that for the flow-based method, initialization from
the measurement is necessary to reproduce the results in Whang et al. (2021) since random initial-
ization does not converge. The GAN method is also able to remove the digits, but cannot accurately
reconstruct the faces as it is unable to project the observation onto the range of the generator. Simi-
larly, the BM3D denoiser fails to recover the underlying signal, confirming the importance of prior
knowledge of the noise in this experiment. The metrics in Fig. 1a support these observations. See
Table 1 for the extended results.

Additionally, we expose the methods in a similar experiment to out-of-distribution (OoD) data. The
images from this dataset not found in the CelebA dataset, which is the data used for training the
models. In fact, the out-of-distribution data is generated using the stable-diffusion text-to-image
model Rombach et al. (2022). We use the exact same hyperparameters as during the experiment on
the CelebA dataset. Quantitative and qualitative results are shown in Fig. 1b and Fig. 3, respectively.
Similarly to the findings of Whang et al. (2021); Asim et al. (2020), the flow-based method is robust
to OoD data, due to their inherent invertibility. We empirically show that the diffusion method is also
resistant to OoD data in inverse tasks with complex noise structures and even outperforms the flow-
based methos. Unsurprisingly, the GAN method performs even more poorly when subjected to OoD
data. More experiments, covering different inverse problem settings can be found in Appendix A.

7

BM3DGANFLOWDIFFUSION1015202530PSNR11.5717.522.9127.260.330.490.830.870.00.20.40.60.81.0SSIMBM3DGANFLOWDIFFUSION1015202530PSNR10.4513.0719.9822.880.270.220.820.840.00.20.40.60.81.0SSIMUnder review as a conference paper at ICLR 2023

Figure 2: Results for our diffusion-based method compared to the baselines; FLOW (Whang et al.,
2021), GAN (Bora et al., 2017), and BM3D (Dabov et al., 2006) on the removing MNIST digits
experiment on 64 × 64 images of the CelebA dataset.

Figure 3: Results for our diffusion-based method on the removing MNIST digits experiment on an
out of distribution dataset, generated using stable diffusion (Rombach et al., 2022).

8

GroundTruthNoisyInputDIFFUSIONFLOWGANBM3DGroundTruthNoisyInputDIFFUSIONFLOWGANBM3DUnder review as a conference paper at ICLR 2023

4.2.2 PERFORMANCE

To highlight the difference in inference time between our method and the baselines, benchmarks
are performed on a single 12GBytes NVIDIA GeForce RTX 3080 Ti, see Table 3 in Appendix B.2.
Although this is not an extensive benchmark, a quick comparison of inference times reveals a 50×
difference in speed between ours and the flow-based method. All the deep generative models need
approximately an equal amount of iterations (T ≈ 600) to converge. However, for the same model-
ing capacity, the flow model requires a substantial higher amount of trainable parameters compared
to the diffusion method. This is mainly due to the restrictive requirements imposed on the architec-
ture to ensure tractable likelihood computation. It should be noted that no improvements to speed
up the diffusion process, such as CCDF (Chung et al., 2022b) are applied for the diffusion method,
giving room for even more improvement in future work.

5 DISCUSSION AND CONCLUSIONS

In this work, we present a framework for removing structured noise using diffusion models. Our
work provides an efficient addition to existing score-based conditional sampling methods incor-
porating knowledge of the noise distribution. We demonstrate our method on natural and out-of-
distribution data and achieve increased performance over the state-of-the-art and established con-
ventional methods for complex inverse tasks. Additionally, the diffusion based method is substan-
tially easier to train using the score matching objective compared to other deep generative methods
and furthermore allows for posterior sampling.

While our method is considerably faster and better in removing structured noise compared to the
flow-based method (Whang et al., 2021), it is not ready (yet) for real-time inference and still slow
compared to GANs (Bora et al., 2017) and classical methods. Luckily, research into accelerating
In addition, although a simple sampling algorithm
the diffusion process are well on their way.
was adapted in this work, many more sampling algorithms for score-based diffusion models exist,
each of which introduces a new set of hyperparameters. For example, the predictor-corrector (PC)
sampler has been shown to improve sample quality (Song et al., 2020). Future work should explore
this wide increase in design space to understand limitations and possibilities of more sophisticated
sampling schemes in combination with the proposed joint diffusion method. Furthermore, the range
of problems to which we can apply the proposed method, can be expanded into non-linear likelihood
models and extend beyond the additive noise models.

Lastly, the connection between diffusion models and continuous normalizing flows through the neu-
ral ODE formulation (Song et al., 2021a) is not investigated, but greatly of interest given the com-
parison with the flow-based method in this work.

6 REPRODUCIBILITY STATEMENT

All code used to train and evaluate the models as presented in this paper can be found at https:
//anonymous.4open.science/r/iclr2023-joint-diffusion. Essentially, the codebase in
https://github.com/yang-song/score_sde_pytorch of Song et al. (2020) is used to train
the score-based diffusion networks, for both data and structured noise, independently. To implement
the proposed inference scheme, the lines in Algorithm 1 should be adapted to create a sampler that
includes both trained diffusion models. Details regarding the training and inference settings used to
reproduce the results in this work can be found in Section 3.2.

REFERENCES

Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Ap-

plications, 12(3):313–326, 1982. 4

Muhammad Asim, Max Daniels, Oscar Leong, Ali Ahmed, and Paul Hand. Invertible generative
models for inverse problems: mitigating representation error and dataset bias. In International
Conference on Machine Learning, pp. 399–409. PMLR, 2020. 1, 6, 7

9

Under review as a conference paper at ICLR 2023

Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Gold-
blum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms
without noise. arXiv preprint arXiv:2208.09392, 2022. 1

Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM journal on imaging sciences, 2(1):183–202, 2009. 1

Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using genera-
tive models. In International Conference on Machine Learning, pp. 537–546. PMLR, 2017. 1, 3,
8, 9, 15

Hyungjin Chung and Jong Chul Ye. Score-based diffusion models for accelerated mri. Medical

Image Analysis, pp. 102479, 2022. 1

Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for

inverse problems using manifold constraints. arXiv preprint arXiv:2206.00941, 2022a. 1

Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating con-
In Proceedings
ditional diffusion models for inverse problems through stochastic contraction.
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12413–12422,
2022b. 1, 9

Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising with
block-matching and 3d filtering. In Image processing: algorithms and systems, neural networks,
and machine learning, volume 6064, pp. 354–365. SPIE, 2006. 6, 8, 15

Giannis Daras, Yuval Dagan, Alex Dimakis, and Constantinos Daskalakis. Score-guided interme-
diate level optimization: Fast langevin mixing for inverse problems. In International Conference
on Machine Learning, pp. 4722–4753. PMLR, 2022a. 1, 3

Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G. Dimakis, and Peyman Milanfar.

Soft diffusion: Score matching for general corruptions, 2022b. 1

Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv

preprint arXiv:1605.08803, 2016. 6

Yonina C Eldar and Gitta Kutyniok. Compressed sensing: theory and applications. Cambridge

university press, 2012. 1

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the
ACM, 63(11):139–144, 2020. 3

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in

Neural Information Processing Systems, 33:6840–6851, 2020. 4

Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir. Robust
compressed sensing mri with deep generative priors. Advances in Neural Information Processing
Systems, 34:14938–14954, 2021a. 1

Ajil Jalal, Sushrut Karmalkar, Alex Dimakis, and Eric Price. Instance-optimal compressed sens-
ing via posterior sampling. In International Conference on Machine Learning, pp. 4709–4720.
PMLR, 2021b. 3

Bowen Jing, Gabriele Corso, Renato Berlinghieri, and Tommi Jaakkola. Subspace diffusion gener-

ative models. arXiv preprint arXiv:2205.01490, 2022. 2

Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-

based generative models. arXiv preprint arXiv:2206.00364, 2022. 1

Bahjat Kawar, Gregory Vaksman, and Michael Elad. Snips: Solving noisy inverse problems stochas-

tically. Advances in Neural Information Processing Systems, 34:21757–21769, 2021. 3

Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.

Advances in neural information processing systems, 31, 2018. 6

10

Under review as a conference paper at ICLR 2023

Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015. 6

Calvin Luo.

Understanding diffusion models: A unified perspective.

arXiv preprint

arXiv:2208.11970, 2022. 1

Stéphane Mallat. A wavelet tour of signal processing. Elsevier, 1999. 1

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep

convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015. 6

Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-

conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022. 2, 7, 8, 13

Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In

International Conference on Learning Representations, 2021. 2

Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.

Advances in Neural Information Processing Systems, 32, 2019. 4

Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.

Advances in neural information processing systems, 33:12438–12448, 2020. 4, 6

Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
In Interna-

Poole. Score-based generative modeling through stochastic differential equations.
tional Conference on Learning Representations, 2020. 1, 4, 5, 6, 9

Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-
based diffusion models. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 1415–1428.
Curran Associates, Inc., 2021a. 9

Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging
with score-based generative models. In International Conference on Learning Representations,
2021b. 1, 5

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society: Series B (Methodological), 58(1):267–288, 1996. 6, 15

Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space.

Advances in Neural Information Processing Systems, 34:11287–11302, 2021. 2

Pascal Vincent. A connection between score matching and denoising autoencoders. Neural compu-

tation, 23(7):1661–1674, 2011. 1, 4

Xinyi Wei, Hans van Gorp, Lizeth Gonzalez-Carabarin, Daniel Freedman, Yonina C Eldar, and
Ruud JG van Sloun. Deep unfolding with normalizing flow priors for inverse problems. IEEE
Transactions on Signal Processing, 70:2962–2971, 2022. 1

Jay Whang, Qi Lei, and Alex Dimakis. Solving inverse problems with a flow-based noise model. In
International Conference on Machine Learning, pp. 11146–11157. PMLR, 2021. 2, 3, 6, 7, 8, 9,
15

11

Under review as a conference paper at ICLR 2023

(a) CelebA

(b) Out-of-distribution data

Figure 4: Quantitative results using PSNR (green) and SSIM (blue) for the compressed sensing with
sinusoidal noise experiment on 64×64 images of the (a) CelebA and (b) out-of-distribution datasets.

A ADDITIONAL EXPERIMENTS

The following section explores additional inverse problems with compressed sensing and structured
noise. The goal is to show the performance of the proposed method in a variety of settings.

A.1 STRUCTURED NOISE WITH COMPRESSED SENSING

The corruption process is defined by y = Ax + nsine with a random Gaussian measurement matrix
(cid:1)(cid:1) for each pixel k. The
A ∈ Rm×d and a noise with sinusoidal variance σk ∝ exp(cid:0)sin(cid:0) 2πk
subsampling factor is defined by the size of the measurement matrix d/m. In this experiment, the
score network sϕ is trained on a dataset generated with sinusoidal noise samples nsine. In Fig. 5 the
results of the compressed sensing experiment and the comparison with the baselines are shown for
an average standard deviation of σN = 0.2 and subsampling of factor d/m = 2. Given the same
hyperparameter settings, we repeat the experiment on the out-of-distribution (OoD) dataset, shown
in Fig. 6. Similar to the results found in Section 4.2.1, the diffusion method is more robust to the
shift in distribution and is able to deliver high quality recovery under the structured noise setting.
In contrast, the flow-based method under-performs when subjected to the OoD data. Quantitative
results on both CelebA and OoD are found in Fig. 4 as well as Table 1 and Table 2, respectively.

16

A.2 REMOVING SINUSOIDAL NOISE
The corruption process is defined by y = x + nsine where the noise variance σk ∝ exp(cid:0)sin(cid:0) 2πk
follows a sinusoidal pattern along each row of the image k. In this experiment, the score network sϕ
is trained on a dataset generated with 1D sinusoidal noise samples nsine. See Fig. 8 for a comparison
of our method to the flow-based method for varying noise variances. Both methods perform quite
well, with the diffusion method having a slight edge. A visual comparison in Fig. 7, however, reveals
that the diffusion method preserves more detail in general.

(cid:1)(cid:1)

16

12

LASSOGANFLOWDIFFUSION1015202530PSNR12.9318.924.9625.520.280.530.780.820.00.20.40.60.81.0SSIMLASSOGANFLOWDIFFUSION1015202530PSNR11.6312.3919.8622.90.340.160.610.820.00.20.40.60.81.0SSIMUnder review as a conference paper at ICLR 2023

Figure 5: Comparison of results from our diffusion method compared to the baselines on the com-
pressed sensing with sinusoidal noise experiment with d/m = 2, σN = 0.2 on 64 × 64 images of
the CelebA dataset.

Figure 6: Results for our diffusion-based method on the compressed sensing with sinusoidal noise
experiment on an out-of-distribution dataset, generated using stable diffusion (Rombach et al.,
2022).

13

GroundTruthDIFFUSIONFLOWGANLASSOGroundTruthDIFFUSIONFLOWGANLASSOUnder review as a conference paper at ICLR 2023

Figure 7: Comparison of results from our diffusion method compared to the baselines on the remov-
ing sinusoidal noise experiment with σN = 0.2 on 64 × 64 images of the CelebA dataset.

Figure 8: Comparison of PSNR values for varying sinusoidal noise variances. Shaded areas repre-
sent the standard deviation on the metric.

14

GroundTruthNoisyInputDIFFUSIONFLOWGANBM3D0.10.20.3Avg.NoiseStd.σN12141618202224PSNRFLOWDIFFUSIONUnder review as a conference paper at ICLR 2023

B EXTENDED RESULTS

B.1 METRICS

Table 1: Results for the experiments and different methods on the CelebA dataset.
⋆Ours, †Whang et al. (2021), ‡Bora et al. (2017), §Dabov et al. (2006), ¶Tibshirani (1996).

MNIST

CS + sine noise

PSNR

SSIM

PSNR

SSIM

⋆DIFFUSION 27.26 ± 1.925
†FLOW
22.90 ± 1.214
‡GAN
17.50 ± 1.404
§BM3D
11.56 ± 1.879
¶LASSO
-

0.865 ± 0.039
0.827 ± 0.051
0.486 ± 0.099
0.326 ± 0.059
-

25.51 ± 1.040
24.96 ± 2.292
18.90 ± 1.343
-
12.93 ± 1.819

0.823 ± 0.044
0.779 ± 0.082
0.529 ± 0.084
-
0.284 ± 0.037

Table 2: Results for the experiments and different methods on the out-of-distribution (OoD) dataset.
⋆Ours, †Whang et al. (2021), ‡Bora et al. (2017), §Dabov et al. (2006), ¶Tibshirani (1996).

MNIST

CS + sine noise

PSNR

SSIM

PSNR

SSIM

⋆DIFFUSION 22.87 ± 4.581
†FLOW
19.98 ± 1.946
‡GAN
13.06 ± 1.788
§BM3D
10.44 ± 1.446
¶LASSO
-

0.842 ± 0.110
0.824 ± 0.081
0.218 ± 0.088
0.274 ± 0.069
-

22.90 ± 1.568
19.85 ± 4.840
12.39 ± 1.693
-
11.62 ± 1.473

0.823 ± 0.082
0.608 ± 0.176
0.159 ± 0.070
-
0.336 ± 0.057

B.2 COMPUTATIONAL PERFORMANCE

Table 3: Inference performance benchmark for all methods.
⋆Ours, †Whang et al. (2021), ‡Bora et al. (2017), §Dabov et al. (2006), ¶Tibshirani (1996).

Number of trainable parameters

Inference time / image [ms]

⋆DIFFUSION
†FLOW
‡GAN
§BM3D

8.9M
25.8M
3.9M
–

1292
61853
59
28.5

[2.153 / it]
[103.1 / it]
[0.059 / it]

15

Under review as a conference paper at ICLR 2023

C PSEUDO-CODE

In this section, we provide pseudo-code for the proposed joint conditional diffusion sampler with
the Euler-Maruyama sampling algorithm as basis. Furthermore, we use the SDE formulation for the
diffusion process which is denoted as an sde object with drift, diffusion and marginal_prob
methods. The latter computes the mean and standard deviation of the diffusion transition ker-
nel at a certain time t. Lastly, there are two trained score networks (NCSNv2) score_data and
score_noise for the data and structured noise respectively.

def j o i n t _ c o n d _ d i f f u s i o n _ s a m p l e r (y , lambda_coeff , mu_coeff , num_steps ):

dt = 1/ num_steps
x = random . normal ( y . shape )
n = random . normal ( y . shape )

for t in linspace (1 , 0 , num_steps ):

# corrupt observation along the diffusion process
mean , std = sde . marginal_prob (y , t )
y_hat = mean + std * random . normal ( y . shape )

# data consistency step for x ( data )
x = x - lambda_coeff * A . T @ ( A @ x - y_hat + n )

# data consistency step for n ( noise )
n = n - mu_coeff * ( n - y_hat + A @ x )

# reverse diffusion step for x ( data )
z = random . normal ( x . shape )
x_hat = x - sde . drift ( t ) * x * dt
x_hat = x_hat + sde . diffusion ( t )**2 * score_data (x , t ) * dt
x_hat = x_hat + sde . diffusion ( t ) * sqrt ( dt ) * z
x = x_hat

# reverse diffusion step for n ( noise )
z = random . normal ( n . shape )
n_hat = n - sde . drift ( t ) * n * dt
n_hat = n_hat + sde . diffusion ( t )**2 * score_noise (n , t ) * dt
n_hat = n_hat + sde . diffusion ( t ) * sqrt ( dt ) * z
n = n_hat

# return the denoised sample x | y
return x

16

