Under review as a conference paper at ICLR 2023

TEST-TIME ADAPTATION FOR BETTER ADVERSARIAL
ROBUSTNESS

Anonymous authors
Paper under double-blind review

ABSTRACT

Standard adversarial training and its variants have been widely adopted in prac-
tice to achieve robustness against adversarial attacks. However, we show in this
work that such an approach does not necessarily achieve near optimal general-
ization performance on test samples. Speciﬁcally, it is shown that under suitable
assumptions, Bayesian optimal robust estimator requires test-time adaptation, and
such adaptation can lead to signiﬁcant performance boost over standard adver-
sarial training. Motivated by this observation, we propose a practically easy-to-
implement method to improve the generalization performance of adversarially-
trained networks via an additional self-supervised test-time adaptation step. We
further employ a meta adversarial training method to ﬁnd a good starting point for
test-time adaptation, which incorporates the test-time adaptation procedure into
the training phase and it strengthens the correlation between the pre-text tasks in
self-supervised learning and the original classiﬁcation task. Extensive empirical
experiments on CIFAR10, STL10 and Tiny ImageNet using several different self-
supervised tasks show that our method consistently improves the robust accuracy
of standard adversarial training under different white-box and black-box attack
strategies.

1

INTRODUCTION

Adversarial Training (AT) (Madry et al., 2018) and its variants (Wang et al., 2019; Zhang et al.,
2019) are currently recognized as the most effective defense mechanism against adversarial attacks.
However, AT generalizes poorly; the robust accuracy gap between the training and test set in AT is
much larger than the training-test gap in standard training of deep networks (Neyshabur et al., 2017;
Zhang et al., 2017). Unfortunately, classical techniques to overcome overﬁtting in standard training,
including regularization and data augmentation, only have little effect in AT (Rice et al., 2020).

Theoretically, as will be shown in Section 3, the loss objective of AT does not achieve optimal robust-
ness. Instead, under suitable assumptions, the Bayesian optimal robust estimator, which represents
the statistical optimal model that can be obtained from training data, requires test-time adaptation.
Compared with the ﬁxed restricted Bayesian robust estimators, the test-time adapted estimators
largely improve the robustness. Therefore, we should perform the test-time adaptation for each test
input to boost the robustness.

To this end, we propose to ﬁne-tune the model parameters for each test mini-batch. Since the labels
of the test images are not available, we exploit self-supervision, which is widely used in the standard
training of networks (Chen et al., 2020b; Gidaris et al., 2018; He et al., 2020). Fine-tuning the
self-supervised tasks has a high gradient correlation with ﬁne-tuning the classiﬁcation task so that
it forms a substitute of ﬁne-tuning the classiﬁcation loss at the inference time. Thus, we expect
minimizing this self-supervised loss function yields better generalization on the test set.

To make our test-time adaptation strategy effective, we need to search for a good starting point that
achieves good robust accuracy after ﬁne-tuning. As will be shown in our experiments, AT itself
does not provide the optimal starting point. We therefore formulate the search for such start point
as a bilevel optimization problem. Speciﬁcally, we introduce a Meta Adversarial Training (MAT)
strategy dedicated to our self-supervised ﬁne-tuning inspired by the model-agnostic meta-learning
(MAML) framework (Finn et al., 2017). To this end, we treat the classiﬁcation of each batch of
adversarial images as one task and minimize the corresponding classiﬁcation error of the ﬁne-tuned

1

Under review as a conference paper at ICLR 2023

network. MAT strengthens the correlation between the self-supervised and classiﬁcation tasks so
that self-supervised test-time adaptation can further improve robust accuracy.

In order to reliably evaluate our method, we follow the suggestions of (Tramer et al., 2020) and
design an adaptive attack that is fully aware of the test-time adaptation. Using rotation and ver-
tical ﬂip as the self-supervised tasks, we empirically demonstrate the effectiveness of our method
on the commonly used CIFAR10 (Krizhevsky et al., 2009), STL10 (Coates et al., 2011) and Tiny
ImageNet (Le & Yang, 2015) datasets under both standard (Andriushchenko et al., 2020; Croce &
Hein, 2020a; Madry et al., 2018) and adaptive attacks in both white-box and black-box attacks. The
experiments evidence that our method consistently improves the robust accuracy under all attacks.
Our contributions can be summarized as follows:

1. We show that the estimators should be test-time adapted in order to achieve the Bayesian optimal
adversarial robustness, even for simple models like linear models. And the test-time adaptation
largely improves the robustness compared with optimal restricted estimators.

2. We introduce the framework of self-supervised test-time ﬁne-tuning for adversarially-trained
networks, showing that it improves the robust accuracy of the test data.

3. We propose a meta adversarial training strategy based on the MAML framework to ﬁnd a good
starting point and strengthen the correlation between the self-supervised and classiﬁcation tasks.

4. The experiments show that our approach is valid on diverse attack strategies, including an adaptive
attack that is fully aware of our test-time adaptation, in both white-box and black-box attacks.

2 RELATED WORK

Adversarial Training. In recent years, many approaches have been proposed to defend networks
against adversarial attacks (Guo et al., 2018; Liao et al., 2018; Song et al., 2018). Among them,
Adversarial Training (AT) (Madry et al., 2018) stands out as one of the most robust and popu-
lar methods, even under various strong attacks (Athalye et al., 2018; Croce & Hein, 2020a). AT
optimizes the loss of adversarial examples to ﬁnd parameters that are robust to adversarial attacks.
Several variants of AT (Wang et al., 2019; Zhang et al., 2019) also achieved and similar performance
to AT (Rice et al., 2020). One important problem that limits the robust accuracy of AT is overﬁtting.
Compared with training on clean images, the gap of robust accuracy between the training and test set
is much larger in AT (Rice et al., 2020). Moreover, traditional techniques to prevent overﬁtting, such
as regularization and data augmentation, have little effect. Recently, some methods have attempted
to ﬂatten the weight loss landscape to improve the generalization of AT. In particular, Adversar-
ial Weight Perturbation (AWP) (Wu et al., 2020) achieves this by designing a double-perturbation
mechanism that adversarially perturbs both inputs and weights. In addition, learning-based smooth-
ing can ﬂatten the landscape and improve the performance (Chen et al., 2021b).

Self-supervised Learning. In the context of non-adversarial training, many self-supervised strate-
gies have been proposed, such as rotation prediction (Gidaris et al., 2018), region/component ﬁll-
ing (Criminisi et al., 2004), patch-base spatial composition prediction (Trinh et al., 2019) and con-
trastive learning (Chen et al., 2020b; He et al., 2020). While self-supervision has also been employed
in AT (Chen et al., 2020a; Kim et al., 2020; Yang & Vondrick, 2020; Hendrycks et al., 2019), their
methods only use self-supervised learning at training time to regularize the parameters and improve
the robust accuracy. By contrast, we propose to perform self-supervised ﬁne-tuning at test time,
which we demonstrate to signiﬁcantly improve the robust accuracy on test images. As will be
shown in the experiments, the self-supervised test-time adaptation has larger and complementary
improvements over the training time self-supervision.

Test-time Adaption. Test-time adaptation has been used in various ﬁelds, such as image super-
resolution (Shocher et al., 2018) and domain adaption (Sun et al., 2020; Wang et al., 2021). While
our work is thus closely related to Test-Time Training (TTT) in (Sun et al., 2020), we target a sig-
niﬁcantly different scenario. TTT assumes that all test samples have been subject to the same distri-
bution shift compared with the training data. As a consequence, it incrementally updates the model
parameters when receiving new test images. By contrast, in our scenario, there is no systematic
distribution shift, and it is therefore more effective to ﬁne-tune the parameters of the original model
for every new test mini-batch. This motivates our MAT strategy, which searches for the initial model
parameters that can be effectively ﬁne-tuned in a self-supervised manner.

2

Under review as a conference paper at ICLR 2023

3 THEORY OF TEST-TIME ADAPTATION

In this section, we study the relationship between the test-time adaptation and Bayesian optimal
robustness, which represents the optimal robustness that can be achieved from the training data,
showing that the test-time adaptation can extend the function classes and improve the robustness of
the model.
Deﬁnition 3.1. For a model F (x) with (cid:96)2 adversarial constraint (cid:107)x(cid:63) −x(cid:107) < ε, we deﬁne its natural
risk and adversarial risk as at point x

Rnat

x (F ) = (F (x) − E[y|x])2, Radv

x (F ) = Rnat

x (F ) + max

(cid:107)x(cid:63)−x(cid:107)<ε

(F (x(cid:63)) − F (x))2

Remarks. We use the MSE loss to deﬁne the natural risk Rnat
x (F ) at point
x. Similar to TRADES (Zhang et al., 2019) The adversarial risk is deﬁned as the sum of natural
risk and the loss changes under adversarial attack, and it can be bounded by the maximum MSE loss
within the adversarial budget

x (F ) and adversarial Radv

ExRadv

x (F ) ≤ Ex max

(cid:107)x(cid:63)−x(cid:107)≤ε

(F (x(cid:63)) − E[y|x])2 ≤ 2ExRadv

x (F ).

Therefore, for the adversarial input x(cid:63) with (cid:107)x(cid:63) − x(cid:107) < ε, small Radv
error on x(cid:63). Small Rnat
robustness respectively

x (F ) guarantee small test
x (F ) represents good clean performance and high adversarial

x (F ) and Radv

In the following deﬁnitions, we deﬁne three algorithms to obtain adversarially robust functions and
compare their adversarial risks.
Deﬁnition 3.2 (Adversarial Training with TRADES). We deﬁne ˆFAT as
(cid:20)
(yi − ˆF (xi))2 + max

ˆFAT = arg min

i ) − ˆF (xi))2

( ˆF (x(cid:63)

n
(cid:88)

(cid:21)

,

(cid:107)x(cid:63)

i −xi(cid:107)<ε

1
n

ˆF

i=1

where xi represents the i-th clean training data and yi represents the clean training label.

Remark. Empirically, adversarial training is a very popular method to achieve robustness. It mini-
mizes the adversarial risk on the training data. Then we consider the Bayesian optimal robustness.
Let F represent a function class. We assume the response is generated by y = F∗(x) + ξ with
prior distribution F∗ ∈ F ∼ PF and ξ ∼ Pξ. Denote X ∈ Rn×d, Y ∈ Rn as the training data
and training response generated by yi = F∗(xi) + ξ. For problems like Bayesian linear regression
(Bishop & Nasrabadi, 2006), function ˆF ∈ F is able to achieve the Bayesian optimal natural risk
EX,Y,y( ˆF (x) − y)2. However, the function class F is not enough to achieve the Bayesian optimal
adversarial risk. The adversarial risk depends on the local Lipschitz of function ˆF , in order to better
trade-off between the Lipschitz and natural risk of the function F , much more complicated func-
tion classes than F are needed to achieve the optimal adversarial robustness. We deﬁned the two
Bayesian functions FRB and FAB that minimize global adversarial risk and adversarial risk at the
speciﬁc point x. FAB extends the function class beyond F and achieves better robustness.
Deﬁnition 3.3 (Restricted Bayesian Robust Function FRB). The restricted Bayesian robust function
FRB minimizes the global adversarial risk inside the function class F

ExEy,X,Y|x

min
ˆF ∈F

(cid:20)
( ˆF (x) − y)2 + max

(cid:107)x(cid:63)−x(cid:107)<ε

( ˆF (x(cid:63)) − ˆF (x))2

(cid:21)

Remark. The Bayesian function represents the best robust function inside the function class F For
any F ∈ F, no training algorithms can achieve better average adversarial risk than FRB.
Deﬁnition 3.4 (Adaptive Bayesian Robust Function FAB). The adaptive Bayesian robust function
FAB inside the function class F that minimizes the adversarial risk at point x is
(cid:21)

Ey,X,Y|x

min
ˆF ∈F

(cid:20)
( ˆF (x) − y)2 + max

(cid:107)x(cid:63)−x(cid:107)<ε

( ˆF (x(cid:63)) − ˆF (x))2

3

Under review as a conference paper at ICLR 2023

Remark. Instead of minimizing the global average Radv
x (F ), FAB minimizes the adversarial risk
in the given input point x. The function depends on the input x so that the model extends the
function class beyond F. For different test inputs, we can use different functions to achieve the
optimal adversarial risk. Therefore, we refer to FAB as the test-time adapted function.

In the following theorem, we show the difference between three functions in the model, where the
test-adapted function FAB signiﬁcantly improves the robustness.
Theorem 3.1 (Linear Models). We consider a linear function classes FLin = {F Lin|F Lin(x; θ) =
x(cid:62)θ, θ ∈ Rd}. The output y is generated by y = x(cid:62)θ∗ + ξ, where θ∗ is independent of x with
θ∗ ∼ N(0, τ 2I), and the noise ξ ∼ N(0, σ2). Let X ∈ Rn×d, Y ∈ Rn denote the training data and
the responses respectively. For linear model F Lin(x; θ), three estimators in Deﬁnition 3.2 to 3.4 are
1
ε2d + 1

AB = (xx(cid:62) + ε2Id)−1xx(cid:62) (cid:98)θLin
(cid:98)θLin
nat ,

(cid:98)θLin
AT = X(X(cid:62)X + nε2In)−1Y,

(cid:98)θLin
RB =

(cid:98)θLin
nat ,

nat = X(X(cid:62)X + λ∗In)−1Y, λ∗ = σ2/τ 2. Furthermore, if each dimension of x is i.i.d.
dxi]4 ≤ M for some universal constant M < ∞, denoting

where (cid:98)θLin
with Ex = 0, Cov(x) = Id/d and E[
∆ = (1 + c + λ∗)2 − 4c, then when n, d → ∞ with n/d = c ∈ (0, 1)

√

Radv

x ( (cid:98)θLin

AT ) = τ 2, Radv

x ( (cid:98)θLin

RB ) = τ 2, Radv

x ( (cid:98)θLin

AB ) = τ 2

(cid:32)

1 −

1 + c + λ∗ −
2(ε2 + 1)

And when c → 1 with SNR= σ2/τ 2 ≤ 1, Radv

x ( (cid:98)θLin

AB ) < Radv

x ( (cid:98)θLin

RB )(1 − 2

3(ε2+1) ).

√

(cid:33)

∆

.

x ( (cid:98)θLin

x ( (cid:98)θLin

AT and (cid:98)θLin

AB ) < Radv

Remarks. In this theorem, we provide the form of the three estimators and their adversarial risks.
The gap of adversarial risk between (cid:98)θLin
RB vanishes when n, d → ∞. The estimator (cid:98)θLin
RB
achieves the optimal robust risk among all linear models. However, for an arbitrary ratio c =
n/d, Radv
RB ), indicating that adaptation to each test data x can improve the
robustness of the model even when compared with the best linear model.
Theorem 3.1 provides the optimal test-time adapted estimator in the linear function classes FLin,
which depends on the clean input x. In Figure 1, we plot the adversarial risk of three estimators for
different adversarial budgets, which clearly shows that our adaptation can signiﬁcantly increase
the robustness. When the input is corrupted with adversarial noise, the same form of the test-time
adapted estimator also signiﬁcantly improves the adversarial risk shown in the following theorem.
Theorem 3.2 (Corrupted Input). We assume the oracle parameter θ∗ is independent of x and has
the prior distribution θ∗ ∼ N(0, τ 2I), and the noise ξ ∼ N(0, σ2). Furthermore, each dimension of
√
x is i.i.d. with Ex = 0, Cov(x) = Id/d and E[
dxi]4 ≤ M for some universal constant M < ∞,
then when n, d → ∞ with n/d = c ∈ (0, 1). Given corrupted input x(cid:63) = x + ε (cid:98)θ/(cid:107) (cid:98)θ(cid:107), with ε < 1,
the adversarial risk of (cid:98)θLin

AB,(cid:63) = (x(cid:63)x(cid:63)(cid:62) + ε2Id)−1x(cid:63)x(cid:63)(cid:62)X(X(cid:62)X + λ∗In)−1Y is

Radv

x ( (cid:98)θLin

AB,(cid:63)) = τ 2

(cid:18)

1 − (1 − ε2 +

2ε2
(1 + ε)2 )

(1 + c + λ∗ +

√

2c
∆)(ε2/(1 + ε)2 + 1)

(cid:19)

< Radv

x ( (cid:98)θLin

RB ).

Remarks. The theorem shows that when the given input is
adversarial, the test-time adaptation can still lower the ad-
versarial risk of the model as Radv
AB,(cid:63)) <
Radv

AB) < Radv

x ( (cid:98)θLin

x ( (cid:98)θLin

x ( (cid:98)θLin

RB ).

In the statistical Bayesian model, we show that the test-
time adaptation can extend the function classes and
achieve the signiﬁcantly lower adversarial risk than the
ﬁxed model. In the practical non-Bayesian classiﬁcation task,
explicit calculation of the optimal model is difﬁcult. Nev-
ertheless, the test-time adaptation also helps to improve the
robustness of the model. As will be shown in the following
section, we perform the self-supervised test-time ﬁne-tuning
to adapt the model to each input, and largely improves the
robust accuracy of the test-time adapted model.

4

Figure 1: The comparison of Radv
x
for (cid:98)θLin
AB,(cid:63). We set
τ 2 = 1, σ2 = 0.2 and d = 250000.

RB , (cid:98)θLin

AB, (cid:98)θLin

AT , (cid:98)θLin

50000100000150000200000250000n0.00.20.40.60.81.0Adversarial Risk =0.10LinATLinRBLinABLinAB,*2035702036200.445950.44600Under review as a conference paper at ICLR 2023

4 METHODOLOGY

We follow the traditional multitask learning formulation (Caruana, 1997) and consider a neural net-
work with a backbone z = E(x; θE) and K + 1 heads. One head f (z; θf ) outputs the classiﬁcation
result while the other K heads g1(z; θg1), ..., gK(z; θgK) correspond to K auxiliary self-supervised
tasks. θ = (θE, θf , θg1, · · · , θgK) encompasses all trainable parameters, and we further deﬁne

F = f ◦ E; Gk = gk ◦ E, k = 1, 2, ..., K.

(1)

Furthermore, let D = {(xi, yi)}n
i=1 be the test
set. For further illustration, the labels of the test set are shown. However, they are unknown to the
networks at test time. We denote the adversarial examples of x as x(cid:63). It satisﬁes (cid:107)x(cid:63) − x(cid:107) ≤ ε, and
ε is the size of the adversarial budget. For any set S, we represent its average loss as

i=1 denote the training set, and (cid:101)D = {((cid:101)xi, (cid:101)yi)}m

L(S) =

1
|S|

(cid:88)

si∈S

L(si)

(2)

where |S| is the number of elements in S. The general classiﬁcation loss, such as the cross-entropy,
is denoted by Lcls. We use the superscript “AT” to denote the adversarial training loss. For example,

LAT

cls (S) =

1
|S|

(cid:88)

xi,yi∈S

max
i −xi(cid:107)≤ε

(cid:107)x(cid:63)

Lcls(F (x(cid:63)

i ), yi) .

(3)

4.1 SELF-SUPERVISED TEST-TIME FINE-TUNING

Our goal is to perform self-supervised learning on the test examples to mitigate the overﬁtting
problem of AT and adapt the model for each data point. To this end, let us suppose that an
adversarially-trained network with parameters θ0 receives a mini-batch of b adversarial test ex-
b , (cid:101)yb)} , As the labels {(cid:101)yi}b
amples (cid:101)B(cid:63) = {((cid:101)x(cid:63)
i=1 are not available, we propose to
ﬁne-tune the backbone parameters θE by optimizing the loss function

1, (cid:101)y1), · · · , ((cid:101)x(cid:63)

LSS( (cid:101)B(cid:63)) =

1
b

K
(cid:88)

Ck

b
(cid:88)

k=1

i=1

LSS,k(Gk((cid:101)x(cid:63)

i ); θE, θgk) ,

(4)

which encompasses K self-supervised tasks. Here, LSS,k represents the loss function of the k-th
task and {Ck}K
k=1 are the weights balancing the contribution of each task. In our experiments, the
LSS,K is the cross-entropy loss to predict rotation and vertical ﬂip.
The number of images b may vary from 1 to m. b = 1 corresponds to the online setting, where only
one adversarial image is available at a time, and the backbone parameters θE are adapted to every
new image. The online setting is the most practical one, as it does not make any assumptions about
the number of adversarial test images the network receives. By contrast, b = m corresponds to the
ofﬂine setting, where all adversarial test examples are available at once. It is similar to transductive
learning (Gammerman et al., 1998; Vapnik, 2013). Note that our online setting differs from the
online test-time training described in TTT (Sun et al., 2020); we do not incrementally update the
network parameters as new samples come, but instead initialize ﬁne-tuning from the same starting
point θ0 for each new test image.

Eqn (4) encourages θE to update in favor of the self-supervised tasks. However, as the classiﬁcation
head f was only optimized for the old backbone E(·; θ0
E), it will typically be ill-adapted to the
new parameters θ∗
E, resulting in a degraded robust accuracy. Furthermore, for a small b, the model
tends to overﬁt to the test data, reducing LSS to 0 but extracting features that are only useful for the
self-supervised tasks. To overcome these problems, we add an additional loss function acting on the
training data that both regularizes the backbone E and optimizes the classiﬁcation head f so that f
remains adapted to the ﬁne-tuned backbone E(·; θ∗
E). Speciﬁcally, let B ⊂ D denote a subset of
the training set. We then add the regularizer
1
|B|

LR(B) =LAT

Lcls(F (x(cid:63)

cls (B) =

i ), yi)

(cid:88)

(5)

(cid:107)x(cid:63)

max
i −xi(cid:107)≤ε

xi,yi∈B

to the ﬁne-tuning process. In short, Eqn (5) evaluates the AT loss on the training set to ﬁne-tune the
parameters θf of the classiﬁcation head. It also forces the backbone E to extract features that can

5

Under review as a conference paper at ICLR 2023

be used to make correct predictions, i.e., to prevent θE from being misled by LSS when b is small.
Combining Eqn (4) and Eqn (5), our ﬁnal test-time adaptation loss is

Ltest( (cid:101)B(cid:63), B) = LSS( (cid:101)B(cid:63)) + CLR(B)
(6)
where C sets the inﬂuence of LR. The algorithms that describe our test-time self-supervised learning
are deferred to Appendix D. As SGD is more efﬁcient for larger amount of data, we use SGD to
optimize θ when b is large (e.g. ofﬂine setting).

4.2 META ADVERSARIAL TRAINING

To make the best out of optimizing Ltest at test time, we should ﬁnd a suitable starting point θ0,
i.e., a starting point such that test-time self-supervised learning yields better robust accuracy. We
translate this into a meta learning scheme, which entails a bilevel optimization problem.
j=1Bj and let B(cid:63)
Speciﬁcally, we divide the training data into s small exclusive subsets D = ∪s
be adversaries of Bj. We then formulate meta adversarial learning as the bilevel minimization of

j to

Lmeta(D;θ) =

1
s

(cid:88)

Bj ⊂D

LAT

cls (Bj; θ∗

j (θ)), where θ∗

j = arg min

θ

LSS(B(cid:63)

j ; θ) ,

(7)

where LSS is the self-supervised loss function deﬁned in Eqn (4) and LAT
cls is the loss function of
AT deﬁned in Eqn (3). As bilevel optimization is time-consuming, following MAML (Finn et al.,
2017), we use a single gradient step of the current model parameters θ to approximate θ∗
j .

j ≈ θ − α∇θLSS(B(cid:63)
θ∗

j ; θ) .

(8)

In essence, this Meta Adversarial Training (MAT) scheme searches for a starting point such that
ﬁne-tuning with LSS will lead to good robust accuracy. If this holds for all training subsets, then we
can expect the robust accuracy after ﬁne-tuning at test time also to increase. Note that, because the
meta learning objective of Eqn (7) already accounts for classiﬁcation accuracy, the regularization by
LR is not needed during meta adversarial learning.
Accelerating Training. To compute the gradient ∇θLmeta(D; θ), we need to calculate the time-
LSS(B(cid:63)
consuming second order derivatives −α∇2
j ) . Considering that AT is
θ
already much slower than standard training (Shafahi et al., 2019), we cannot afford another signif-
icant training overhead. Fortunately, as shown in (Finn et al., 2017), second order derivatives have
little inﬂuence on the performance of MAML. We therefore ignore them and take the gradient to be

cls (Bj; θ∗

j ; θ)∇θ∗

LAT

j

∇θLmeta(D; θ) ≈

1
s

(cid:88)

Bj ⊂D

∇θ∗

j

LAT

cls (Bj; θ∗

j ) .

(9)

However, by ignoring the second order gradient, only the parameters on the forward path of the
classiﬁer F , i.e., θE and θf , will be updated. In other words, optimizing Eqn (7) in this fashion will
not update {θgk}K
k=1. To nonetheless encourage each self-supervised head Gk to output the correct
prediction, we incorporate an additional loss function encoding the self-supervised tasks,

LAT

SS (D) =

(cid:88)

k

CkLAT

SS,k(D) =

Ck
|D|

(cid:88)

k

(cid:88)

xi∈D

max
i −xi(cid:107)≤ε

(cid:107)x(cid:63)

LSS,k(Gk(x(cid:63)

i )) .

(10)

Note that we use the adversarial version of LSS to provide robustness to the self-supervised tasks,
which, as shown in (Chen et al., 2020a; Hendrycks et al., 2019; Yang & Vondrick, 2020), is beneﬁcial
for the classiﬁer. The ﬁnal meta adversarial learning objective therefore is

where C (cid:48) balances the two losses. Algorithm 1 shows the complete MAT algorithm.

Ltrain(D) = Lmeta(D) + C (cid:48)LAT

SS (D)

(11)

5 EXPERIMENTS

Experimental Settings. Following previous works (Cui et al., 2020; Huang et al., 2020), we con-
sider (cid:96)∞-norm attacks with an adversarial budget ε = 0.031(≈ 8/255). We evaluate our method

6

Under review as a conference paper at ICLR 2023

Algorithm 1 Meta Adversarial Training
Input: Training set D; Learning rate α, β; Iterations T ; Weights Ck and C (cid:48)
Output: Starting parameters θ0 for the test-time ﬁne-tuning
1: for t = 1 to T do
2:
3:

Sample q exclusive batches of training images B1, B2, · · · , Bq ⊂ D
Using PGD to ﬁnd the adversaries B(cid:63)
j,i−xj,i(cid:107)≤ε
for batches B1, B2, · · · , Bq do
j = θ − α∇θLSS(B(cid:63)
θ∗
j ; θ)
lmeta,j = LAT
cls (Bj; θ∗
j )
(cid:104)

4:
5:
6:
7:
8:
9: end for
10: return Trained parameters θ0 = θ

end for
θ = θ − β
q

lmeta,j + C (cid:48)∇θLAT

j,i = arg max(cid:107)x(cid:63)

SS (Bj; θ)

j : x(cid:63)

∇θ∗

(cid:80)

Bj

(cid:105)

j

Lcls(F (x(cid:63)

j,i), yj,i)

on three datasets: CIFAR10 (Krizhevsky et al., 2009), STL10 (Coates et al., 2011) and Tiny Im-
ageNet (Le & Yang, 2015). We also use two different network architectures: WideResNet-34-
10 (Zagoruyko & Komodakis, 2016) for CIFAR10, and ResNet18 (He et al., 2016) for STL10 and
Tiny ImageNet. The hyperparameters are provided in the Appendix D.

Self-Supervised Tasks. In principle, any self-supervised tasks can be used for test-time ﬁne-tuning,
as long as they are positively correlated with the robust accuracy. However, for the test-time ﬁne-
tuning to remain efﬁcient, we should not use too many self-supervised tasks. Furthermore, as we
aim to support the fully online setting, where only one image is available at a time, we cannot
incorporate a contrastive loss (Chen et al., 2020b; He et al., 2020; Kim et al., 2020) to LSS. In
our experiments, we therefore use two self-supervised tasks that have been shown to be useful to
improve the classiﬁcation accuracy: Rotation Prediction and Vertical Flip Prediction.

Attack Methods. In the white-box attacks, the attacker knows every detail of the defense method.
Therefore, we need to assume that the attacker is aware of our test-time adaptation method and will
adjust its strategy for generating adversarial examples accordingly. Suppose that the attacker is fully
aware of the hyperparameters for test-time adaptation. Then, ﬁnding adversaries (cid:101)B(cid:63) of the clean
subset (cid:101)B can be achieved by maximizing the adaptive loss
Lattack(F ((cid:101)x(cid:63)
(cid:101)x(cid:63)
i = arg max
(cid:107)(cid:101)x(cid:63)
i −xi(cid:107)≤ε

i ), y; θT ( (cid:101)B(cid:63))) ,

(12)

where Lattack refers to the general attack loss, such as the cross-entropy or the difference of logit
ratio (DLR) (Croce & Hein, 2020a). We call this objective in Eqn (12) adaptive attack, which can
be either performed in white-box or black-box attacks. We consider four common white-box and
black-box attack methods: PGD-20 (Madry et al., 2018), AutoPGD (both cross-entropy and DLR
loss) loss (Croce & Hein, 2020a), FAB (Croce & Hein, 2020b) and Square Attack (Andriushchenko
et al., 2020). We apply both the standard and adaptive versions of these methods. Particularly,
AutoPGD we use is a strong version that maximizes the loss function that continues when ﬁnding
adversarial examples (Croce et al., 2022). More details are provided in the Appendix E.
Baselines. We compare our method with the following methods: 1) Regular AT, which uses LAT
cls
in Eqn (3). 2) Regular AT with an additional self-supervised loss, i.e., using LAT
SS for AT,
where LAT
SS is given in Eqn (10). This corresponds to the formulation of (Hendrycks et al., 2019).
3) MAT (Algorithm 1) without test-time ﬁne-tuning.

cls + C (cid:48)LAT

5.1 ROBUST ACCURACY

CIFAR10. Table 1a shows the robust accuracy for different attacks and using two different tasks for
ﬁne-tuning. The adaptive attack is not applicable to models without ﬁne-tuning. As we inject dif-
ferent self-supervised tasks into the AT stage, and as different self-supervised tasks may impact the
robust accuracy differently (Chen et al., 2020a), the robust accuracy without ﬁne-tuning still varies.
The vertical ﬂipping task yields better robust accuracy before ﬁne-tuning but its improvement after
ﬁne-tuning is small. By contrast, rotation prediction achieves low robust accuracy before ﬁne-tuning,

7

Under review as a conference paper at ICLR 2023

Table 1: Robust accuracy on CIFAR10, STL10 and Tiny ImageNet of the test-time ﬁne-tuning on
both the online and the ofﬂine settings. We use an (cid:96)∞ budget ε = 0.031. FT stands for ﬁne-tuning.
We underline the accuracy of the strongest attack and highlight the highest accuracy among them.
(a) CIFAR10 with WideResNet-34-10.

Tasks Methods

Square Attack

PGD-20

AutoPGD

FAB

Worst

None AT

-
-
-

-
-
-

-
-
-

-
-
-

55.74%
56.64%
57.35%

StandardAdaptiveStandardAdaptiveStandardAdaptive GMSA StandardAdaptive
52.14%
62.51%
52.57%
AT w/o FT 63.54%
53.09%
MAT w/o FT 63.96%
Online FT
Ofﬂine FT
AT w/o FT 62.09%
MAT w/o FT 66.15%
Online FT
Ofﬂine FT
AT w/o FT 65.64%
MAT w/o FT 65.75%
Online FT
Ofﬂine FT

51.30%
51.85%
53.04%
65.52% 65.85% 59.52% 59.50% 57.93% 56.96% 57.60% 75.58% 77.69% 56.62%
78.12% 68.60% 57.21%
67.05% 65.75% 61.17% 59.71% 58.77% 57.63%
51.23%
-
51.24%
52.98%
-
53.02%
66.91% 66.16% 61.47% 59.40% 58.74% 56.79% 58.53% 75.68% 80.57% 55.98%
75.60% 72.24% 57.01%
67.23% 65.60% 61.82% 59.69% 59.26% 58.06%
52.95%
-
53.05%
53.76%
-
53.85%
67.34% 66.80% 61.79% 60.46% 59.23% 57.70% 59.60% 76.39% 79.80% 57.21%
76.89% 71.58% 57.88%
68.50% 66.05% 62.87% 60.54% 60.25% 58.26%

51.34%
51.87%
53.09%

59.19%
59.51%

55.50%
59.73%

53.16%
53.99%

52.79%
53.41%

-
-
-

-
-
-

-
-
-

-
-

-
-

-
-

-
-

-
-

-
-

-

Rotation

VFlip

Rotation
+
VFlip

Tasks Methods

Square Attack

PGD-20

AutoPGD

FAB

Worst

(b) STL10 with ResNet18.

None AT

Rotation
+
VFlip

Standard Adaptive Standard Adaptive Standard Adaptive Standard Adaptive
44.83%
AT w/o FT
44.00%
MAT w/o FT 44.75%
Online FT
Ofﬂine FT

35.58%
35.78%
33.65%
33.72%
35.31%
35.60%
45.07% 46.19% 40.31% 40.24% 39.53% 40.85% 51.25% 51.08% 39.21%
47.86% 48.03% 45.21% 43.33% 43.78% 43.20% 58.49% 54.13% 42.57%

37.89%
36.92%
38.66%

35.64%
33.73%
35.38%

-
-
-

-
-
-

-
-
-

-
-
-

Tasks Methods

Square Attack

PGD-20

AutoPGD

FAB

Worst

(c) Tiny ImageNet with ResNet18.

None AT

Rotation
+
VFlip

Standard Adaptive Standard Adaptive Standard Adaptive Standard Adaptive
28.5%
AT w/o FT
29.5%
MAT w/o FT 29.3%
Online FT
Ofﬂine FT

17.2%
17.5%
16.7%
17.1%
16.7%
16.9%
30.2% 30.2% 24.0% 23.2% 18.9% 18.1% 33.7% 31.6% 17.7%
32.4% 31.0% 25.6% 24.1% 23.7% 20.6% 36.5% 27.7% 20.1%

20.6%
22.2%
23.1%

17.2%
16.7%
16.8%

-
-
-

-
-
-

-
-
-

-
-
-

Table 2: The statistics ρ((cid:101)x(cid:63)) and two
self-supervised tasks. The dataset is CI-
FAR10 and the network is WideResNet-
34-10. Adversarial budget ε = 0.031
Tasks E(ρ((cid:101)x(cid:63))) P (ρ((cid:101)x(cid:63)) > 0)
Rotation
VFlip

68.51%
72.16%

0.15
0.22

Figure 2: Empirical cdf of ρ((cid:101)x(cid:63)
WideResNet-34-10. Adversarial budget ε = 0.031

i ) on CIFAR10 and

but its improvement after ﬁne-tuning is the largest. Using both tasks together combines their effect
and yields the highest overall accuracy after test-time adaptation. Note that our self-supervised test-
time ﬁne-tuning, together with meta adversarial learning, consistently improves the robust accuracy
under different attack methods. Under the strongest adaptive AutoPGD, test-time ﬁne-tuning using
both tasks achieves a robust accuracy of 57.70%, signiﬁcantly outperforming regular AT.

STL10 and Tiny ImageNet. As using both the rotation and vertical ﬂip prediction led to the highest
overall accuracy on CIFAR10, we focus on this strategy for STL10 and Tiny ImageNet. Table 1b
and 1c shows the robust accuracy on STL10 and Tiny ImageNet using a ResNet18. Our approach
also signiﬁcantly outperforms regular AT on these datasets.

Ofﬂine Test-time Adapattion. As shown in Table 1a, 1b, 1c, the ofﬂine ﬁne-tuning further improves
the robust accuracy over the online version.

8

0.50.00.51.0(xadv)0.00.20.40.60.81.0Rotation1.00.50.00.51.0(xadv)0.00.20.40.60.81.0Vertical FlipUnder review as a conference paper at ICLR 2023

Diverse Attacks. Recommended by (Croce et al., 2022), in Appendix C.1, we evaluate our method
on diverse attacks including transfer attack, expectation attack and boundary attack, where test-time
adaptation all improves the robustness of the model.

5.2 METHOD ANALYSIS

We observe the signiﬁcant positive correlation between the gradient of self-supervised loss LSS and
the classiﬁcation loss Lcls. Deﬁne

ρ((cid:101)x(cid:63)

i ) =

∇θE

Lcls((cid:101)x(cid:63)
Lcls((cid:101)x(cid:63)

i , (cid:101)yi)T ∇θE
i , (cid:101)yi)(cid:107)2(cid:107)∇θE

LSS((cid:101)x(cid:63)
i )
LSS((cid:101)x(cid:63)
i )(cid:107)2

(cid:107)∇θE

,

(13)

LSS((cid:101)x(cid:63)

i )) − Lcls((cid:101)x(cid:63)

i , (cid:101)yi; θE − η∇θE

i , (cid:101)yi; θE) ≈ −ηρ((cid:101)x(cid:63)

and approximate Lcls by the Taylor expansion
Lcls((cid:101)x(cid:63)
As θE contains millions of parameters, its gradient norm is typically large. Therefore, gradient
descent w.r.t. LSS should act as a good substitute for optimizing Lcls when ρ((cid:101)x(cid:63)
i ) is signiﬁcantly
larger than 0. We further conﬁrm this empirically. For all adversarial test inputs (cid:101)x(cid:63) ∼ (cid:101)D(cid:63), we regard
ρ((cid:101)x(cid:63)) as a random variable and calculate its empirical statistics on the test set. Table 2 shows the
empirical statistics of an adversarially-trained model on CIFAR10, and Figure 2 shows the c.d.f. of
ρ((cid:101)x(cid:63)). The mean of ρ((cid:101)x(cid:63)) is indeed signiﬁcantly larger than 0 and P (ρ((cid:101)x(cid:63)) > 0) is larger than the
robust accuracy of the adversarially-trained network (50%-60%), which implies that self-supervised
test-time ﬁne-tuning helps to correctly classify the adversarial test images.

i , (cid:101)yi)(cid:107)2(cid:107)∇θE

LSS((cid:101)x(cid:63)

Lcls((cid:101)x(cid:63)

i )(cid:107)∇θE

i )(cid:107)2 .

We further provide the theoretical analysis in a linear model in Theorem B.1, which shows that the
correlated gradient signiﬁcantly strengthens the robustness and lowers natural risk. Besides, the
correlated gradient also helps the model to move closer to the Bayesian robust estimator (cid:98)θAB.

5.3 ABLATION STUDY

Meta Adversarial Training. To show the effectiveness of MAT, we perform an ablation study to
ﬁne-tune the model with regular AT (i.e., setting α = 0 in line 5 of Algorithm 1). Table 7 shows that
the robust accuracy and the improvements of ﬁne-tuning are consistently worse without MAT.

Accuracy Improvement on Inputs with Different Adversarial Budget. As shown in Table 8, we
set ε = 0.015 to perform the online test-time ﬁne-tuning, showing that our method is also able to
improve the robust accuracy of inputs with different adversarial budgets.
Removing LSS or LR. To study the effect of LSS and LR in Ltest, we report the robust accuracy
after online ﬁne-tuning using only LR and LSS in Table 9. While, as expected, removing LSS tends
to reduce more accuracy than removing LR. It shows the beneﬁts of our self-supervised test-time
ﬁne-tuning strategy. Nevertheless, the best results are obtained by exploiting both loss terms.

Improvement on Clean Images. As predicted by Theorem 3.1 and B.1, our method is able to
improve not only the robust accuracy but also the natural accuracy. As shown in Table 10, our
approach increases the clean image accuracy by test-time adaptation. This phenomenon further
strengthens our conjecture that the improvement of robust accuracy is due to the improvement of
generalization instead of gradient masking.

6 CONCLUSION

In linear models and two-layer random networks, we theoretically demonstrate the necessity of test-
time adaptation for the model to achieve optimal robustness. To this end, we propose self-supervised
test-time ﬁne-tuning on adversarially-trained models to improve their generalization ability. Further-
more, we introduce a MAT strategy to ﬁnd a good starting point for our self-supervised ﬁne-tuning
process. Our extensive experiments on CIFAR10, STL10 and Tiny ImageNet demonstrate that our
method consistently improves the robust accuracy under different attack strategies, including strong
adaptive attacks where the attacker is aware of our test-time adaptation technique.
In these ex-
periments, we utilize three different sources of self-supervision: rotation prediction, vertical ﬂip
prediction and the ensemble of them.

9

Under review as a conference paper at ICLR 2023

REFERENCES

Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square at-
tack: a query-efﬁcient black-box adversarial attack via random search. In European Conference
on Computer Vision, pp. 484–501. Springer, 2020.

Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning, pp. 274–283. PMLR, 2018.

Zhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices,

volume 20. Springer, 2010.

Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, vol-

ume 4. Springer, 2006.

Rich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.

Jiefeng Chen, Xi Wu, Yang Guo, Yingyu Liang, and Somesh Jha. Towards evaluating the robustness
of neural networks learned by transduction. In International Conference on Learning Represen-
tations, 2021a.

Jinghui Chen and Quanquan Gu. Rays: A ray searching method for hard-label adversarial attack.
In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pp. 1739–1747, 2020.

Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial
robustness: From self-supervised pre-training to ﬁne-tuning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 699–708, 2020a.

Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overﬁtting
In International Conference on Learning

may be mitigated by properly learned smoothening.
Representations, 2021b. URL https://openreview.net/forum?id=qZzy5urZw9.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597–1607. PMLR, 2020b.

Chen Cheng, John Duchi, and Rohith Kuditipudi. Memorize to generalize: on the necessity of
interpolation in high dimensional linear regression. arXiv preprint arXiv:2202.09889, 2022.

Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artiﬁcial intelli-
gence and statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011.

Antonio Criminisi, Patrick P´erez, and Kentaro Toyama. Region ﬁlling and object removal by
IEEE Transactions on image processing, 13(9):1200–1212,

exemplar-based image inpainting.
2004.

Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In International Conference on Machine Learning, pp. 2206–
2216. PMLR, 2020a.

Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive
In International Conference on Machine Learning, pp. 2196–2205. PMLR,

boundary attack.
2020b.

Francesco Croce, Sven Gowal, Thomas Brunner, Evan Shelhamer, Matthias Hein, and Taylan
Cemgil. Evaluating the adversarial robustness of adaptive test-time defenses. arXiv preprint
arXiv:2202.13711, 2022.

Jiequan Cui, Shu Liu, Liwei Wang, and Jiaya Jia. Learnable boundary guided adversarial training.

arXiv preprint arXiv:2011.11164, 2020.

10

Under review as a conference paper at ICLR 2023

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, pp. 1126–1135. PMLR,
2017.

A. Gammerman, V. Vovk, and V. Vapnik. Learning by transduction. In Proceedings of the Fourteenth
Conference on Uncertainty in Artiﬁcial Intelligence, UAI’98, pp. 148–155, San Francisco, CA,
USA, 1998. Morgan Kaufmann Publishers Inc. ISBN 155860555X.

Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=S1v4N2l0-.

Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering
the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint
arXiv:2010.03593, 2020.

Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial
images using input transformations. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=SyJ7ClWCb.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
In Proceedings of the IEEE/CVF Conference on

unsupervised visual representation learning.
Computer Vision and Pattern Recognition, pp. 9729–9738, 2020.

Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learn-
ing can improve model robustness and uncertainty. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.
cc/paper/2019/file/a2b15837edac15df90721968986f7f8e-Paper.pdf.

Lang Huang, Chao Zhang, and Hongyang Zhang. Self-adaptive training: beyond empirical risk

minimization. In Advances in Neural Information Processing Systems, volume 33, 2020.

Hoki Kim, Woojin Lee, and Jaewook Lee. Understanding catastrophic overﬁtting in single-step
adversarial training. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35,
pp. 8119–8127, 2021.

Minseon Kim, Jihoon Tack, and Sung Ju Hwang. Adversarial self-supervised contrastive learning.

In Advances in Neural Information Processing Systems, 2020.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.

2009.

Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.

Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against
adversarial attacks using high-level representation guided denoiser. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 1778–1787, 2018.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.

Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring gener-
alization in deep learning.
In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/10ce03a1ed01077e3e289f3e53c72813-Paper.pdf.

11

Under review as a conference paper at ICLR 2023

Leslie Rice, Eric Wong, and Zico Kolter. Overﬁtting in adversarially robust deep learning.

In

International Conference on Machine Learning, pp. 8093–8104. PMLR, 2020.

Kuniaki Saito, Donghyun Kim, Stan Sclaroff, and Kate Saenko. Universal domain adaptation
through self supervision. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 16282–16292. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/bb7946e7d85c81a9e69fee1cea4a087c-Paper.pdf.

Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph
Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free!
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch´e-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
7503cfacd12053d309b6bed5c89de212-Paper.pdf.

Changhao Shi, Chester Holtz, and Gal Mishne. Online adversarial puriﬁcation based on self-

supervised learning. In International Conference on Learning Representations, 2020.

Assaf Shocher, Nadav Cohen, and Michal Irani. “zero-shot” super-resolution using deep internal
learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3118–3126, 2018.

Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. In Interna-
tional Conference on Learning Representations, 2018. URL https://openreview.net/
forum?id=rJUYGxbCW.

Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time train-
ing with self-supervision for generalization under distribution shifts. In International Conference
on Machine Learning, pp. 9229–9248. PMLR, 2020.

Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. In Advances in Neural Information Processing Systems, volume 33,
2020.

Trieu H Trinh, Minh-Thang Luong, and Quoc V Le. Selﬁe: Self-supervised pretraining for image

embedding. arXiv preprint arXiv:1906.02940, 2019.

Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media,

2013.

Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully
test-time adaptation by entropy minimization. In International Conference on Learning Repre-
sentations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c.

Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.

Improving
adversarial robustness requires revisiting misclassiﬁed examples. In International Conference on
Learning Representations, 2019.

Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust gener-

alization. Advances in Neural Information Processing Systems, 33, 2020.

Junfeng Yang and Carl Vondrick. Multitask learning strengthens adversarial robustness. In European

Conference on Computer Vision. Springer, 2020.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Edwin R. Hancock Richard
C. Wilson and William A. P. Smith (eds.), Proceedings of the British Machine Vision Conference
(BMVC), pp. 87.1–87.12. BMVA Press, September 2016. ISBN 1-901725-59-6. doi: 10.5244/C.
30.87. URL https://dx.doi.org/10.5244/C.30.87.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.

12

Under review as a conference paper at ICLR 2023

Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, pp. 7472–7482. PMLR, 2019.

13

Under review as a conference paper at ICLR 2023

A PROOFS OF THEOREMS

A.1 PRELIMINARY: MARCHENKO-PASTUR LAW AND TRANSFORMATION OF EIGENVALUES

Before the proof linear models, we ﬁrst give the asymptotic spectrum of the matrix K = X(cid:62)X, and
some useful results of the trace of the transformation of K.

Lemma A.1 (Marchenko-Pastur Law, Theorem 3.4 in (Bai & Silverstein, 2010)). We deﬁne the
eigenvalues of X(cid:62)X: λ1 > λ2 > · · · λn has distribution with c.d.f.

Hn(s) =

1
n

n
(cid:88)

i=1

1λi≤s.

If each dimension of x is i.i.d. with Ex = 0, Cov(x) = Id/d and E[
dxi]4 ≤ M for some universal
constant M < ∞, then when n, d → ∞ with n/d = c ∈ (0, 1), for any bounded function g, when
n, d → ∞ with c = n/d ∈ [0, ∞)

√

(cid:90)

g(s)dHn(s) →

(cid:90)

g(s)dH(s),

with p.d.f. dH(s)

dH(s) =

1
2πc
c)2 and λ+ = (1 +

√

√

c)2.

(cid:112)(λ+ − s) (s − λ−)
s

1s∈[λ−,λ+]ds,

where λ− = (1 −
Lemma A.2. If each dimension of x is i.i.d. with Ex = 0, Cov(x) = Id/d and E[
some universal constant M < ∞, then when n, d → ∞ with n/d = c ∈ (0, 1),

√

dxi]4 ≤ M for

Tr(K(K + λIn)−2) = d

√

1 + c + λ −
∆
2

√

∆

,
√

Tr(K2(K + λIn)−2) = d

(1 + c + λ) −

2

∆

(1 −

λ
√
∆

),

Tr((K + λIn)−2) = d

where ∆ = (1 + c + z)2 − 4c.

(1 + c)(1 + c + λ) − 4c − (1 − c)
2λ2

√

∆

√

∆

,

Proof. We deﬁne three transformations of the eigenvalue of dH(s)

(cid:90)

t1(z) =

s

(s + z)2 dH(s),

(cid:90)

t2(z) =

s2

(s + z)2 dH(s),

(cid:90)

t3(z) =

1

(s + z)2 dH(s),

(14)

for z ∈ [0, ∞). They can be calculated by the Stieltjes transformation of Marchenko-Pastur law.

By Marchenko-Pastur semicircular law, the Stieltjes transformation of dH(s) is (Lemma 3.11 in
(Bai & Silverstein, 2010) and Lemma 4.4 in (Cheng et al., 2022))

m(z) =

(cid:90)

1
s − z

dH(s) =

1 − c − z − (cid:112)(1 + c − z)2 − 4c
2cz

Let ∆ = (1 + c + z)2 − 4c

and

t(z) = m(−z) =

1 − c + z − (cid:112)(1 + c + z)2 − 4c
−2cz

,

(cid:90)

1

(s + z)2 dH(s) = −

(cid:90)

d
dz

1
s + z

dH(s) = −

dt(z)
dz

.

14

Under review as a conference paper at ICLR 2023

Then

(cid:90)

t1(z) =

(cid:90)

dH(s) −

z

dt(z)
dz

=

1 + c + z −
∆
2c

√

√

∆

,

(s + z)2 dH(s) = t(z) + z
(cid:90)

z2

(s + z)2 dH(s) = 1 − 2zt(z) − z2 dt(z)

dz

1
s + z
(cid:90)

2z
s + z

t2(z) =1 −

dH(s) +

(1 + c + z) −

=

2c

(cid:90)

d
dz

1
s + z

t3(z) = −

√

∆

(1 −

dH(s) =

),

z
√
∆
dt(z)
dz

=

(1 + c)(1 + c + z) − 4c − (1 − c)
2cz2

∆

√

√

∆

.

(15)

(16)

(17)

The trace operation can be translated into:

Tr(K(K + λIn)−2) = n

(cid:90)

s

(s + λ)2 dHn(s),

Tr(K2(K + λIn)−2) = n

Tr((K + λIn)−2) = n

(cid:90)

Therefore, when n, d → ∞ with c = n/d ∈ (0, 1),

(cid:90)

s2

(s + λ)2 dHn(s),
1

(s + λ)2 dHn(s).

Tr(K(K + λIn)−2) →n

1 + c + λ −
∆
2c

√

Tr(K2(K + λIn)−2) →n

(1 + c + λ) −

2c

√

∆

,

√

∆

= d

√

∆

1 + c + λ −
√
2
∆
λ
√
∆

) = d
√

(1 −

Tr((K + λIn)−2) →n

=d

√

(1 + c)(1 + c + λ) − 4c − (1 − c)
2cλ2
(1 + c)(1 + c + λ) − 4c − (1 − c)
2λ2

√

∆

∆

∆

√

∆

.

(1 + c + λ) −

√

∆

2

(1 −

λ
√
∆

),

where ∆ = (1 + c + λ)2 − 4c.

A.2 PROOF OF THEOREM 3.1

We separate the proof into three parts for three estimators separately.

Proof of (cid:98)θLin

AT . For (cid:98)θAT

Lin, taking gradient with respect to the objective function

1
n

n
(cid:88)

i=1

(cid:2)(yi − x(cid:62)

i θ) + ε2(cid:107)θ(cid:107)2(cid:3) ,

2
n

(Y − X(cid:62)θ) + 2nεθ = 0.

AT = X(X(cid:62)X + nε2In)−1Y
(cid:98)θLin

we obtain

Therefore,

Its natural risk is

Rnat

x ( (cid:98)θLin

AT ) =Eξ,θ∗,x(x(cid:62)( (cid:98)θLin

AT − θ∗))2 =

1
d

(cid:107) (cid:98)θLin

AT − θ∗(cid:107)2

=

1
d

Eξ,θ∗ (cid:107)θ∗ − X(X(cid:62)X + λIn)−1(X(cid:62)θ∗ + ξ)(cid:107)2

(18)

=τ 2(1 −

n
d

) +

λ2τ 2
d

Tr((K + λIn)−2) +

σ2
d

Tr(K(K + λIn)−2)

15

Under review as a conference paper at ICLR 2023

And the Lipschitz constant is

L( (cid:98)θLin

AT )2 =(cid:107) (cid:98)θLin

AT (cid:107)2

=Eξ,θ∗ (cid:107)(XX(cid:62) + λId)−1X(X(cid:62)θ∗ + ξ)(cid:107)2
=τ 2Tr (cid:0)(K + λIn)−2K2(cid:1) + σ2Tr (cid:0)(K + λIn)−2K(cid:1) .

When λ = nε2 → ∞, by Lemma A.2,

Tr(K(K + λIn)−2) → 0, Tr(K2(K + λIn)−2) → 0,

λ2Tr((K + λIn)−2) →

Therefore,

Radv

x ( (cid:98)θLin

AT ) = Rnat

x ( (cid:98)θLin

AT ) + ε2L( (cid:98)θLin

AT ) → τ 2

(19)

(20)

(21)

n
d

Lemma A.3. If the oracle parameter θ∗ is independent of x and has the prior distribution θ∗ ∼
N(0, τ 2I), and the noise ξ ∼ N(0, σ2). Furthermore, if we assume each dimension of x is i.i.d.
with Ex = 0, Cov(x) = Id/d and E[
dxi]4 ≤ M for some universal constant M < ∞, when
n, d → ∞ with n/d = c ∈ (0, 1), then for λ∗ = σ2/τ 2 and (cid:98)θ = 1

A X(X(cid:62)X + λ∗In)Y ,

√

Rnat

x ( (cid:98)θ) = τ 2 −

τ 2
A

((1 + c + λ∗) −

√

∆)(1 −

1
2A

),

L( (cid:98)θ)2 =

dτ 2
2A2 ((1 + c + λ∗) −

where ∆ = (1 + c + λ∗)2 − 4c.

√

∆),

Proof.

Rnat

x ( (cid:98)θ) =

=

1
d
τ 2
d

Eξ,θ∗ (cid:107)θ∗ −
(cid:32)(cid:18)

Tr

Id −

1
A
1
A

X(X(cid:62)X + λ∗In)−1(X(cid:62)θ∗ + ξ)(cid:107)2

X(K + λ∗In)−1X(cid:62)

(cid:19)2(cid:33)

+

σ2
d

Tr

(cid:18) 1
A2 X(K + λ∗In)−2X(cid:62)

(cid:19)

τ 2
dA2 Tr (cid:0)K2(K + λ∗In)−2(cid:1) +

(22)

=

) +

(d −

2n
A

2τ 2λ∗
dA

τ 2
d
σ2
dA2 Tr (cid:0)(K + λ∗In)−2K(cid:1) .
According to Lemma A.2, when n, d → ∞ with c = n

Tr (cid:0)(K + λ∗In)−1(cid:1) +

Tr((K + λ∗In)−1) = d

Tr(K(K + λ∗In)−2) = d

d ∈ (0, 1),
√

1 − c + λ∗ −

∆

−2λ∗
1 + c + λ∗ −
√
2

∆

√

∆

Tr(K2(K + λ∗In)−2) = d

(1 + c + λ∗) −

2

where λ∗ = σ2/τ 2 and ∆ = (1 + c + λ∗)2 − 4c. Therefore,

√

∆

(1 −

λ∗√
∆

),

Rnat

x ( (cid:98)θlin

bayes) = τ 2 −

τ 2
A

((1 + c + λ∗) −

√

∆)(1 −

1
2A

).

And

L( (cid:98)θ)2 =Eξ,θ∗ (cid:107)

X(X(cid:62)X + λ∗In)−1(X(cid:62)θ∗ + ξ)(cid:107)2

1
A
(cid:16)(cid:0)X(K + λ∗In)−1X(cid:62)(cid:1)2(cid:17)

τ 2
A2 Tr
τ 2
A2 Tr (cid:0)K2(K + λ∗In)−2(cid:1) +

=

=

σ2
A2 Tr (cid:0)(K + λ∗In)−2K(cid:1) .

16

+

σ2
A2 Tr

(cid:18) 1
A2 X(K + λ∗In)−2X(cid:62)

(cid:19)

(23)

Under review as a conference paper at ICLR 2023

Therefore,

L( (cid:98)θ) =

dτ 2
2A2 ((1 + c + λ∗) −

√

∆).

Proof of (cid:98)θLin

RB . It is well known that the posterior distribution of θ∗ is

θ∗|X, Y ∼ N(X(X(cid:62)X + λ∗In)−1Y, σ2(XX(cid:62) + λ∗Im)−1),

where λ∗ = σ2/τ 2. According to the deﬁnition of Bayesian estimator,

(cid:98)θLin
RB = arg min

Eθ∗|XEx

(cid:16)

(x(cid:62)( (cid:98)θ − θ∗))2 + ε2(cid:107) (cid:98)θ(cid:107)2(cid:17)

.

(cid:98)θ

As the linear model only allows ﬁxed (cid:98)θ for each x, we obtain

d
d (cid:98)θ

Eθ∗|X

(cid:18) 1
d

(cid:107) (cid:98)θ − θ∗(cid:107)2 + ε2(cid:107) (cid:98)θ(cid:107)2

(cid:19)

=Eθ∗|X

=Eθ∗|X

(cid:18) 1
d

d
d (cid:98)θ
(cid:18) 2
d

(cid:107) (cid:98)θ − θ∗(cid:107)2 + ε2(cid:107) (cid:98)θ(cid:107)2

(cid:19)

( (cid:98)θ − θ∗) + 2ε2 (cid:98)θ

(cid:19)

= 0.

Therefore,

(cid:98)θLin
RB =

1
ε2d + 1

Eθ∗|Xθ∗ =

1
ε2d + 1

X(X(cid:62)X + λ∗In)−1Y.

Using Lemma A.3, when d → ∞

Rnat

x ( (cid:98)θLin

RB ) =τ 2 −

τ 2
ε2d + 1

((1 + c + λ∗) −

√

∆)

2ε2d + 1
2(ε2d + 1)

→ τ 2,

L( (cid:98)θLin

RB )2 =

dτ 2

2(ε2d + 1)2 ((1 + c + λ∗) −

√

∆) → 0.

Summarizing the results,

Rnat

x ( (cid:98)θLin

RB ) = Rnat

x ( (cid:98)θLin

RB ) + ε2L( (cid:98)θLin

RB )2 → τ 2

(24)

(25)

(26)

(27)

Proof of (cid:98)θLin

AB . Bayesian robust estimator of each x, which optimizes (x(cid:62)( (cid:98)θ − θ∗))2 + ε2(cid:107) (cid:98)θ(cid:107)2 is

(cid:98)θLin
AB = arg min

Eθ∗|X

(cid:16)

(x(cid:62)( (cid:98)θ − θ∗))2 + ε2(cid:107) (cid:98)θ(cid:107)2(cid:17)

,

(cid:98)θ

where it is well known that the posterior distribution of θ∗ is

θ∗|X, Y ∼ N(X(X(cid:62)X + λ∗In)−1Y, σ2(XX(cid:62) + λ∗Im)−1).

Taking the gradient w.r.t (cid:98)θ gives the solution

AB(x) =(xx(cid:62) + ε2Id)−1xx(cid:62)X(X(cid:62)X + λ∗In)−1Y
(cid:98)θLin

=

xx(cid:62)
ε2 + x(cid:62)x

(cid:98)θLin
nat ,

(28)

where (cid:98)θLin
natural risk.

nat = X(X(cid:62)X + λ∗In)−1(X(cid:62)θ∗ + ξ) with λ∗ = σ2/τ 2 is the Bayesian estimator for

For its natural risk Rnat

x ( (cid:98)θLin

AB),

Rnat

x ( (cid:98)θLin

AB) = Eθ∗,ξEx(x(cid:62)( (cid:98)θLin

AB − θ∗))2

As

x(cid:62) (cid:98)θ =

x(cid:62)xx(cid:62)
ε2 + x(cid:62)x

nat = x(cid:62)
(cid:98)θLin

(cid:18) x(cid:62)x

ε2 + x(cid:62)x

(cid:19)

,

(cid:98)θLin
nat

17

Under review as a conference paper at ICLR 2023

then

Rnat

x ( (cid:98)θLin

AB) = Eθ∗,ξEx(x(cid:62)(

x(cid:62)x
ε2 + x(cid:62)x

nat − θ∗))2.
(cid:98)θLin

When d → ∞, x(cid:62)x → 1 in probability. Therefore,

x(cid:62)x
ε2 + x(cid:62)x

→

1
1 + ε2 .

Then when d → ∞,

By Lemma A.3,

Rnat

x ( (cid:98)θLin

AB) = Eθ∗,ξ(cid:107)

1
ε2 + 1

nat − θ∗(cid:107)2
(cid:98)θLin

Rnat

x ( (cid:98)θLin

AB) = τ 2 −

τ 2
ε2 + 1

((1 + c + λ∗) −

√

∆)(1 −

1
2(ε2 + 1)

)

As x(cid:62)x → 1 in probability when d → ∞,

x(cid:62)x

(ε2+x(cid:62)x)2 → 1

(ε2+1)2 . Then

L( (cid:98)θLin

AB)2 →

1
d(ε2 + 1)2

Eθ∗,ξ(cid:107) (cid:98)θLin

nat (cid:107)2.

(29)

(30)

(31)

From Lemma A.3,

Eθ∗,ξ(cid:107) (cid:98)θLin

nat (cid:107)2 = dτ 2 (1 + c + λ∗) −

2

√

∆

.

Summarizing two parts, the adversarial risk is

Radv

x ( (cid:98)θLin

AB) =τ 2 −

τ 2
2(ε2 + 1)

((1 + c + λ∗) −

√

∆) ≤ τ 2(1 −

c
(ε2 + 1)(1 + c + λ∗)

).

A.3 PROOF OF THEOREM 3.2

Proof. As (cid:98)θLin

AB = xx(cid:62) (cid:98)θLin

nat

ε2+x(cid:62)x , the adversarial input

x(cid:63) = ε

(cid:98)θLin
AB
(cid:107) (cid:98)θLin
AB(cid:107)

= x + ε

x
(cid:107)x(cid:107)

As d → ∞, (cid:107)x(cid:107) → 1 in probability. Therefore, x(cid:63) = (1 + ε)x. Taking it into

AB,(cid:63) = (x(cid:63)x(cid:63)(cid:62) + ε2Id)−1x(cid:63)x(cid:63)(cid:62)X(X(cid:62)X + λ∗In)−1Y,
(cid:98)θLin

we obtain

From Eqn (30) and 31,

(cid:98)θLin
AB,(cid:63) =

(1 + ε)2xx(cid:62) (cid:98)θLin
nat
ε + (1 + ε)2x(cid:62)x

Rnat

x ( (cid:98)θLin

AB,(cid:63)) =τ 2

(cid:18)

1 −

(1 + ε)2

√

(cid:19)

∆)

+

ε2 + (1 + ε)2 (1 + c + λ∗ −
√

τ 2(1 + ε)4

2(ε2 + (1 + ε)2)2 (1 + c + λ∗ −
√

τ 2(1 + ε)4

2(ε2 + (1 + ε)2)2 (1 + c + λ∗ −

∆)

∆).

L( (cid:98)θLin

AB,(cid:63))2 =

Therefore,

Radv

x ( (cid:98)θLin

AB,(cid:63)) =Rnat

x ( (cid:98)θLin

AB,(cid:63)) + ε2L( (cid:98)θLin

AB,(cid:63))2 = τ 2

(cid:32)

1 − (1 − ε2 +

2ε2
(1 + ε)2 )

√

∆
1 + c + λ∗ −
2(ε2/(1 + ε)2 + 1)

(cid:33)

18

Under review as a conference paper at ICLR 2023

B CORRELATED GRADIENTS

In the following Theorem, we show that with correlated gradient, one gradient descent step like our
method largely improves the natural and adversarial risk of the model.
Theorem B.1. We assume the oracle parameter θ∗ is independent of x and has the prior distribution
θ∗ ∼ N(0, τ 2I), and the noise ξ ∼ N(0, σ2). Let (cid:98)θ0 = (cid:98)θAT be the estimator of adversarial training
of the linear model F Lin(x; θ):

(cid:98)θ0 = X(X(cid:62)X + nε2In)−1Y.
When receiving a new test data point (x(cid:63), y) and taking one gradient descent step with cor-
related gradient (cid:98)g: (cid:98)θ1 = (cid:98)θ0 − ηˆg, where (cid:107)x(cid:63) − x(cid:107) ≤ ε is an adversarial example of x,
(cid:98)θ0L(F Lin(x, (cid:98)θ0), y)) = ρ > 0 and η is the learning rate. Let x = (x1, · · · , xd) where xi
Corr(ˆg, ∇
i=1 are i.i.d. with E[xi] = 0, Var[xi] = 1/d.
is the i-th element of x. We further assume that {xi}d
And E[
dxi]4 ≤ M for some universal constant M < ∞. When n, d → ∞ with c = n/d ∈ (0, 1),
with the optimal learning rate,

√

Rnat

x ( (cid:98)θ0) − Rnat

x ( (cid:98)θ1) ≥

τ 2ρ2
(((1 + ε)2 + σ2/τ 2)(1 + ε)2(1 + ε)2)2 −

Radv

x ( (cid:98)θ0) − Radv

x ( (cid:98)θ1) ≥

2τ 2ρ2
((1 + ε)2 + σ2/τ 2)(1 + ε)2(1 + ε)2
τ 2ρ2(1 − ε)2
((1 + ε)2 + σ2/τ 2)(1 + ε)2(1 + ε2)

.

Remarks. With correlated gradients, improvements of Rnat
x are both positive when having
ρ > 0. By taking correlated gradient descent on the parameter, we get large improvements of both
natural and adversarial risks.

x and Radv

Theorem B.1 shows that ﬁne-tuning with correlated gradient largely improves both clean perfor-
mance and robustness of the models. In addition, for linear models, the Bayesian optimal estimator
AB (cid:107) x. And (cid:98)θ1 (cid:107) ˆg with Corr(x(cid:63), ˆg) = ρ. As x(cid:63) is close to x, with a proper learning rate, we
is (cid:98)θLin
can get close to Bayesian robust estimator with correlated gradient descent.

B.1 PROOF OF THEOREM B.1

Proof. For a new test input (x(cid:63), y), where (cid:107)x(cid:63) − x(cid:107) ≤ ε is an adversarial example near the input,
its MSE loss is

Taking the gradient w.r.t θ

L(x, y, θ) =

1
2

(x(cid:63)(cid:62)θ∗ + ξ − x(cid:63)(cid:62)θ)2.

∇Lθ(x(cid:63), y, θ) = (x(cid:63)(cid:62)θ − x(cid:63)(cid:62)θ∗ − ξ)x(cid:63).

Suppose the self-supervised task gives a correlated version of gradient and updates (cid:98)θ0 with one step
of gradient descent

(cid:98)θ1 = (cid:98)θ0 − η[(x(cid:63)(cid:62) (cid:98)θ0 − x(cid:63)(cid:62)θ∗ − ξ)ˆg],
where Corr(ˆg, x(cid:63)) = ρ and E[ˆg(cid:62)ˆg] = E[x(cid:63)(cid:62)x(cid:63)]. From Theorem 3.1, when λ = nε2 → ∞,
Rnat

x ( (cid:98)θ0) and (cid:107) (cid:98)θ0(cid:107)2 can be simpliﬁed as:

Eθ∗,ξ(cid:107) (cid:98)θ0(cid:107)2 =

τ 2(1 + c) + σ2
ε4d

+ o(

1
d

) → 0, Rnat

x ( (cid:98)θ0) = Eξ,θ∗ (cid:107) (cid:98)θ0 − θ∗(cid:107)2 → τ 2.

Therefore, when d → ∞

Then when d → ∞,

(cid:98)θ1 → η[(x(cid:63)(cid:62)θ∗ + ξ)ˆg].

Ex(cid:107) (cid:98)θ1(cid:107)2 =η2θ(cid:62)
∗

Ex[ˆg(cid:62)ˆgx(cid:63)(cid:62)x(cid:63)]θ∗ + η2ξ2Ex[ˆg(cid:62)ˆg] + 2η2ξθ(cid:62)
∗

Ex[ˆg(cid:62)x(cid:63)].

19

Under review as a conference paper at ICLR 2023

Therefore,

Ex,θ∗,ξ(cid:107) (cid:98)θ1(cid:107)2 =η2 (cid:0)τ 2Ex[ˆg(cid:62)ˆgx(cid:63)(cid:62)x(cid:63)] + σ2Ex[ˆg(cid:62)ˆg](cid:1) .

By decomposing ˆg = ρx(cid:63) + (cid:112)1 − ρ2z∗, we can obtain,

Ex,θ∗,ξ(cid:107) (cid:98)θ1(cid:107)2 =η2 (cid:0)τ 2Ex[(x(cid:63)(cid:62)x(cid:63))2] + σ2Ex[x(cid:63)(cid:62)x(cid:63)](cid:1) .

As (cid:107)x(cid:63) − x(cid:107) ≤ ε,

For natural risk

Ex,θ∗,ξ(cid:107) (cid:98)θ1(cid:107)2 ≤η2 (cid:0)τ 2(1 + ε)4 + σ2(1 + ε)2(cid:1) .

Rnat

x ( (cid:98)θ1) =Ex,ξ,θ∗ (cid:107)x(cid:62)(θ∗ − (cid:98)θ1)(cid:107)2

→τ 2 + η2τ 2Ex[x(cid:63)(cid:62)x(cid:63)(x(cid:62)ˆg)2] + η2σ2Ex[(x(cid:62)ˆg)2] − 2ητ 2E[x(cid:62)x(cid:63)x(cid:62)ˆg].

By (cid:107)x(cid:63) − x(cid:107) ≤ ε, Corr(ˆg, x(cid:63)) = ρ and E[ˆg(cid:62)ˆg] = E[x(cid:63)(cid:62)x(cid:63)],

Rnat

x ( (cid:98)θ1) ≤τ 2 + η2τ 2(1 + ε)4 + η2σ2(1 + ε)2 − 2ητ 2ρ(1 − ε)2.

Therefore,

Radv

x ( (cid:98)θ1) ≤τ 2 + η2τ 2(1 + ε2)(1 + ε)4 + η2σ2(1 + ε2)(1 + ε)2 − 2ητ 2ρ(1 − ε)2.

Optimizing η get

With η∗,

and

η∗ =

ρτ 2(1 − ε)2
(τ 2(1 + ε)4 + σ2(1 + ε)2)(1 + ε2)

.

Radv

x ( (cid:98)θ1) ≤ τ 2(1 −

ρ2(1 − ε)2
((1 + ε)2 + σ2/τ 2)(1 + ε)2(1 + ε2)

),

(32)

(33)

Rnat

x ( (cid:98)θ1) ≤τ 2 −

τ 2ρ2
(((1 + ε)2 + σ2/τ 2)(1 + ε)2(1 + ε)2)2 −

2τ 2ρ2
((1 + ε)2 + σ2/τ 2)(1 + ε)2(1 + ε)2 .
(34)

Compared with (cid:98)θ0

we have improvements of

Radv

x ( (cid:98)θ0) =τ 2, Rnat

x ( (cid:98)θ0) = τ 2,

Rnat

x ( (cid:98)θ0) − Rnat

x ( (cid:98)θ1)

≥

τ 2ρ2
(((1 + ε)2 + σ2/τ 2)(1 + ε)2(1 + ε)2)2 −

2τ 2ρ2
((1 + ε)2 + σ2/τ 2)(1 + ε)2(1 + ε)2

and

Radv

x ( (cid:98)θ0) − Radv

x ( (cid:98)θ1) ≥

τ 2ρ2(1 − ε)2
((1 + ε)2 + σ2/τ 2)(1 + ε)2(1 + ε2)

.

(35)

(36)

C ADDITIONAL EXPERIMENTS

C.1 DIVERSE ATTACKS

Transfer Attack.
In Table 3, we perform a transfer attack from the static adversarial defense.
We use the robust networks with the same architecture as the substitute model, and the test-time
adaptation also improves the robust accuracy.

Expectation Attack. In Table 4, we show the results of the expectation attack. We modify the adap-
tive attack and average the gradient from 10 ﬁne-tuned models, whose training batches are different.
We evaluate the model using the ensemble of rotation and vertical ﬂip as the self-supervised task on
CIFAR10. We evaluate the model with Adaptive-AutoPGD-EOT and Adaptive-SquareAttack-EOT.
One is the strongest attack in our method and the other is a black-box attack that is less likely to be

20

Under review as a conference paper at ICLR 2023

Table 3: Accuracy on transfer attack on CIFAR10.

Methods
Without FT
Online FT

Rotation
VFlip
84.77% 86.55%
86.10% 87.00%

Rotation + VFlip
86.36%
87.10%

Table 4: Accuracy on expectation attack on CIFAR10 with the ensemble of rotation and vertical ﬂip
task.

Attacks
Adaptive-AutoPGD
Adaptive-AutoPGD-EOT
Adaptive-SquareAttack
Adaptive-SquareAttack-EOT

w/o Fine-tuning w/ Fine-tuning

53.99%
53.99%
65.75%
65.75%

57.70%
57.65%
66.80%
66.73%

affected by gradient masking. The experiment shows that the expectation attack has little inﬂuence
on the improvement of our test-time adaptation.

Boundary Attack. We use one of the SOTA decision-based attacks: RayS (Chen & Gu, 2020). We
test it on CIFAR10 with the ensemble of rotation and vertical ﬂip. Table 5 shows that our method
also improves the robust accuracy of the decision-based attack.

GMSA (Chen et al., 2021a) with AutoPGD. GMSA is a recently proposed attack algorithm targeted
at the test-time model adaptation. We use the GMSA with AutoPGD to attack our method, and the
results are shown in Table 6. Under GMSA, our test-time adaptation still signiﬁcantly improves the
robust accuracy. Moreover, Table 6 also demonstrates the strength of our adaptive attack strategy as
it achieves a higher success rate than GMSA.

C.2 ABLATION STUDY

Meta Adversarial Training. Our meta training strategy in Algorithm 1 aims to strengthen the cor-
relation between the self-supervised tasks and classiﬁcation. To show its effectiveness, we perform
an ablation study where we ﬁne-tune the model with regular AT (i.e., setting α = 0 in line 5 of
Algorithm 1). We then perform the same test-time ﬁne-tuning on the model without MAT, using
the same hyperparameters as in the MAT case. As shown in Table 7, the robust accuracy and the
improvements of ﬁne-tuning are consistently worse without MAT.

Accuracy Improvement on Inputs with Different Adversarial Budget. Our method is also able
to improve the robust accuracy of inputs with different adversarial budgets. As shown in Table 8,
we set (cid:96)∞ budget of the adversarial inputs to be 0.015 to perform the online test-time ﬁne-tuning.
The robust accuracy is ed improved.
Removing LSS or LR. In our previous experiments, test-time ﬁne-tuning was achieved using a
combination of two loss functions: LSS and LR. To study the effect of each of these terms sep-
arately, we remove either one of them from Ltest. In Table 9, we report the robust accuracy after
online ﬁne-tuning using only LR and only LSS. While, as expected, removing LSS tends to reduce
more accuracy than removing LR. It shows the beneﬁts of our self-supervised test-time ﬁne-tuning
strategy. Nevertheless, the best results are obtained by exploiting both loss terms.

Accuracy Improvement on Clean Images. As shown in Eqn (30) and Theorem B.1, our method
is able to improve not only the robust accuracy but also the natural accuracy of clean images on
adversarially-trained models. To evidence this, we maintain all the components of our model and
simply replace the adversarial input images with clean images (i.e. replacing (cid:101)B(cid:63) with clean inputs
(cid:101)B in Algorithm 2) and perform the same self-supervised test-time ﬁne-tuning.
As shown in Table 10, our approach increases the clean image accuracy. This phenomenon further
strengthens our conjecture that the improvement of robust accuracy is due to the improvement of
generalization instead of perturbing the model parameters, because randomly perturbing the param-
eters usually lowers the natural accuracy of the model.

21

Under review as a conference paper at ICLR 2023

Table 5: Accuracy on RayS on CIFAR10 with the ensemble of rotation and vertical ﬂip task.

Attacks
RayS
Adaptive-RayS

w/o Fine-tuning w/ Fine-tuning

65.61%
-

77.38%
75.03%

Table 6: Accuracy under GMSA on CIFAR10.

VFlip
Rotation
Tasks
Meta AT w/o FT
53.90% 52.79%
Meta AT w/ FT + Adaptive AutoPGD 56.96% 56.79%
57.60% 58.53%
Meta AT w/ FT + GMSA AutoPGD

Rotation+VFlip
53.16%
57.70%
59.63%

Attacking Objectives. The improvement of the test-time adaptation is not affected by the attack
objectives. Even if no information of the ground truth label is incorporated in the attack, the test-
time adaptation improves the robust accuracy. When the attacker randomly lowers the score of the
false label to perform the adversarial attack, if our method uses the information of the leaked label
to improve the robust accuracy, it will predict the false label and reduce the accuracy. However, as
shown in Table 11, the self-supervised test-time ﬁne-tuning improves the robust accuracy on these
“adversarial” images. Besides, previous experiments on clean images already show that test-time
ﬁne-tuning is effective even if there is no information of the ground truth label.

C.3 ADDITIONAL COMPARISON

Comparison with SOAP (Shi et al., 2020). Our method is different from SOAP as we are ﬁne-
tuning the model to adapt to new examples instead of purifying the input. We apply SOAP-RP to
the adversarially-trained model and ﬁnd that its improvement is marginal. Under AutoPGD, the
accuracy is improved from 53.09% to 53.57%. This improvement is much smaller than our method,
whose improvement is from 53.09% to 57.93%. SOAP only has little effect when combined with
the commonly used AT.

Combination with (Gowal et al., 2020). We combine our test-time adaptation with AT using ad-
ditional data (Gowal et al., 2020). We apply our Meta AT to it with the ensemble of rotation and
vertical ﬂip. Using a WideResNet-28-10, it achieves a robust accuracy of 62.07% under AutoPGD.
With our test-time adaptation, the robust accuracy is improved to 64.34%. The improvement of
robust accuracy is 2.27%.

Robust Accuracy v.s Fine-tuning Steps. Figure 3 shows the robust accuracy at each step of the
test-time ﬁne-tuning for different self-supervised tasks and attack methods. When using the standard
version of attacks, the robust accuracy gradually increases as ﬁne-tuning proceeds. When using our
adaptive attacks, the adversarial examples are generated to attack the network with θT (T = 10)
instead of θ0. Thus, when the parameters gradually change from θ0 to θT , the accuracy drops.

Inference Time. Table 12 shows the inference time for different methods. While the inference time
for our method is larger than SOAP and the normal method when the batch size is 1, the inference
time gets closer when using a larger batch size. And the batch size of 20 or more is a common
scenario of the inference. In order to achieve the statistical optimal adversarial risk, additional time

Table 7: Ablation study on the online test-time ﬁne-tuning. The dataset is CIFAR10 and the task is
the “Rotation + VFlip”. All attacks are standard attacks. SA stands for Square Attack.

Methods
Regular AT
Online FT
Improvement
Meta AT
Online FT
Improvement

SA

65.64% 59.19%
66.26% 60.18%
0.62%
0.99%
65.75% 59.51%
67.34% 61.79%
1.59% 2.28%

FAB
PGD-20 AutoPGD
53.05%
53.16%
75.26%
56.86%
22.21%
3.70%
53.85%
53.99%
59.23%
76.39%
5.24% 22.54%

22

Under review as a conference paper at ICLR 2023

Table 8: Robust test accuracy on CIFAR10 of the online test-time ﬁne-tuning. We use the same
WideResNet-34-10 as in Table 1a, which is trained with (cid:96)∞ budget 0.031. The inputs are in the (cid:96)∞
ball of ε = 0.015. The self-supervised task is the ensemble of rotation and vertical ﬂip.

Methods

Square Attack

PGD-20

AutoPGD

FAB

Standard Adaptive Standard Adaptive Standard Adaptive Standard Adaptive

Meta AT w/o FT 78.01%
Online FT

-

75.34%

-

72.72%

-

72.58%

-

80.50% 79.87% 77.14% 76.75% 77.25% 74.93% 82.04% 83.76%

Table 9: Ablation study on the online test-time ﬁne-tuning. The dataset is CIFAR10 and the task is
the “Rotation + VFlip”. All attacks are standard attacks. Removing the LSS or LR results in lower
robust accuracy than the full method. SA stands for Square Attack.

Methods
Before FT
Online FT
Removing LR
Removing LSS

SA

PGD-20 AutoPGD
53.99%

FAB
65.75% 59.51%
53.85%
67.34% 61.79% 59.23% 76.39%
75.63%
66.83% 60.45%
75.08%
65.44% 60.24%

57.32%
55.64%

Table 10: Accuracy on clean images. Networks are trained with corresponding meta adversarial
training.

Methods
Without FT
Online FT

VFlip
Rotation
84.77% 86.55%
86.10% 87.00%

Rotation + VFlip
86.36%
87.10%

of test-time adaptation is necessary. Reducing the inference time is an important future work of
these kinds of methods.

Combination with TRADES (Zhang et al., 2019). Table 13 shows the robust accuracy of combin-
ing our test-time adaptation with TRADES. Our test-time adaptation improves the robust accuracy
by about 4%, which shows our approach can improve various types of robust training methods.

C.4 VISUALIZATION

In Figure 4, we show the visualization of several examples that our test-time adaptation successfully
corrects the misclassiﬁed examples. The input examples are generated by AutoPGD on CIFAR10,
and we ﬁne-tune the network with the ensemble of Rotation and VFlip tasks. It shows our test-
time adaptation reduces the loss for the whole neighbourhood of the input examples to increase the
accuracy of the model.

In Figure 5, we show the histograms of the loss values for the successful and unsuccessful test-time
adapted models. For each input instance, if the test-time adaptation corrects the wrong prediction,
we count it as successful. And if the misclassiﬁed instance is not correctly predicted after our test-
time adaptation, it is counted as an unsuccessful one. The ﬁgure illustrates that our method can
adapt the model to correctly classify the instances close to the decision boundary (with medium loss
value). However, for the highly misclassiﬁed instances (with large loss value), which are far away
from the decision boundary, our test-time adaptation cannot make the model change so much to
predict correct labels for them.

D DETAILS OF OUR EXPERIMENTAL SETTING

D.1 HYPERPARAMETERS

Meta Adversarial Training. The algorithm of Meta Adversarial Training is shown in Algorithm 1.
We consider an (cid:96)∞ norm with an adversarial budget ε = 0.031. We also use two different net-
work architectures: WideResNet-34-10 for CIFAR10 and ResNet18 for STL10 and Tiny ImageNet.
Following the common settings for AT, we train the network for 100 epochs using SGD with a

23

Under review as a conference paper at ICLR 2023

Table 11: Experiments to rule out the possibility of label leaking. We use the WideResNet-34-10
trained with (cid:96)∞ budget ε = 0.031 and show the robust test accuracy on CIFAR10 of the online
test-time ﬁne-tuning. The self-supervised task is the ensemble of rotation and vertical ﬂip.

Methods

Square Attack

PGD-20

AutoPGD

FAB

Standard Adaptive Standard Adaptive Standard Adaptive Standard Adaptive

Meta AT w/o FT 85.43%
Online FT

-

85.60%

-

85.00%

-

86.22%

-

86.56% 87.63% 86.29% 86.68% 86.10% 85.90% 87.61% 86.97%

Figure 3: Robust accuracy at different steps of the online test-time ﬁne-tuning on CIFAR10.

Table 12: Average inference time for each instance using different methods.

Batch Size
Normal
SOAP (Shi et al., 2020)
Ours

1
17.1ms
163ms
545ms

5
14.5ms
91.2ms
168ms

10
13.2ms
75.3ms
118ms

20
12.8ms
73.1ms
83.9ms

40
11.7ms
72.5ms
82.9ms

momentum factor of 0.9 and a weight decay factor of 5 × 10−4. The learning rate β starts at 0.1
and is divided by a factor of 10 after the 50-th and again after the 75-th epochs. The step size α
in Eqn (8) is the same as β. The factor C (cid:48) in Eqn (11) is set to 1.0. We use 10-iteration PGD
(PGD-10) with a step size of 0.007 to ﬁnd the adversarial image B(cid:63)
j at training time. The weight
of each self-supervised task is set to Ck = 1
K . We set |Bj| = 32 and sample 8 batches B1, ..., B8
in each iteration. Furthermore, we save the model after the 51-st epoch for further evaluation, as
the model obtained right after the ﬁrst learning rate decay usually yields the best performance (Rice
et al., 2020).

We use PGD with the standard cross-entropy loss to generate adversarial examples at training time
in line 3, line 6 and line 8 of Algorithm 1. The hyperparameters of the attacks are as follows:

• Line 3: PGD-10 with step size 0.007.
• Line 6: As θ∗

j is similar to θ, the adversarial examples at this step are similar to those at
Line 4. To save training time, we therefore choose the starting point of the attack as the
adversarial examples in Line 4 and use PGD-2 with a step size of 0.005.

• Line 8: PGD-3 with step size 0.02.

Online Test-time Fine-tuning. The algorithm for online ﬁne-tuning is shown in Algorithm 2. We
ﬁne-tune the network for T = 10 steps with a momentum of 0.9 and a learning rate of η = 5×10−4.
We set Ck = 1
K and C = 15.0. In line 2 of Algorithm 2, we sample a batch B ⊂ D containing 20
training images. In line 3, we use PGD-10 with a step size of 0.007.

Ofﬂine Test-time Fine-tuning. The algorithm for ofﬂine ﬁne-tuning is shown in Algorithm 3. As
stochastic gradient descent is more efﬁcient for a large amount of data, we use stochastic gradient
descent in the ofﬂine ﬁne-tuning. This is the main difference between Algorithm 2 (online ﬁne-

24

246810Steps0.640.650.660.670.68AccuracyStandard Square Attack246810Steps0.580.590.600.610.62AccuracyStandard PGD-20246810Steps0.530.540.550.560.570.580.59AccuracyStandard AutoPGD246810Steps0.550.600.650.700.75AccuracyStandard FABRotationVertical FlipEnsemble246810Steps0.6600.6650.6700.6750.6800.685AccuracyAdaptive Square Attack246810Steps0.600.610.620.63AccuracyAdaptive PGD-20246810Steps0.570.580.590.600.61AccuracyAdaptive AutoPGD246810Steps0.780.790.800.810.820.83AccuracyAdaptive FABRotationVertical FlipEnsembleUnder review as a conference paper at ICLR 2023

Figure 4: Visualization of several examples that our test-time adaptation successfully changes the
wrong prediction. Each row represents an example of loss surfaces before ﬁne-tuning, after ﬁne-
tuning and the loss changes of our ﬁne-tuning. The origin point represents the clean example.
Following (Kim et al., 2021), x-axis represents the direction of the adversarial example and y-axis
is a random direction. The white line is the decision boundary. As the ﬁne-tuned model correctly
classiﬁes the input example, the decision boundary does not exist in the neighbourhood of the clean
input for the ﬁne-tuned model.

25

0.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random DirectionBefore Fine-tuning0.00.20.40.60.81.01.21.40.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random DirectionAfter Fine-tuning0.00.20.40.60.81.01.21.40.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random DirectionLoss Change1.00.80.60.40.20.00.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random Direction0.00.20.40.60.81.01.21.40.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random Direction0.00.20.40.60.81.01.21.40.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random Direction1.00.80.60.40.20.00.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random Direction0.00.20.40.60.81.01.21.40.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random Direction0.00.20.40.60.81.01.21.40.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random Direction1.00.80.60.40.20.00.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random Direction0.00.20.40.60.81.01.21.40.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random Direction0.00.20.40.60.81.01.21.40.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random Direction1.00.80.60.40.20.00.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random Direction0.00.20.40.60.81.01.21.40.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random Direction0.00.20.40.60.81.01.21.40.020.000.02Adversarial Direction0.030.020.010.000.010.020.03Random Direction1.00.80.60.40.20.0Under review as a conference paper at ICLR 2023

Table 13: Combination with our test-time adaptation with TRADES on CIFAR10 with the ensemble
of rotation and vertical ﬂip tasks.

Standard AutoPGD Adaptive AutoPGD GMSA AutoPGD

Meta AT w/o FT
Online FT

54.06%
59.63%

57.74%

59.39%

Figure 5: Histograms of the loss values for the successful and unsuccessful test-time adapted models.

tuning) and Algorithm 3 (ofﬂine ﬁne-tuning). We also ﬁne-tune the network for 10 epochs. The
batch size of each (cid:101)B(cid:63)

j is 128. The other hyperparameters are the same as in the online version.

i : (cid:98)yi

rate η; Steps T ; Weights Ck and C

Algorithm 2 Self-supervised Test-time Fine-tuning
Input: Initial parameters θ0; Adversarial test images (cid:101)B(cid:63) = {(cid:101)x(cid:63)
Output: Prediction of (cid:101)x(cid:63)
1: for t = 1 to T do
2:
3:
4:
5:
6: end for
7: return Prediction (cid:98)yi = arg maxj F ((cid:101)x(cid:63)

Sample a batch of training images B ⊂ D
Find adversarial x(cid:63)
Calculate Ltest in Eqn (6)
θt = θt−1 − η∇θt−1Ltest( (cid:101)B(cid:63), B; θt−1)

i of training image xi ∈ B by PGD attack.

i ; θT )j

i }b

i=1; Training data D; Learning

Attacks. The detailed settings of each attack are provided below:

• PGD-20. We use 20 iterations of PGD with step size γ = 0.003. The attack loss is the

cross-entropy.

• AutoPGD. We use both the cross-entropy and the difference of logits ratio (DLR) as the

attack loss. The hyperparameters are the same as in (Croce & Hein, 2020a).

• FAB. We use the code from (Croce & Hein, 2020a) and keep the hyperparameters the same.

• Square Attack. We set T = 2000 and the initial fraction of the elements p = 0.3. The other

hyperparameters are the same as in (Andriushchenko et al., 2020).

For the adaptive versions, we set the interval u = (cid:100)T /5(cid:101).

D.2 SELF-SUPERVISED TASKS

Rotation Prediction is a widely used self-supervision task proposed in (Gidaris et al., 2018) and
has been employed in AT as an auxiliary task to improve the robust accuracy (Chen et al., 2020a;
Hendrycks et al., 2019). Following (Gidaris et al., 2018), we create 4 copies of the input image by
rotating it with Ω = {0◦, 90◦, 180◦, 270◦}. The task then consists of a 4-way classiﬁcation problem,
where the head grotate aims to predict the correct rotation angle. The loss for an image x is the average

26

02468Cross Entropy Before Fine-tuning0.00.20.40.60.81.0DensityRotation246Cross Entropy Before Fine-tuning0.00.20.40.60.81.01.2DensityVFlip0246Cross Entropy Before Fine-tuning0.00.20.40.60.81.01.2DensityRotation+VFlipSuccessfulUnsuccessfulUnder review as a conference paper at ICLR 2023

Algorithm 3 Self-supervised Test-time Fine-tuning with SGD

Input:

Initial parameters θ0; Adversarial test images (cid:101)B(cid:63) = {(cid:101)x(cid:63)
η; Steps T ; Weights Ck and C

i }b

i : ˆyi

j in (cid:101)B(cid:63)

1 , ..., (cid:101)B(cid:63)
r

1 , ..., (cid:101)B(cid:63)

Divide (cid:101)B(cid:63) into r subsets (cid:101)B(cid:63)
for (cid:101)B(cid:63)

Output: Prediction of (cid:101)x(cid:63)
1: for t = 1 to T do
2:
3:
4:
5:
6:
7:
8: end for
9: return Prediction ˆyi = arg maxj F ((cid:101)x(cid:63)

Sample a batch of training images B ⊂ D
Find adversarial x(cid:63)
θt = θt−1 − η∇θt−1Ltest( (cid:101)B(cid:63)

j , B; θt−1)

i ; θT )j

end for

r do

i of training image xi ∈ B by PGD attack.

i=1; Training data D; Learning rate

cross-entropy over the 4 copies, given by

Lrotate(x) = −

1
4

(cid:88)

ω∈Ω

log(Grotate(xω)ω) ,

(37)

where xω is the rotated image with angle ω ∈ Ω, Grotate = grotate ◦ E denotes the classiﬁer for
rotation prediction, and Grotate(·)ω is the predicted probability for the ω angle. The head grotate is a
fully-connected layer followed by a softmax layer.

Vertical Flip (VFlip) Prediction is a self-supervised task similar to rotation prediction and has
also been used for self-supervised learning (Saito et al., 2020). In essence, we make two copies of
the input image and ﬂip one copy vertically. The head gvﬂip then contains a 2-way fully-connected
layer followed by a softmax layer and predicts whether the image is vertically ﬂipped or not. The
corresponding loss for an image x is

Lvﬂip(x) = −

1
2

(cid:88)

v∈V

log(Gvﬂip(xv)v) ,

(38)

where V = {ﬂipped, not ﬂipped} is the operation set and Gvﬂip = gvﬂip ◦ E. xv denotes and
transformed input and Gvﬂip(·)v is the probability of operation v. Note that we do not ﬂip the
image horizontally as it is a common data augmentation technique and classiﬁers typically seek to
be invariant to horizontal ﬂip.

E ADAPTIVE ATTACKS

In the white-box attacks, the attacker knows every detail of the defense method. Therefore, we need
to assume that the attacker is aware of our test-time adaptation method and will adjust its strategy for
generating adversarial examples accordingly. Here, we discuss one such strong adaptation strategy
targeted to our method.

Suppose that the attacker is fully aware of the hyperparameters for test-time adaptation. Then,
ﬁnding adversaries (cid:101)B(cid:63) of the clean subset (cid:101)B can be achieved by maximizing the adaptive loss

(cid:101)x(cid:63)
i = arg max
(cid:107)(cid:101)x(cid:63)
i −xi(cid:107)≤ε

Lattack(F ((cid:101)x(cid:63)

i ), y; θT ( (cid:101)B(cid:63))) ,

(39)

where Lattack refers to the general attack loss, such as the cross-entropy or the difference of logit ra-
tio (DLR) (Croce & Hein, 2020a). Let θT be the ﬁne-tuned test-time parameters using Algorithm 2.
At the k-th step of the attack, it depends on the input (cid:101)B(k) = {((cid:101)x(k)
j
θt+1 = θt − η∇θtLtest( (cid:101)B(k), B) ,
(40)
where Ltest and B are the loss function and subset of training images mentioned in Eqn (6). As
θT is a function of the input (cid:101)B(k), we can calculate the end-to-end gradient of (cid:101)x(k)
i ∈ (cid:101)B(k) as

j=1 via the update

, (cid:101)yj)}b

27

Under review as a conference paper at ICLR 2023

i

∇

(cid:101)x(k)

Lattack(F ((cid:101)x(k)
); θT ( (cid:101)B(k))). However, θT goes through T gradient descent steps, and thus
θT ( (cid:101)B(k)) requires T -th order derivatives of the backbone E, which is
calculating the gradient ∇
virtually impossible if T or the dimension of θE is large. We therefore approximate the gradient as

(cid:101)x(k)

i

i

(41)

Grad((cid:101)x(k)

i

) ≈ ∇

Lattack(F ((cid:101)x(k)

); θT ) ,

i

i

(cid:101)x(k)
which treats θT as a ﬁxed variable so that high-order derivatives from θT ( (cid:101)B(k)((cid:101)x(k)
)) can be
avoided. Although this approximation makes Grad((cid:101)x(k)
) inaccurate, common white-box attacks
use projected gradients, which are robust to such inaccuracies. For example, PGD only uses the sign
of the gradient under an (cid:96)∞ adversarial budget. Note that solving the maximization in Eqn (12) does
not necessarily require calculating the gradient Grad((cid:101)x(k)
). For instance, we will also use Square
Attack (Andriushchenko et al., 2020), a strong score-based black-box attack, to maximize Eqn (12)
and generate adversaries for (cid:101)B.
As another approximation to save time, one can also ﬁx θT for several iterations. This leverages the
intuition that attack strategies often make small changes to the input (cid:101)x, and thus, for the intermediate
images in the k-th and (k+1)-th steps, θT ( (cid:101)B(k)) and θT ( (cid:101)B(k+1)) should be close. Therefore, a
general version of our adaptive attacks only updates θT every u iterations, with u a hyperparameter.

i

i

i

In Algorithm 4, 6, 5 and 7, we show the algorithms for (cid:96)∞ norm-based adaptive PGD, AutoPGD,
Square Attack and FAB, respectively. The main difference between the original and adaptive ver-
sions is the target loss function for maximization. The reader may refer to (Andriushchenko et al.,
2020; Croce & Hein, 2020a;b) for a more detailed description of the steps in these algorithms (e.g.,
the condition for decreasing the learning rate in AutoPGD).

Algorithm 4 (cid:96)∞ Norm Adaptive PGD Attack
Input: Test images (cid:101)B = {((cid:101)xi, (cid:101)yi)}; Attack loss Lattack; Step size γ; Iterations T ; Intervals u;

Adversarial budget ε; Trained parameters of the network θ0.

Get ﬁnal parameters θT by taking (cid:101)B(cid:48) as input image for Algorithm 2: θ = θT

if t mod u = 0 then

Output: Adversarial images (cid:101)B(cid:63) = {(cid:101)x(cid:63)
i }
1: Add random noise to (cid:101)xi in (cid:101)B and get (cid:101)B(cid:48)
2: for t = 1 to T do
3:
4:
5:
6:
7:
8:
9:
10: end for
11: return Adversarial image (cid:101)x(cid:63)

Grad((cid:101)x(cid:48)
i) = ∇
i = Clip[(cid:101)xi−ε,(cid:101)xi+ε]((cid:101)x(cid:48)
(cid:101)x(cid:48)
end for

Lattack(F ((cid:101)x(cid:48)

end if
for (cid:101)x(cid:48)

i in (cid:101)B(cid:48) do

i = (cid:101)x(cid:48)

(cid:101)x(cid:48)

i

i

i), (cid:101)yi; θ)
i + γSign(Grad((cid:101)x(cid:48)

i)))

28

Under review as a conference paper at ICLR 2023

Algorithm 5 (cid:96)∞ Norm Adaptive AutoPGD
Input: Test images (cid:101)B = {((cid:101)xi, (cid:101)yi)}; Attack loss Lattack; Step size γ; Iterations T ; Intervals u;
Adversarial budget ε; Parameter of the adversarially-trained network θ0; Decay iterations W =
{w0, ..., wn}; Momentum ξ

(cid:101)xi

i + γSign(Grad((cid:101)xi)))

Lattack(F ((cid:101)xi), (cid:101)yi; θ)

Output: Adversarial image (cid:101)B(cid:63) = {(cid:101)x(cid:63)
i }
1: Get ﬁnal parameter θT by taking (cid:101)B as input image for Algorithm 2.
2: θ = θT
3: for (cid:101)xi in (cid:101)B do
(cid:101)x0
i = (cid:101)xi
4:
5: Grad((cid:101)xi) = ∇
i = Clip[(cid:101)xi−ε,(cid:101)xi+ε]((cid:101)x0
(cid:101)x1
6:
i = Lattack(F ((cid:101)x0
l0
7:
i = Lattack(F ((cid:101)x1
l1
8:
i , l1
i = max{l0
l∗
i }
9:
i = l0
i if l∗
i = (cid:101)x0
(cid:101)x∗
10:
11: end for
12: for t = 1 to T − 1 do
if t mod u = 0 then
13:
14:
15:
16:
17:
18:

Get ﬁnal parameter θT by taking (cid:101)B∗ = {(cid:101)x∗
θ = θT

i ), (cid:101)yi; θ)
i ), (cid:101)yi; θ)
i else (cid:101)x∗

end if
for i = 1, ..., | (cid:101)B| do
(cid:101)xt

Lattack(F ((cid:101)xt

i = (cid:101)x1

i) = ∇

i

i

i), (cid:101)yi; θ)
i + γSign(Grad((cid:101)xt
i + ξ(zt+1
i − zt

Grad((cid:101)xt
zt+1
i = Clip[(cid:101)xi−ε,(cid:101)xi+ε]((cid:101)xt
(cid:101)xt+1
i = Clip[(cid:101)xi−ε,(cid:101)xi+ε]((cid:101)xt
i = Lattack(F ((cid:101)xt+1
lt+1
i
i = lt+1
i = (cid:101)xt+1
i > l∗
(cid:101)x∗
i
i
if k ∈ W and satisfy the condition of dropping learning rate then
γ = γ/2 and (cid:101)xt+1

i ) + (1 − ξ)((cid:101)xt

), (cid:101)yi; θ)
if lt+1

i = (cid:101)x∗

i − (cid:101)xt−1

i)))

i

i

))

i

and l∗

21:
22:
23:
24:
25:
26:
27: end for
28: return Adversarial image (cid:101)x(cid:63)

end if
end for

i = (cid:101)x∗

i

19:

20:

i } as input image for Algorithm 2.

29

Under review as a conference paper at ICLR 2023

Algorithm 6 (cid:96)∞ Norm Adaptive Square Attack
Input: Test images (cid:101)B = {((cid:101)xi, (cid:101)yi)}; Attack loss Lattack; Step size γ; Iterations T ; Intervals u;
Image size w; Color channels c; Adversarial budget ε; Parameter of the adversarially-trained
network θ0.

Get ﬁnal parameter θT by taking (cid:101)B(cid:48) as input image for Algorithm 2.
θ = θT

ht ←− side length of the square to modify (according to some schedule)
δ ←− array of zeros of size w × w × c
Sample uniformly r, s ∈ {0, ..., w − ht} ⊂ N
for j = 1, ..., c do

i in (cid:101)B(cid:48) do

end if
for (cid:101)x(cid:48)

if t mod u = 0 then

Output: Adversarial image (cid:101)B(cid:63) = {(cid:101)x(cid:63)
i }
1: Add noise to (cid:101)xi in (cid:101)B and get (cid:101)B(cid:48)
2: for t = 1 to T do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22: end for
23: return Adversarial image (cid:101)x(cid:63)

end for
i = Clip[(cid:101)xi−ε,(cid:101)xi+ε]((cid:101)x(cid:48)
(cid:101)xnew
i = Lattack(F ((cid:101)xnew
lnew
if lnew < l∗ then
(cid:101)x(cid:48)
i = (cid:101)xnew
i
l∗
i = lnew
i
end if
end for

i + δ)
), (cid:101)yi; θ)

i = (cid:101)x(cid:48)

i

i

ρ ←− Uniform(−2ε, 2ε)
δr+1:r+ht,s+1:s+ht = ρ · 1ht×ht

30

Under review as a conference paper at ICLR 2023

Algorithm 7 (cid:96)∞ Norm Adaptive FAB

Input: Test images (cid:101)B = {((cid:101)xi, (cid:101)yi)}; Step size γ; Iterations T ; Intervals u; Adversarial budget ε;

Trained parameters of the network θ0; αmax, η, β.

Get ﬁnal parameters θT by taking (cid:101)B(cid:48) as input image for Algorithm 2: θ = θT

i; θ)l
i;θ)l−F ((cid:101)x(cid:48)
i)l−Grad((cid:101)x(cid:48)

i;θ)yi |

i)yi (cid:107)1

Output: Adversarial images (cid:101)B(cid:63) = {(cid:101)x(cid:63)
i }
1: Add random noise to (cid:101)xi in (cid:101)B and get (cid:101)B(cid:48)
2: v = +∞
3: for t = 1 to T do
4:
5:
6:
7:
8:

if t mod u = 0 then

end if
for (cid:101)x(cid:48)

i in (cid:101)B(cid:48) do

9:

10:
11:

12:

13:

i

F ((cid:101)x(cid:48)
|F ((cid:101)x(cid:48)
(cid:107)Grad((cid:101)x(cid:48)

Grad((cid:101)x(cid:48)
i)l = ∇
(cid:101)x(cid:48)
s = arg minl(cid:54)=yi
δt = proj∞((cid:101)x(cid:48)
δt
orig = proj∞((cid:101)xi, πs, C)
α = min

i, πs, C)

(cid:26)

orig (cid:107)∞

(cid:101)x(cid:48)
i = projC
if (cid:101)x(cid:48)

(cid:107)δt(cid:107)∞
(cid:107)δt(cid:107)∞+(cid:107)δt
(cid:16)
(1 − α)((cid:101)x(cid:48)
i is not classiﬁed as yi then
if (cid:107)(cid:101)x(cid:48)
i − (cid:101)xi(cid:107)∞ < v then
(cid:101)x(cid:63)
i = (cid:101)x(cid:48)
v = (cid:107)(cid:101)x(cid:48)
end if
i = (1 − β)(cid:101)xi + β(cid:101)x(cid:48)
(cid:101)x(cid:48)
end if
end for

14:
15:
16:
17:
18:
19:
20:
21:
22: end for
23: return Adversarial image (cid:101)x(cid:63)

i
i − (cid:101)xi(cid:107)∞

i

i

(cid:27)

∈ [0, 1]

, αmax

i + ηδt) + α((cid:101)xi + ηδt

(cid:17)
orig)

31

